# (PART) Models {-}

# Relationships

<!-- > “[M]y ally is the Force, and a powerful ally it is. Life creates it, makes it grow. Its energy surrounds us, binds us. Luminous beings are we, not this crude matter. You must feel the Force flow around you. Here, between you, me, the tree, the rock, yes, even between the land and the ship.”</b> -->
<!-- > -->
<!-- > `r tufte::quote_footer('Yoda. -- Episode V: The Empire Strikes Back')` -->

The most interesting research questions in social science are about relationships. A relationship is the way in which two or more variables or concepts are connected.^[Everything that can be measured (see  chapter [Can we measure everything?]).] The analysis of relationships is a fundamental approach used to understand the interplay and patterns of interactions within a social system.

This chapter introduces the concepts of *variance*, *covariance*, and *correlation*, and how they can help us understand the connections between two variables. The specific statistical form of the analysis depends on the levels of measurement of these variables. In this example we quantify a relationship between two continuous variables that may be illustrated in a *scatter plot*. 

A correlation estimate may not represent the relationship best. Beware of spurious relationships, for they can lead us astray in our quest for understanding. Correlation can be the beginning of a more thorough research inquiry. 

<!-- What is the relationship between beauty and employment chances? What is the connection between money and happiness? How does remote work change ones productivity? Is social support related to longevity of marriages?  -->

<!-- What factors contribute to the success and longevity of marriages across different cultures? What are the long-term effects of childhood attachment styles on adult romantic relationships? Does owning a pet makes you live longer? -->

<!-- Recall that there is a statistical relationship between two variables when the average score on one differs systematically across the levels of the other. In this section, we revisit the two basic forms of statistical relationship introduced earlier in the book—differences between groups or conditions and relationships between quantitative variables—and we consider how to describe them in more detail. -->

<!-- https://plato.stanford.edu/entries/relations/ -->
<!-- go together flow together -->

## Storks Deliver Babies

<!-- https://www.nabu.de/imperia/md/content/bergenhusen/130618-nabu-weissstorchenzensus.pdf -->
<!-- https://www.nabu.de/news/2005/zensus.html -->
<!-- https://en.wikipedia.org/wiki/White_stork#/media/File:Europe_stork.png -->

The "stork-baby-relationship" is a classic example used to illustrate the concept that correlation does not imply causation. People noticed a correlation between the number of storks (birds) in an area and the number of human babies born. Stork populations and birth rates seem to increase together.

```{r, echo=FALSE, out.width="80%", fig.cap="The first known pair in Finland (2015), representing a northward expansion compared to the species' historical breeding range.", fig.align='center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Haikara_%28White_stork%29_%281%29_Koski_Tl._9.7.2015.jpg/1280px-Haikara_%28White_stork%29_%281%29_Koski_Tl._9.7.2015.jpg") 
```

<!-- The following story is based on  -->
<!--     Matthews, R. (2000). Storks deliver babies (p= 0.008). Teaching Statistics, 22(2), 36-38. -->

We have data for 28 countries in 2005. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(readxl)
library(tidyverse)
library(countrycode)

# Stork Data
storks <- readxl::read_xlsx("./data/Storks/Storks_Clean.xlsx", sheet = 1)
storks <- storks %>% filter(Year == 2005)

# Tabular Data
library(DT)
datatable(storks)
```

`Storks` is the number of pairs of storks in that country. The `Area` is in square kilometers. `Population` is the total population in million whereas `UrbanPop` is the population living in urban areas. `Fertility` refers to the total fertility rate, that is the average number of children that would be born to a female over their lifetime. 

## Scatterplot 

The *scatterplot* is a two-dimensional instrument that shows the number of storks on the x-axis and the population on the y-axis. The blue line illustrates the linear trend between the variables. Since its slope is increasing, it suggests a positive connection between storks and population, i.e. countries with more pairs of storks also tend to have a higher population.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(ggpmisc)

ggplot(storks, aes(x = Storks, y = Population)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F) +
  theme_minimal()
```

The data doesn't cluster in a nice dot cloud. Some countries have almost no storks, whereas there are thousands in other places. Countries seem to be very different in the number of storks. Let's explore this in more detail.

## Map

The map show the number of stork pairs for the selected countries. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Retrieve the map data for Participant Countries 
# library("maps")
# stork_map <- map_data("world", region = storks$code)

# ggplot(stork_map) +
#   geom_map(aes(map_id = region), 
#            map = stork_map) +
#   geom_polygon(data = stork_map, aes(x = long, y = lat, group = group), 
#                colour = 'black', fill = NA) +
#   expand_limits(x = world_map$long, y = world_map$lat) +
#   scale_fill_brewer(name = "Counts", palette = "Reds") +
#   theme_void() +
#   coord_fixed()

# library(geosphere)
# 
# centroids <- stork_map %>% 
#   group_by(region) %>% 
#   group_modify(~ data.frame(centroid(cbind(.x$long, .x$lat))))
# 
# storks95 <- storks %>% filter(Year == 1995)
# 
# ggplot(stork_map) +
#   geom_map(aes(map_id = region), 
#            map = stork_map) +
#   geom_polygon(data = stork_map, aes(x = long, y = lat, group = group), 
#                fill = NA) +
#   expand_limits(x = stork_map$long, y = stork_map$lat) +
#   scale_fill_brewer(name = "Counts", palette = "Reds") +
#   theme_void() +
#   coord_fixed() +
#   geom_point(data = centroids, aes(lon, lat), col = "red")


# https://cran.r-project.org/web/packages/rworldmap/vignettes/rworldmap.pdf
library(rworldmap)

#create a map-shaped window
#mapDevice('x11')

#join to a coarse resolution map
capture.output(spdf <- joinCountryData2Map(storks, joinCode="ISO3", nameJoinColumn="code"), file='NUL' )

# create map for value
mapCountryData(spdf, 
               mapTitle = "Number of Stork Pairs accross Countries", 
               nameColumnToPlot = "Storks",
               catMethod = "fixedWidth", 
               mapRegion = "Eurasia",
               #oceanCol = "lightblue",
               numCats = 12,
               colourPalette = "white2Black")
```

There seem to be some extreme values in the number of storks. In other words, the range goes from 3 pairs in Albania to 52500 in Poland. There is high variability in the number of storks. The variance is a measure for variability of data. 

## Variance

To calculate the variance, we subtract each data point from the mean. Then square those deviations and add them up. Finally there is a scaling factor, we divide by the inverse number of observations minus 1. 

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **variance** is defined as the **average quadratic deviation from the mean**.
::::

$$var(x) = \frac{1}{n-1} \sum (x_i - \overline{x} )^2$$

The <u id='variance'>variance</u> of `Storks` and can be calculated via `var()` in R. 

```{r, echo=FALSE}
tippy::tippy_this(elementId = "variance", tooltip = "The term variance was first introduced by Ronald Fisher in his 1918 paper *The Correlation Between Relatives on the Supposition of Mendelian Inheritance*.")
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
var(storks$Storks)
```

:::: {.dedicated}
::: {.titeldedicated}
<h2> Truly Dedicated </h2>
:::
When you have collected data from every member of the population that you’re interested in, you can get an exact value for *population variance*. When you collect data from a sample, the *sample variance* is used to make estimates or inferences about the population variance. Sample variance is divided by $n-1$. Population variance is divided by $n$. 
::::


<!-- The variance of `x` can be calculated from its components in R as a combination of `sum()`, `mean()` and `length()`. -->

<!-- ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # Variance by hand -->
<!-- sum( (x-mean(x))^2 )/(length(x) - 1)  -->
<!-- ``` -->

## Standard Deviation 

The variance is in squared unit and thus hard to interpret. The standard deviation is derived from variance and tells, on average, how far each value lies from the mean. It’s the square root of variance and calculated via `sd()`. Variance and standard deviation both measure the variability of a variable.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
sd(storks$Storks)
```

<!-- https://www.scribbr.com/statistics/variance/ -->
<!-- http://stla.overblog.com/schematizing-the-variance-as-a-moment-of-inertia -->
<!-- https://stats.stackexchange.com/questions/72208/visualising-the-variance -->

The one-dimensional variable `Storks` is visualized as points on a line. The mean number of storks is `r mean(storks$Storks)` (red bold line). The standard variation is `r sd(storks$Storks)` and surrounds the mean. 

```{r, echo=F, eval=T, message=FALSE, warning=FALSE, out.width = '80%', fig.align="center"}
library(ggplot2)
library(tidyverse)

# x=c(4,13,19,25,29)
# y=c(10,12,28,32,38)
# data <- data.frame(x, y) 

data <- data.frame(x = storks$Storks,
                   y = storks$Population)

x <- data$x
y <- data$y

ggplot(aes(x=x, y=c(0)), data=data) + 
  geom_point(size=3) +
  labs(title="Distribution of the number of storks around the mean") +
  geom_vline(xintercept=mean(data$x), size=1.5, color="red") + 
  theme_minimal() +
  theme(axis.line=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        plot.background=element_blank()) +
  geom_segment(aes(x = mean(data$x), 
                   y = 0.025, 
                   xend = mean(data$x) + sd(data$x), 
                   yend = 0.025),
               size = 1,
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(aes(x = mean(data$x), 
                   y = -0.025, 
                   xend = mean(data$x) - sd(data$x), 
                   yend = -0.025),
               size = 1,
               arrow = arrow(length = unit(0.5, "cm"))) + ylim(-0.05, 0.05) +
  geom_label(aes(x=mean(data$x) + sd(data$x), y=0.035, label = "+ 1 SD")) +
  geom_label(aes(x=mean(data$x) - sd(data$x), y=-0.035, label = "- 1 SD")) + 
  geom_label(aes(x=mean(data$x), y=0.035, label = "mean"), color="red")
```

<!-- https://stats.stackexchange.com/questions/311174/what-does-it-mean-when-three-standard-deviations-away-from-the-mean-i-land-ou -->

:::: {.dedicated}
::: {.titeldedicated}
<h2> Truly Dedicated </h2>
:::
In statistics, the empirical rule states that 

- 68% of the observed data will occur within the first standard deviation, 
- 95% will take place in the second deviation and 
- 99.7% within the third standard deviation

of the mean within a normal distribution. See [68–95–99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) 
::::

The following scatterplot shows the number of storks and the population with their respective means as bold red lines. 

```{r, echo=F, eval=TRUE, out.width = '80%', fig.align="center"}
ggplot(aes(x=x, y=y), data=data) + 
  geom_point(size=3) +
  labs(title="Distribution of storks and population around their means") +
  geom_vline(xintercept=mean(data$x), size=1.5, color="red") + 
  geom_hline(yintercept=mean(data$y), size=1.5, color="red") +
  theme_minimal()
```

Now, that we know how to describe the variation of each of the two variables, we look for a measure that reflects the co-variation of both variables, i.e. how they change in relation to each other. The new grid of means is a good starting point. When most data points fall into lower left and upper right quadrant, we call this a positive relationship. 

## Covariance  

Covariance is a measure of the joint variability of two variables. The main idea of covariance is to classify three types of relationships: positive, negative or no relationship. 

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **covariance** between two variables is the **product of the deviations of x and y from their respective means**. 
::::

$$cov(x,y) = \frac{1}{n-1} \sum\limits (x_i - \bar{x})(y_i - \bar{y})$$ 

For each data point, we multiply the differences with the respective mean. This results in several rectangular areas starting at the intersection of means as a new origin. The covariance sums up all these areas. Finally, the covariance is adjusted by the number of observations. When both values are smaller or greater than the mean, the result will be positive. 

In R, the covariance is calculated by `cov()`:

```{r, echo=TRUE, message=FALSE, warning=FALSE}
cov(x,y)
```

<!-- The covariance of `x` can be calculated from its components in R as a combination of `sum()`, `mean()` and `length()`. -->

<!-- ```{r, message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE} -->
<!-- # Covariance by hand -->
<!-- (sum((x-mean(x))*(y-mean(y)))) / (length(x)-1) -->
<!-- ``` -->

<!-- https://evangelinereynolds.netlify.app/post/geometric-covariance/ -->
<!-- https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean -->

The covariance confirms what we saw in the first scatterplot, the relationship between storks and population is positive. Let's try to visualize the calculation procedure of the covariance. 

```{r, echo=F, eval=TRUE, message=FALSE, warning=FALSE, out.width = '80%', fig.align="center"}
simple.arrow <- arrow(length = unit(0.2, "cm"))

ggplot(aes(x=x, y=y), data=data) + 
  geom_point(size=1) +
  labs(title="Distribution of x and y around their means") +
  theme_bw() +
  geom_vline(xintercept=mean(data$x), size=1.5, color="red") + 
  geom_hline(yintercept=mean(data$y), size=1.5, color="red") +
  geom_segment(aes(y = mean(data$y), xend = x, yend = y, colour=y>mean(data$y)), arrow = simple.arrow) +
  geom_segment(aes(x = mean(data$x), xend = x, yend = y, colour=x>mean(data$x)), arrow = simple.arrow) +
  geom_rect(aes(
    xmin = ifelse(x > mean(x), mean(x), x),
    xmax = ifelse(x > mean(x), x, mean(x)),
    ymin = ifelse(y > mean(y), mean(y), y),
    ymax = ifelse(y > mean(y), y, mean(y)),
    fill = (x-mean(x)) * (y-mean(y)) > 0
  ),
  alpha = 0.1) +
  geom_point() +
  theme_minimal() + 
  geom_label(aes(x=11000, y=35, label = "Sum of all squares: 2409346")) +
  theme(legend.position="none")
```

:::: {.challenge}
::: {.titelchallenge}
<h2> Your Turn </h2>
:::
Can you validate the covariance result from `cov(x,y)` from the sum of squares from the figure?
::::

Covariance qualifies a relationship as positive or negative, i.e. the direction of the relationship. Covariance is expressed in units that vary with the data. Because the data are not standardized, you cannot use the covariance statistic to assess the strength of a linear relationship (a covariance of 117.5 can be very low for one relationship and 0.23 very high for another relationship). Therefore, we need one more measure that is similar and related to the covariance, the correlation coefficient. 

## Correlation 

To assess the strength of a relationship between two variables a correlation coefficient is commonly used. It brings variation to a standardized scale between -1 to +1.

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **correlation coefficient** is a statistical measure of the strength and direction of the relationship between two variables.
::::

$$r(x,y) = \frac{\sum\limits (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum\limits (x_i - \bar{x})^2 \sum\limits (y_i - \bar{y})^2}}$$

Does the numerator and denominator remind you of something? The formula is made of the components variance and covariance. Thus, the correlation coefficient formula is often expressed in short as:

$$r(x,y,) = \frac{Cov(x,y)}{\sqrt{Var(x) Var(y)}}$$

`cor()` is a basic function to calculate the correlation coefficient. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Basic function 
cor(x,y)
```

The correlation coefficients confirms ones more, there is a positive relationship. You will find thresholds for different fields of research that classify the magnitude of the correlation coefficient as *weak*, *moderate* and *strong*. Social science usually accept lower correlation values to be meaningful. One possible classification could be:

- above 0.4 is strong
- between 0.2 and 0.4 is moderate, 
- and those below 0.2 are considered weak.
 
Thus we may consider the stork-population-relationship as weak to moderate. Keep in mind that these thresholds are not set in stone. Now, let's turn to `cor.test()`, a more sophisticated version including a hypothesis test.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Advanced function 
cor.test(x,y)
```

<!-- ## Statistical significance: p<0.05 (or rule of thumb t>2) -->
<!-- ## t value is 5.6757 -->
<!-- ## p value is 0.01084  -->

The correlation test is based on a t-value (t = `r cor.test(x,y)$statistic`) and returns a p-value (`r cor.test(x,y)$p.value`) for statistical significance. 

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **p value** is a statistical measure to determine whether the results of a statistical analysis are statistically significant or if they could have occurred due to random chance. 
::::

Small p-values below 0.05 are usually considered to be statistically significant. This is not the case for our stork-population relationship. 

<!-- Please look up the exact formula to produce this t- and p-value and code it in R (each 1 pt).  -->

<!-- Hint: For the t-value you can use a combination of `cor()` and `sqrt()`. For the p-value you can use the `pt()`.  -->

<!-- ```{r, echo=TRUE, message=FALSE, warning=FALSE} -->
<!-- ## Calculate t-value for correlation -->
<!-- t <- cor(x,y) * sqrt(3) / sqrt(1-cor(x,y)^2)  -->
<!-- t -->
<!-- ## Calculate p-value for correlation -->
<!-- ## https://stackoverflow.com/questions/46186115/calculating-p-values-for-given-t-value-in-r -->
<!-- ## needs: t, df and pt (Student t cumulative Distribution) -->
<!-- p <- (1 - pt(q = abs(t), df = 3))*2 -->
<!-- p -->
<!-- ``` -->

<!-- https://stats.stackexchange.com/questions/32464/how-does-the-correlation-coefficient-differ-from-regression-slope -->

There is an awesome connection from correlation coefficient to the simple regression coefficient.

:::: {.amazing}
::: {.titelamazing}
<h2> Amazing Fact </h2>
:::
The correlation coefficient and the simple regression coefficient coincide when the two variables are on the same scale. The most common way of achieving this is through standardization.  

$$\beta = cor(Y,X) \cdot \frac{SD(Y)}{SD(X)} $$

:::

Here is the replication:

```{r, echo=TRUE}
# The data
df = data.frame(x=c(4,13,19,25,29), y=c(10,12,28,32,38))

# The correlation coefficient 
cor_coef <- cor(df$x, df$y, method="pearson")
cor_coef
```

The correlation coefficient is `r cor.test(x,y)$statistic`.

```{r, echo=TRUE}
# The regression coefficient 
linear_model <- lm(y~x, data=df)
reg_coef <- linear_model$coefficients[2]
reg_coef
```

The regression coefficient of `x` is `r linear_model$coefficients[2]`. Here is the connection:

```{r, echo=TRUE}
# The connection
cor_coef * sd(y) / sd(x)
```

Alternatively standardize the data first, then calculate correlation and regression: 

```{r, echo=TRUE}
# The connection
df_scaled <- as.data.frame(scale(df, center = TRUE, scale = TRUE))

# The correlation on standardized variables
cor(df_scaled$x, df_scaled$y, method="pearson")
```

```{r, echo=TRUE}
# The regression on standardized variables
lm(y~x, data=df_scaled)$coefficients[2]
```




