# Regression 

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri("images/workhorse.jpg"), 
               alt = 'horse', 
               style = 'position:absolute; top:0; right:0; padding:10px;',
               width="100")
```

This chapter introduces the workhorse of empirical research in the social science: Regression. 

> “As an undergraduate **I studied economics, which meant I studied a lot of regressions**. It was basically 90% of the curriculum (when we’re not discussing supply and demand curves, of course). The [effect of corruption on sumo wrestling](https://www.aeaweb.org/articles?id=10.1257/000282802762024665)? Regression. [Effect of minimum wage changes on a Wendy’s in NJ](https://www.nber.org/system/files/working_papers/w4509/w4509.pdf)? Regression. Or maybe [The Zombie Lawyer Apocalypse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2550498) is more your speed (O.K., not a regression, but the title was cool).”</b>

`r tufte::quote_footer('-- Amanda West (2020) -- [A Beginner’s Guide to Instrumental Variables](https://towardsdatascience.com/a-beginners-guide-to-using-instrumental-variables-635fd5a1b35f)')`

<!-- > "We are gonna talk about what has been, what is and what always will be the **most important statistical technique ever: Regression.** Almost every analysis you will read or see uses regression in some way of another." -->

<!-- `r tufte::quote_footer('-- Prof. Matt Masten (2015) -- [Introduction to Regression Analysis: Causal Inference Bootcamp](https://www.youtube.com/watch?v=ROLeLaR-17U&list=PL1M5TsfDV6Vu-GcB4Eb1P1KQ3wVxMI9Mn)')` -->

<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- library("vembedr") -->
<!-- embed_url("https://www.youtube.com/watch?v=ROLeLaR-17U") %>% -->
<!--   use_start_time("0m3") %>% -->
<!--   use_align("center") -->
<!-- ``` -->

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/_emSBt_4uWE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<!-- :::: {.amazing} -->
<!-- ::: {.titelamazing} -->
<!-- <h2> Amazing Fact </h2> -->
<!-- ::: -->
<!-- Linear regression was first introduced by **Sir Francis Galton** in the late 19th century, and he used it to study the relationship between the height of parents and their children.  -->
<!-- :::: -->

## Old but Gold

<!-- Your favorite topic -->
<!-- > “Every artist was first an amateur.” -->
<!-- `r tufte::quote_footer('-- Ralph Waldo Emerson (1803--1882), US-American essayist, lecturer, philosopher, abolitionist and poet.')` -->
<!-- This I speculate that the quote from Mr. Emerson is also true for social scientists.  -->

Are older artists are better than younger artists? While experience and maturity can certainly contribute to an artist's skill and creativity, there are many factors that can influence the quality of an artist's work, such as natural talent, dedication, training, and access to resources. Is there an optimum age for artistic performance as compared to athletic performances which reaches a peak in youth? Do artists improve their skills and performance over an entire lifetime step-by-step such that the longer you live, the more you have time to practice? Does exceptional art happens randomly?  Perhaps it takes time to become more well-known. You need time to travel and show or sell your art in different places. Thus when you produce "more art" you increase the chance to be discovered by the public or a patron? Have you ever heard of an artist who exactly created one piece of art? 

There seems to be something to the story.

> "Paul Cezanne died in October 1906 at the age of 67. In time he would be generally regarded as the most influential painter who had worked in the nineteenth century (e.g., Clive Bell, 1982; Clement Greenberg, 1993). Art historians and critics would also agree that his greatest achievement was the work he did late in his life."
>
> `r tufte::quote_footer('-- Galenson, D. W., & Weinberg, B. A. (2001). Creating modern art: The changing careers of painters in France from impressionism to cubism. American Economic Review, 91(4), 1063-1071.')`

<!-- <p style="margin-bottom:1cm;"></p> -->

What does *better* mean? In this chapter the research question is:

```{r, echo=FALSE, eval=FALSE}
tippy::tippy_this(elementId = "AER", tooltip = "AER is a famous and reliable economic journal.")
```

<center>
**What is the relationship between the age of an artist and his productivity?**
</center>

<!-- &nbsp; -->

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
A **research question** is 

- focused on a single issue, 
- specific enough to answer thoroughly 
- and feasible to answer within the given time frame or practical constraints 
- not to mention relevant to your field of study.   
::::

<!-- There are high-quality journal paper analyzing this question.  -->

<!-- Ekelund Jr, R. B., Jackson, J. D., & Tollison, R. D. (2015). Age and productivity: An empirical study of early American artists. Southern Economic Journal, 81(4), 1096-1116. -->

<!-- ::: {.infobox .question data-latex="warning"} -->
<!-- What exactly is productivity and how can we measure it?  -->
<!-- ::: -->

But what exactly is productivity and how can we measure it? To keep things simple we follow the literature and measure *productivity* via auction prices for paintings. That's a very *economic perspective* on art. 

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
**Operationalization** is the process of defining the measurement of a phenomenon that is not directly measurable.
::::

This is what we gonna explore:

<center>
**What is the relationship between the age of artists and auction price for their paintings?**
</center>

<!-- <p style="color:#808080">What is intelligence and how can we measure it? Perhaps with the intelligence quotient (IQ). Perhaps something else.</p> -->

<!-- <sup>What is intelligence and how can we measure it? Perhaps with the intelligence quotient (IQ),<sup>  -->

## Data is everywhere

<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- library("vembedr") -->
<!-- embed_url("https://vimeo.com/303322440") %>% -->
<!--   #use_start_time("1m32") %>% -->
<!--   use_align("center") -->
<!-- ``` -->

<!-- Thus, the higher paintings were selling in an auction (in million US dollar, inflation adjusted), the higher is the artist's productivity. That's not ideal, but the best approach that we have at this time. I select some of the top selling paintings from a Wikipedia list: -->

Researchers use auction price data for which they have to pay. We use free information from a Wikipedia [List of most expensive paintings](https://en.wikipedia.org/wiki/List_of_most_expensive_paintings) of all time. The auction prices are inflation-adjusted by consumer price index in millions of United States dollars in 2019. That's another interesting economic procedure, that we take for given at this analysis.

### Data in a table

<!-- https://www.askart.com/Search_Artist_Auction_Records.aspx -->
<!-- https://datatables.net/ -->

```{r warning=FALSE, message=FALSE, echo=FALSE}
#setwd("C:\\Users\\Marco2020\\Dropbox\\Viadrina\\Regression")
artists <- read.csv(file = 'data/Art/Artists.csv', sep=";", dec=",", encoding="UTF-8")
names(artists)[1] <- "Artist"
#names(artists)[4] <- "Age at Painting"
#names(artists)[5] <- "Age at Death"
#names(artists)[6] <- "Birth Year"
```

The table is created with the `DT` package in *datatable* format. This exploits the full potential of html documents, i.e. the data is searchable and sortable. The first rows are displayed, but in principle it can include the entire dataset. 

```{r warning=FALSE, message=FALSE, echo=FALSE}
DT::datatable(artists, rownames = FALSE)
```

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
**Tabular data** is common in data analysis. You can create a table in Word or Excel. 
::::

### Data in a graph

Two continuous variables are plotted in a **scatterplot**. The **x-axis** is called **<u id='age'>abscissa </u>** and the **y-axis** is called **<u id='price'>ordinate</u>**.

```{r, echo=FALSE}
tippy::tippy_this(elementId = "price", tooltip = "here: price")
tippy::tippy_this(elementId = "age", tooltip = "here: age")
```

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
images <- c("https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Thomas_Eakins%2C_American_-_Portrait_of_Dr._Samuel_D._Gross_%28The_Gross_Clinic%29_-_Google_Art_Project.jpg/95px-Thomas_Eakins%2C_American_-_Portrait_of_Dr._Samuel_D._Gross_%28The_Gross_Clinic%29_-_Google_Art_Project.jpg",  # CLINIC 
            "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Darmstadtmadonna.jpg/95px-Darmstadtmadonna.jpg", # Darmstadt Madonna
            "https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Rideau%2C_Cruchon_et_Compotier%2C_par_Paul_C%C3%A9zanne.jpg/95px-Rideau%2C_Cruchon_et_Compotier%2C_par_Paul_C%C3%A9zanne.jpg", # Rideau 
            "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Claude_Monet_-_Meules_%28W_1273%29.jpg/95px-Claude_Monet_-_Meules_%28W_1273%29.jpg" # Meules
         )
```

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
library(ggplot2)
ggplot(artists, aes(x = Age.at.Death, y = Price, label = Painting)) + 
  geom_point(size=2) + 
  #geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_text(aes(label=Painting),hjust=-0.05, vjust=0) +
  xlim(20, 120) + ylim(80, 160) +
  #ggtitle("The relationship between auction price and age of artist.") +
  xlab("Age at Death") + ylab("Auction price (in million US $)") + 
  theme_minimal()
```

Note that the axis beginning is not zero. The decision where axes start was made by the `ggplot` package for this data. Remember, every unit on the y-axis represents a million US dollars. Do we need to show the age between 0 and 20? How many famous artists died before 20 and sold paintings for a hundred million US dollars? 

<!-- https://stephanieevergreen.com/non-zero-axis-rules/ -->
<!-- http://stephanieevergreen.com/y-axis/ -->

:::: {.amazing}
::: {.titelamazing}
<h2> When It’s OK to NOT Start Your Axis at Zero. </h2>
:::
When the data really don’t fluctuate very much but a rise of small values like 1.4 or 1.4% is a big deal. With a graph that starts at zero, these changes can't be detected. The data scientist has to decide. 
::::

### The trend

The graph suggests a **positive trend** between price and age. There is an **increase** in price for older artists. The older the artist, the higher the auction prices.

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
library(ggplot2)
ggplot(artists, aes(x = Age.at.Death, y = Price, label = Painting)) + 
  geom_point(size=2) + 
  #geom_line() + 
  #geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_text(aes(label=Painting),hjust=-0.05, vjust=0) +
  geom_segment(aes(x = 50, y = 90, xend = 120, yend = 150),size = 3,
                  arrow = arrow(length = unit(0.5, "cm"))) +
  xlim(20, 120) + ylim(80, 160) +
  #ggtitle("The relationship between auction price and age of artist.") +
  xlab("Age at Death") + ylab("Auction price (in million US $)") + 
  theme_minimal()
```

### The blackbox

The mission is to find a mathematical function that describes the trend. In other words, we are looking for the black box that transforms the input into the output:

<center>
![](images/function-fx-x2.svg)
</center>

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
A **mathematical function** is an expression, rule, or law that defines a relationship between one variable (the **independent variable**, on the x-axis) and another variable (the **dependent variable**, on the y-axis).
::::

From looking at the graph, here are two suggestions: 

$$\begin{align}
\text{price} = 80 + 0.5 \cdot \text{age} \tag{Suggestion 1} \\
\text{price} = 90 + 0.2 \cdot \text{age} \tag{Suggestion 2} \\
\end{align}$$

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
A **linear function** is defined by two components, **intercept** (with the y-axis) and **slope**.
::::

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
library(ggplot2)
ggplot(artists, aes(x = Age.at.Death, y = Price, label = Painting)) + 
  geom_point(size=2) + 
  geom_abline(intercept = 80, slope = 0.5, color="lightgrey", linetype="dashed", size=1.5)+
  #geom_abline(intercept = 90, slope = 0.5, color="lightgrey", linetype="dashed", size=1.5)+
  geom_abline(intercept = 90, slope = 0.2, color="lightgrey", linetype="dashed", size=1.5)+
  geom_text(aes(label=Painting),hjust=-0.05, vjust=0) +
  #ggtitle("The relationship between auction price and age of artist.") +
  xlab("Age at Death") + ylab("Auction price (in million US $)") + 
  xlim(20, 120) + ylim(80, 160) +
  theme_minimal()
```

How can we compare the two suggested lines? Which linear function represents the relationship best?

### Nobody's perfect
<!-- https://drsimonj.svbtle.com/visualising-residuals -->

We all make mistakes. So do the linear functions:

$$ \begin{align}
\text{price} &= 80 + 0.5 \cdot \text{age} \tag{Suggestion 1} \\
102.5 &= 80 + 0.5 \cdot 45 \tag{Age for Holbein} \\
\end{align}$$

The equation tells (or predicts) that for any artist at the age of 45 it expects a auction price for a painting of 102.5 million US Dollar. Darmstadt Madonna was sold for 85 million US dollar. The linear function overestimated the true value. When you look at the graph, you see some predictions are more accurate (close to the true values) than others. All are either above or below the line. 

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
A **residual** (or error) is the vertical distance between the actual and the predicted value.
::::

<!-- The regression estimator is ordinary least squares. The overall goal is to minimize the squared residuals. But what's that? It's the error or vertical distance. Sometimes the blue line lies below or above the real observations. It hardly ever goes through one of them.  -->

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
fit <- lm(Price ~ Age.at.Death, data = artists)
artists$predicted <- predict(fit)   # Save the predicted values
artists$residuals <- residuals(fit) # Save the residual values

artists$predicted_guess = 80 + 0.5*artists$Age.at.Death
artists$residuals_guess = artists$predicted_guess - artists$Price

# COLOR
# High residuals (in abolsute terms) made more red on actual values.
ggplot(artists, aes(x = Age.at.Death, y = Price, label = Painting)) + 
  geom_point(size=2) + 
  geom_abline(intercept = 80, slope = 0.5, color="lightgrey", linetype="dashed", size=1.5)+
  geom_text(aes(label=Painting),hjust=-0.05, vjust=0) +
  #ggtitle("The relationship between auction price and age of artist.") +
  xlab("Age at Death") + ylab("Auction price (in million US $)") + 
  xlim(20, 120) + ylim(80, 160) +
  theme_minimal() +
  geom_segment(aes(xend = Age.at.Death, yend = predicted_guess), alpha = .2) +
  # > Color adjustments made here...
  geom_point(aes(color = abs(residuals_guess))) + # Color mapped to abs(residuals)
  scale_color_continuous(low = "black", high = "red") +  # Colors to use here
  guides(color = FALSE) +  # Color legend removed
  # <
  geom_point(aes(y = predicted_guess), shape = 1) 
```

### Vocab Wrap-Up

Let's wrap up regression vocab! Find an equation that describes the phenomenon of interest. Equation I shows a generic statistical model; equation II a generic linear model. 

$$ \begin{align}
\text{outcome} &= f(\text{explanatory}) + \text{noise} \tag{I} \\
\text{outcome} &= \text{intercept} + \text{slope} \cdot \text{explanatory} + \text{noise} \tag{II} \\
\end{align} $$

A **regression model** is suggested by the researcher. A more concrete regression model looks like this:

$$Y = \beta_1 + \beta_2 X + \epsilon$$

A model can be easy or complicated. It definitely contains variables and parameters. 

- **Variables**: Things that we measure (or have data).
- **Parameters**: Constant values we believe to represent some fundamental truth about the relationship between the variables. 

The calculation is called an **estimation**. In textbooks the same equation can be found with hats:

$$	 \widehat{Y}  = \widehat{\beta}_1 + \widehat{\beta}_2 \cdot X $$
$\widehat{Y}$ are called the **fitted or predicted** values. $\widehat{\beta}$ are **regression coefficients** (this is the estimate of the unknown population parameter). As we have seen in the graph before, the differences between the actual and the predicted values are the residuals $e = Y - \widehat{Y}$.

The fitting procedure is called **ordinary least squares** (OLS). 


## For the truly dedicated 

<!-- div { -->
<!--   background-color: lightblue; -->
<!-- } -->

<!-- This is a cool graphic -->
<!-- https://www.quora.com/Why-is-the-coefficient-of-determination-never-used-in-Machine-Learning -->

<!-- ::: {.infobox .information data-latex="warning"} -->
<!-- Now we have all the ingredients we need. <details> -->
<!-- <summary>*Let's get this party started!*</summary> -->
<!-- ![Carlton Dance](https://media1.tenor.com/images/6519b006f53708454922375a82c23682/tenor.gif?itemid=15161860) -->
<!-- </details> -->
<!-- ::: -->

The overall goal is to make as little as possible mistakes! What kind of mistake? The deviation from the observed values! What could come to your mind is to **minimize the sum of all errors**: 

$$\sum e \rightarrow \min$$
But wait, there is more. Is it fair to say that the sum should be small? Compare **The Scream** and **Meules**, their deviations are $+17.5$ and $-13.6$ (very similar). So taken these two together, there's almost not mistake! That is to say, positive and negative deviations cancel each other out. Thus we need one more twist in the story: 

$$\sum e^2 \rightarrow \min$$
The goal of OLS is to **minimize the residual sum of squares** or the **sum of squared residuals**.

### Algebra 

:::: {.amazing}
::: {.titelamazing}
<h2> Amazing Fact </h2>
:::
Algebra comes from Arabic, meaning "reunion of broken parts".
::::

<!-- We often work with matrices and vectors.  -->

Let's introduce matrix notation. We began with $X$ and $Y$ being *variables* in the equation:

$$Y = \beta_0 + \beta_1 X + \epsilon $$
<!-- Each data point is a combination of values. -->

We turn this into: 

$$y = X \beta + \epsilon $$
Capital letters like $X$ represent a matrix (a table with rows and column), and small letters like $y$ and $e$ represent vectors. Since there are 6 observations and two parameters in our model we get:

$$ \begin{align}
\begin{pmatrix} Y_1 \\ Y_2 \\ Y_3 \\ Y_5 \\ Y_6 \end{pmatrix} &= \begin{pmatrix} 1 & X_{11} \\ 1 & X_{12} \\ 1 & X_{13} \\ 1 & X_{14} \\ 1 & X_{15} \\ 1 & X_{16}   \end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \\ \epsilon_6 \\ \end{pmatrix} 
\end{align}$$

Let's turn to the goal, minizing the residual sum of squares (RSS). By convention, the *normal* version of a vector is a vertical list of numbers in big parentheses (i.e. a column vector). To *transpose* a vector means change between the row and column format. Squaring a vector thus means the row version of the vector times the column version of the vector:

$$\sum e^2 = e^T \cdot e \rightarrow \min$$
Notice that the sum operator is gone. [Matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication){target="_blank"} requires multiplying all elements pairwise with each other and summing them up. Plug in the residuals $e = y - X \beta$ in the equation:

$$\begin{align}
	\sum e^2 &= e^T \cdot e \\
	&= (y - X \beta )^T (y - X \beta) \tag{$(A+B)^T = A^T + B^T$}\\
	&= (y^T - X^T \beta^T) (y - X \beta) \\
	&= y^T y - y^T X \beta - X^T \beta^T y + X^T \beta^T X \beta \\
	&= y^2 \underbrace{- 2 \beta^T X^T y}_{??} + \beta^2 X^2  \\
\end{align}$$

Did you notice what happened in the middle? The transpose of the first term is equal to the second: 

$$\begin{align}
	(y^T X \beta)^T = y X^T \beta^T
\end{align}$$

### Analysis 

:::: {.amazing}
::: {.titelamazing}
<h2> Amazing Fact </h2>
:::
From Medieval Latin, analysis means "resolution of anything complex into simple elements" (opposite of synthesis).
::::

<!-- *Functions* are key to analysis. -->

Next, we are ready to **optimize**. Optimization (in math and economics) is done by **differentiation**:

<!-- (setting the first derivative equal to zero). What are the betas? -->

$$\begin{align}
	\frac{\partial RSS}{\partial \beta} &= -2 X^T y + 2 \beta X^T X = 0 \tag{first derivative = zero} \\
	2 \beta X^T X &= 2 X^T y \tag{rearrange terms}\\
	\beta X^T X &= X^T y \tag{the "normal equation"} \\
	\beta &= (X^T X)^{-1} X^T y \tag{Bam}\\
\end{align}$$

Those $\beta$ coefficients are the first and most important regression results. Retrieve them step by step to enhance your understanding of the math and coding as the same time.

### Take the Long Way Home 

<!-- ::: {.infobox .information data-latex="warning"} -->
<!-- <details> -->
<!-- <summary>*The computer does the magic for us.*</summary> -->
<!-- <center> -->
<!-- ![Magic Alice](https://thumbs.gfycat.com/RemorsefulFairIndianpangolin-max-1mb.gif) -->
<!-- </center> -->
<!-- </details> -->
<!-- ::: -->

```{r echo=FALSE, warning=FALSE}
y <- as.vector(artists$Price)
X <- as.matrix(cbind(rep(1, nrow(artists)), artists$Age.at.Death))
```

First, retrieve matrix $X$ from the data set:

```{r echo=TRUE, warning=FALSE}
X
```

Second, the transpose of $X$ has two rows and six columns (use `t()`):

```{r echo=TRUE, warning=FALSE}
t(X)
```

Next, calculate the square of the matrix (transpose times original):

```{r echo=TRUE, warning=FALSE}
t(X)%*%X
```

The inverse of the matrix product can be calculated by `solve()`:

```{r echo=TRUE, warning=FALSE}
solve(t(X)%*%X)
```

Next, multiply the inverse with the transpose from the right:

```{r echo=TRUE, warning=FALSE}
solve(t(X)%*%X) %*% t(X)
```

Finally, multiply the vector $y$:

```{r echo=TRUE, warning=FALSE}
solve(t(X)%*%X) %*% t(X) %*% y
```

It's the $\beta$ vector! The first entry is the **intercept** and the second is the **slope** of the linear function. The following graph shows the line created from intercept and slope in a scatter plot. 

<!-- There are two numbers. -->

## Survival of the Fittest Line 

The linear equation that best describes the data is this: 

$$Price = 22.3452 + 1.1657 \cdot Age$$

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
fit <- lm(Price ~ Age.at.Death, data = artists)
artists$predicted <- predict(fit)   # Save the predicted values
artists$residuals <- residuals(fit) # Save the residual values

# High residuals (in absolute terms) made more red on actual values.
ggplot(artists, aes(x = Age.at.Death, y = Price, label = Painting)) + 
  geom_point(size=2) + 
  geom_smooth(method="lm", se=FALSE, color="darkgreen") +
  #geom_abline(intercept = 80, slope = 0.5, color="lightgrey", linetype="dashed", size=1.5)+
  geom_text(aes(label=Painting),hjust=-0.05, vjust=0) +
  #ggtitle("The relationship between auction price and age of artist.") +
  xlab("Age at Death") + ylab("Auction price (in million US $)") + 
  xlim(20, 120) + ylim(80, 160) +
  theme_minimal() +
  geom_segment(aes(xend = Age.at.Death, yend = predicted), alpha = .2) +
  # > Color adjustments made here...
  geom_point(aes(color = abs(residuals))) + # Color mapped to abs(residuals)
  scale_color_continuous(low = "black", high = "red") +  # Colors to use here
  guides(color = FALSE) +  # Color legend removed
  # <
  geom_point(aes(y = predicted), shape = 1) 
```

## On the Shoulders of Giants 

Fortunately, we are [standing on the shoulders of giants](https://en.wikipedia.org/wiki/Standing_on_the_shoulders_of_giants). Clever people implemented the linear regression (command `lm()`) and all kinds of regressions and statistical tests in R. 

```{r echo=TRUE, warning=FALSE}
lm(Price ~ Age.at.Death, data = artists)
```

The workhorse packs up work. 

```{r echo=F, warning=FALSE, fig.align='center'}
knitr::include_graphics("images/workhorse.jpg")
```

