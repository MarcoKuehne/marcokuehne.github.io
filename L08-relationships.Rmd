# (PART) Models {-}

# Relationships

> “[M]y ally is the Force, and a powerful ally it is. Life creates it, makes it grow. Its energy surrounds us, binds us. Luminous beings are we, not this crude matter. You must feel the Force flow around you. Here, between you, me, the tree, the rock, yes, even between the land and the ship.”</b>
>
> `r tufte::quote_footer('Yoda. -- Episode V: The Empire Strikes Back')`

The most interesting research questions in social science are about relationships. What is the relationship between the land and the ship? Is is the force? How can the relation between the tree and the land be made palpable?

A relationship is the way in which two or more variables are connected. We pose that everything that can be measured. Relationships between two (continuous) variables are quantified via *covariance* or *correlation* and may be illustrated in a scatter plot. 

<!-- Recall that there is a statistical relationship between two variables when the average score on one differs systematically across the levels of the other. In this section, we revisit the two basic forms of statistical relationship introduced earlier in the book—differences between groups or conditions and relationships between quantitative variables—and we consider how to describe them in more detail. -->

<!-- https://plato.stanford.edu/entries/relations/ -->

<!-- go together flow together -->


## Variance

This is <s>Sparta</s> data. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
x=c(4,13,19,25,29)
y=c(10,12,28,32,38)
```

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **variance** is defined as the **average quadratic deviation from the mean**.
::::

$$var(x) = \frac{1}{n-1} \sum (x_i - \overline{x} )^2$$

The <u id='variance'>variance</u> of `x` is `r var(x)` and can be calculated via `var()` in R. 

```{r, echo=FALSE}
tippy::tippy_this(elementId = "variance", tooltip = "The term variance was first introduced by Ronald Fisher in his 1918 paper *The Correlation Between Relatives on the Supposition of Mendelian Inheritance*.")
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Built-in command
var(x)
```

<!-- The variance of `x` can be calculated from its components in R as a combination of `sum()`, `mean()` and `length()`. -->

<!-- ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # Variance by hand -->
<!-- sum( (x-mean(x))^2 )/(length(x) - 1)  -->
<!-- ``` -->

The standard deviation is derived from variance and tells, on average, how far each value lies from the mean. It’s the square root of variance. Variance and standard deivation measure the variability of a variable.

<!-- https://www.scribbr.com/statistics/variance/ -->

:::: {.dedicated}
::: {.titeldedicated}
<h2> Truly Dedicated </h2>
:::
When you have collected data from every member of the population that you’re interested in, you can get an exact value for population variance. When you collect data from a sample, the sample variance is used to make estimates or inferences about the population variance.
::::

<!-- http://stla.overblog.com/schematizing-the-variance-as-a-moment-of-inertia -->
<!-- https://stats.stackexchange.com/questions/72208/visualising-the-variance -->

The one-dimensional variable `x` is visualized as points on a line. The mean of `x` is `r mean(x)` (red bold line). The standard variation of `x` is `r sd(x)` and surrounds the mean. 

```{r, echo=F, eval=T, message=FALSE, warning=FALSE, out.width = '80%', fig.align="center"}
library(ggplot2)
library(tidyverse)

x=c(4,13,19,25,29)
y=c(10,12,28,32,38)
data <- data.frame(x, y) 

ggplot(aes(x=x, y=c(0)), data=data) + 
  geom_point(size=3) +
  labs(title="Distribution of variable x around the mean") +
  geom_vline(xintercept=mean(data$x), size=1.5, color="red") + 
  theme_minimal() +
  theme(axis.line=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        plot.background=element_blank()) +
  geom_segment(aes(x = mean(data$x), 
                   y = 0.025, 
                   xend = mean(data$x) + sd(data$x), 
                   yend = 0.025),
               size = 1,
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(aes(x = mean(data$x), 
                   y = -0.025, 
                   xend = mean(data$x) - sd(data$x), 
                   yend = -0.025),
               size = 1,
               arrow = arrow(length = unit(0.5, "cm"))) + ylim(-0.05, 0.05) +
  geom_label(aes(x=mean(data$x) + sd(data$x), y=0.035, label = "+ 1 SD")) +
  geom_label(aes(x=mean(data$x) - sd(data$x), y=-0.035, label = "- 1 SD")) + 
  geom_label(aes(x=mean(data$x), y=0.035, label = "mean"), color="red")
```

<!-- https://stats.stackexchange.com/questions/311174/what-does-it-mean-when-three-standard-deviations-away-from-the-mean-i-land-ou -->

:::: {.dedicated}
::: {.titeldedicated}
<h2> Truly Dedicated </h2>
:::
In statistics, the empirical rule states that 99.7% of data occurs within three standard deviations of the mean within a normal distribution. To this end, 68% of the observed data will occur within the first standard deviation, 95% will take place in the second deviation, and 97.5% within the third standard deviation.
::::

The *scatterplot* is a two-dimensional instrument. It shows the original information from `x` and `y` and their respective means as bold red lines. 

```{r, echo=F, eval=TRUE, out.width = '80%', fig.align="center"}
## GGplot
ggplot(aes(x=x, y=y), data=data) + 
  geom_point(size=3) +
  labs(title="Distribution of x and y around their means") +
  geom_vline(xintercept=mean(x), size=1.5, color="red") + 
  geom_hline(yintercept=mean(y), size=1.5, color="red") +
  theme_minimal()
```

## Covariance  

Covariance is a measure of the joint variability of two variables. The main idea of covariance is to classify three types of relationships: positive, negative or no relationship. For each data point, we multiply the differences with the respective mean. When both values are smaller or greater than the mean, the result will be positive.

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **covariance** between two variables is the **product of the deviations of x and y from their respective means**. 
::::

$$cov(x,y) = \frac{1}{n-1} \sum\limits (x_i - \bar{x})(y_i - \bar{y})$$ 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Built-in command
cov(x,y)
```

<!-- The covariance of `x` can be calculated from its components in R as a combination of `sum()`, `mean()` and `length()`. -->

<!-- ```{r, message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE} -->
<!-- # Covariance by hand -->
<!-- (sum((x-mean(x))*(y-mean(y)))) / (length(x)-1) -->
<!-- ``` -->

<!-- https://evangelinereynolds.netlify.app/post/geometric-covariance/ -->
<!-- https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean -->

Now we turn to the visualization of the covariance. For each data point, we multiply the differences with the respective mean. This results in several rectangular areas starting at the intersection of means as a new origin. The covariance sums up all these areas.

```{r, echo=F, eval=TRUE, message=FALSE, warning=FALSE, out.width = '80%', fig.align="center"}
library(ggplot2)

simple.arrow <- arrow(length = unit(0.2, "cm"))

ggplot(aes(x=x, y=y), data=data) + 
  geom_point(size=1) +
  labs(title="Distribution of x and y around their means") +
  theme_bw() +
  geom_vline(xintercept=mean(x), size=1.5, color="red") + 
  geom_hline(yintercept=mean(y), size=1.5, color="red") +
  geom_segment(aes(y = mean(y), xend = x, yend = y, colour=y>mean(y)), arrow = simple.arrow) +
  geom_segment(aes(x = mean(x), xend = x, yend = y, colour=x>mean(x)), arrow = simple.arrow) +
  geom_rect(aes(
    xmin = ifelse(x > mean(x), mean(x), x),
    xmax = ifelse(x > mean(x), x, mean(x)),
    ymin = ifelse(y > mean(y), mean(y), y),
    ymax = ifelse(y > mean(y), y, mean(y)),
    fill = (x-mean(x)) * (y-mean(y)) > 0
  ),
  alpha = 0.1) +
  geom_point() +
  theme_minimal() + 
  geom_label(aes(x=10, y=30, label = "Sum of all five squares: 470")) +
  theme(legend.position="none")
```

:::: {.challenge}
::: {.titelchallenge}
<h2> Your Turn </h2>
:::
Validate the covariance result from `cov(x,y)` by mental calculation.
::::

## Correlation 

Covariance quantifies a relationship and is similar to correlation. Covariance is expressed in units that vary with the data. Because the data are not standardized, you cannot use the covariance statistic to assess the strength of a linear relationship (a covariance of 117.5 can be very low for one relationship and 0.23 very high for another relationship).

To assess the strength of a relationship between two variables a correlation coefficient is commonly used. It brings variation to a standardized scale of 1 to +1.

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **correlation coefficient** is a statistical measure of the strength and direction of the relationship between two variables.
::::

$$r(x,y) = \frac{\sum\limits (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum\limits (x_i - \bar{x})^2 \sum\limits (y_i - \bar{y})^2}}$$

Does the numerator and denominator remind you of something? The formula is made of the components variance and covariance. Thus, the correlation coefficient formula is often expressed in short as:

$$r(x,y,) = \frac{Cov(x,y)}{\sqrt{Var(x) Var(y)}}$$

`cor()` is a basic function to calculate the correlation coefficient. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Basic function
cor(x,y)
```

`cor.test()` is a more sophisticated version including a hypothesis test.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# More advanced function 
cor.test(x,y)
```

<!-- ## Statistical significance: p<0.05 (or rule of thumb t>2) -->
<!-- ## t value is 5.6757 -->
<!-- ## p value is 0.01084  -->

<!-- ::: {.infobox2 .information data-latex="warning"} -->
<!-- The **correlation coefficient** is the preferred way to describe a linear relationship.  -->
<!-- ::: -->

The correlation test is based on a t-value (t = `r cor.test(x,y)$statistic`) and returns a p-value (`r cor.test(x,y)$p.value`) for statistical significance. 

<!-- Please look up the exact formula to produce this t- and p-value and code it in R (each 1 pt).  -->

<!-- Hint: For the t-value you can use a combination of `cor()` and `sqrt()`. For the p-value you can use the `pt()`.  -->

<!-- ```{r, echo=TRUE, message=FALSE, warning=FALSE} -->
<!-- ## Calculate t-value for correlation -->
<!-- t <- cor(x,y) * sqrt(3) / sqrt(1-cor(x,y)^2)  -->
<!-- t -->
<!-- ## Calculate p-value for correlation -->
<!-- ## https://stackoverflow.com/questions/46186115/calculating-p-values-for-given-t-value-in-r -->
<!-- ## needs: t, df and pt (Student t cumulative Distribution) -->
<!-- p <- (1 - pt(q = abs(t), df = 3))*2 -->
<!-- p -->
<!-- ``` -->


<!-- https://stats.stackexchange.com/questions/32464/how-does-the-correlation-coefficient-differ-from-regression-slope -->

There is an awesome connection from correlation coefficient to the simple regression coefficient.

:::: {.amazing}
::: {.titelamazing}
<h2> Amazing Fact </h2>
:::
The correlation coefficient and the simple regression coefficient coincide when the two variables are on the same scale. The most common way of achieving this is through standardization.  

$$\beta = cor(Y,X) \cdot \frac{SD(Y)}{SD(X)} $$

:::

Here is the replication:

```{r, echo=TRUE}
# The data
df = data.frame(x=c(4,13,19,25,29), y=c(10,12,28,32,38))

# The correlation coefficient 
cor_coef <- cor(df$x, df$y, method="pearson")
cor_coef
```

The correlation coefficient is `r cor.test(x,y)$statistic`.

```{r, echo=TRUE}
# The regression coefficient 
linear_model <- lm(y~x, data=df)
reg_coef <- linear_model$coefficients[2]
reg_coef
```

The regression coefficient of `x` is `r linear_model$coefficients[2]`. Here is the connection:

```{r, echo=TRUE}
# The connection
cor_coef * sd(y) / sd(x)
```

Alternatively standardize the data first, then calculate correlation and regression: 

```{r, echo=TRUE}
# The connection
df_scaled <- as.data.frame(scale(df, center = TRUE, scale = TRUE))

# The correlation on standardized variables
cor(df_scaled$x, df_scaled$y, method="pearson")
```

```{r, echo=TRUE}
# The regression on standardized variables
lm(y~x, data=df_scaled)$coefficients[2]
```







