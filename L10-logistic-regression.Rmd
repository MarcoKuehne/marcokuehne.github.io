# Logistic Regression 

Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).

## Think inside the box 

<!-- https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning -->

The table contains data on the relationship between *hours of study* and the *outcome of an exam* (pass/fail) sorted ascending for study hours. The `pass` outcome is coded `1` for "Yeah, I passed the exam" and `0` for "I'll be back". 

```{r, eval=TRUE, echo=F, warning=FALSE, message=FALSE}
library(tidyverse)
library(DT)
# mydata <- data.frame(pass = c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1),
#                      hours = c(0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,
#                                2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50))

mydata <- data.frame(hours = c(0.75,1.00,1.50,1.75,1.75,2.00,2.50,
                               2.75,3.00,3.25,4.25,5.00),
                     pass = c(0,0,0,0,1,0,0,1,0,1,1,1))
mydata$pass <- as_factor(mydata$pass)

datatable(t(mydata), 
              options = list(lengthChange = FALSE,
                             pageLength = 12,
                             dom = 't', 
                             rownames = FALSE,
                             headerCallback = JS(
              "function(thead, data, start, end, display){",
              "  $(thead).remove();",
              "}"))) %>% 
  formatStyle(columns = colnames(.), fontSize = '50%')

# library(knitr)
# library(kableExtra)
# as.data.frame(t(mydata)) %>% kable(col.names = NULL) %>% kable_styling(font_size = 8)
``` 

<!-- Please do a side-by-side plot via `ggplot` and `gridExtra` package with `pass` on the y-axis and `hours` of study on the x-axis. Please fit a line though the left panel and a squiggle through the right panel. Use the layer `geom_smooth` with `method` option either `lm` (linear model) or `glm` (general linear model).  -->

Linear regression is agnostic to the structure of the data. `lm()` fits a linear model to the data. Logistic regression acknowledges the floor and ceiling of values between 0 and 1. It squeezes the line to a squiggle inside the box. Well, the x-axis don't necessarily have this restriction. 

```{r, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
mydata <- data.frame(pass = c(0,0,0,0,1,0,0,1,0,1,1,1),
                     hours = c(0.75,1.00,1.50,1.75,1.75,2.00,2.50,
                               2.75,3.00,3.25,4.25,5.00))

library(gridExtra)
plot1 <- ggplot(mydata, aes(x=hours, y=pass)) +
  geom_point(size=2) + 
  geom_smooth(method="lm", se=FALSE) + ylim(-0.1,1.05) +
  labs(title= "Linear Regression")+ xlab("Hours of Study") + 
  theme_minimal()

plot2 <- ggplot(mydata, aes(x=hours, y=pass)) +
  geom_point(size=2) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se=FALSE) +
  ggtitle("Logistic Regression") + xlab("Hours of Study") +
  geom_segment(aes(x = 0, y = -0.03, xend = 6, yend = -0.03),size = 1.5) + # under
  geom_segment(aes(x = -0.03, xend = -0.03, y = 0, yend = 1),size = 1.5) + # left
  geom_segment(aes(x = 0, y = 1.03, xend = 6, yend = 1.03),size = 1.5) + # upper 
  geom_segment(aes(x = 6.03, xend = 6.03, y = 0, yend = 1),size = 1.5) + # right
  ylim(-0.1,1.05) +
  theme_minimal()

grid.arrange(plot1, plot2, ncol=2)
```

<!-- Why do we take all the trouble doing the transformation from probability to log odds?  One reason is that  -->

It is usually difficult to model a variable which has restricted range, such as probability. Get around the restricted range problem with a transformation. 

## From probability to odds 
<!-- https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/ -->
<!-- https://www.statisticshowto.com/log-odds/ -->
<!-- Odds and Log Odds: https://www.youtube.com/watch?v=ARfXDSkQf1Y -->

Everybody has a good sense for probability. Not so for odds. 

- Let’s say that the probability to pass the exam is $0.8 = 80\%$.
- Then the probability of failure is $1 – 0.8 = 0.2 = 20\%$. 

The odds of passing the exam are defined as the ratio of the probabilities: $\text{odds} = \frac{0.8}{0.2} = 4$. That is to say that the odds of passing are 4 to 1. Odds provide a measure of the likelihood of a particular outcome. 

<!-- Please do the following: -->
<!-- - Create a data.frame called `data`. -->
<!-- - Within `data`, create a sequence of values ranging from 0 to 1 in 0.01 steps and store it as `p` (probability). -->
<!-- - Within `data`, calculate the corresponding odds, store as `odds`.  -->
<!-- - Create a simple plot showing the relation between odds and probability using the `plotly` package.  -->

Probability is naturally restricted between 0 and 1, what about odds? Odds range from 0 to positive infinity.

```{r eval=TRUE, echo=F, message=FALSE, warning=FALSE}
p <- seq(from=0, to=1, by=0.01)
odds <- p/(1-p)
data <- data.frame(p, odds)

# library(plotly)
# plot_ly(data = data, x = ~p, y = ~odds)

ggplot(data, aes(x=p, y = odds)) + geom_point() + theme_minimal()
```

The transformation from probability to odds is a monotonic transformation, meaning the odds increase as the probability increases or vice versa. 

## From odds to log of odds 

<!-- Please calculate the log of odds, store it as `logodds` (can be another object). Create a side-by-side graph showing the relation between probability and odds and probability and logodds with the `plotly` package.  -->

The `log()` of values between 0 and 1 is negative, above 1 positive. Logodds range between negative and positive infinity. Range problem solved. 

```{r, eval=TRUE, echo=F}
data$logodds <- log(data$odds)

#library(plotly)
# fig1 <- plot_ly(data = data, x = ~p, y = ~odds)
# fig1 <- fig1 %>% add_lines(name = ~"odds")
# fig2 <- plot_ly(data = data, x = ~p, y = ~logodds)
# fig2 <- fig2 %>% add_lines(name = ~"logodds")

fig1 <- ggplot(data, aes(x=p, y = odds)) + geom_point() + theme_minimal()
fig2 <- ggplot(data, aes(x=p, y = logodds)) + geom_point() + theme_minimal()

grid.arrange(fig1, fig2, ncol=2)
```

Imagine you flip the axes, doesn't this look like the initial S curve we were looking for?

<!-- https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_simple_logistic_logodds.htm -->
<!-- https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_log_or_ln.htm -->
<!-- https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_simple_logistic_coefficients.htm -->
<!-- http://faculty.cas.usf.edu/mbrannick/regression/Logistic.html -->
<!-- Logistic regression estimation is based on the concept of Maximum Likelihood estimation.  -->

## Titanic Survival 

<!-- https://www.freevectors.net/titanic-vector-image-37720 -->

```{r, out.width="50%", fig.cap="\\label{fig:titanic}RMS Titanic.", fig.align='center', eval=T, echo=F}
knitr::include_graphics("images/titanic.png")
```

<!-- <div style="background-image:url("images/titanic.png"); opacity:0.5;"> -->
<!-- </div> -->

> The sinking of the Titanic is one of the most infamous shipwrecks in history.
> On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.
> While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.
>
> `r tufte::quote_footer('--- [Kaggle Competition](https://www.kaggle.com/competitions/titanic/overview)')`

### The Data 

<!-- Please register at Kaggle. Go to <https://www.kaggle.com/c/titanic/data> and download the datasets (`train.csv`, `test.csv`, `gender_submission.csv`). Put them in your working directory. The following merge commands should work.  -->

Load a comprehensive dataset on Titanic. 

```{r, eval=TRUE, echo=TRUE}
titanic <- read.csv("https://raw.githubusercontent.com/MarcoKuehne/marcokuehne.github.io/main/data/titanic.csv")
titanic <- titanic[,-1]
```

Let's check the number of missing values per variable: 

```{r, eval=TRUE, echo=TRUE}
## missings per variable 
sapply(X = titanic, FUN = function(x) sum(is.na(x)))
```

There are 2 missing values in `Embarked` and 263 in `Age`. The most missings fall upon `Cabin` though. Cabin information is not a very useful predictor for survival, thus we can remove this information for our analysis. We conduct a *complete case analysis*. The dataset should look like this (n = 1043): 

```{r, eval=TRUE, echo=TRUE}
titanic <- titanic %>% 
  select(!c(Cabin, PassengerId, Name, Ticket)) %>% 
  filter(complete.cases(.))

DT::datatable(titanic, rownames = FALSE)
```

### No Predictor Variables

In an empty model, there is only the intercept and no predictor:

$$logit (p) = \beta_0$$ 

```{r, eval=TRUE, echo=TRUE}
library(tidyverse)
logistic0 = glm(Survived ~ 1, family = binomial(link = 'logit'), data = titanic)

library(modelsummary)
modelsummary(list("Survival" = logistic0), 
             title = 'Empty Model.',
             gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.')
```
What does $-0.414$ represent? 

$$logit (p) = \log \frac{p}{1-p} = \beta_0 = -0.414 $$ 

What is *p* here? The overall probability of survival (`Survived` = 1).  Let’s take a look at the frequency table for survival.

```{r, eval=T, echo=T}
table(titanic$Survived)
```

- $p = 415/1043 = 0.3978907$ is the share of survivors. 
- $0.3978907 / (1-0.3978907) = 0.660828$ is the odds of surviving.
- $log(0.660828) = -0.4142617$ is the logodds of surviving. 

In logistic regression models regression coefficients are reported in log odds.

<!-- This means \( log (1/(1-p)) = \) `r round(logistic0$coefficients,2)`.  -->

<!-- Please recalculate the intercept from model `logistic0`, i.e. the value `r logistic0$coefficients`. -->

<!-- Use the probability of survival from `titanic` data (check the count of `titanic$Survived`). Store and show the probability of survival in a vector named `survive_p`. Store and show the odds of survival as `survive_odds`. Calculate and store the logodds of survival as `survive_logodds` (this should be equivalent to the empty model intercept).  -->

<!-- From probability, calculate odds and logodds (this should be equivalent to the empty model intercept).  -->
<!-- Let's take a look at the frequency table for survival and derive this value step by step: -->

```{r, eval=F, echo=F}
##table(titanic$Survived)
##table(titanic$Survived)[1]
##table(titanic$Survived)[2]

survive_p <- table(titanic$Survived)[2]/(table(titanic$Survived)[2]+table(titanic$Survived)[1]) 
survive_odds <- survive_p/(1-survive_p)
survive_logodds <- log(survive_odds)

## ## 38.9% survived
## 398/(398+623)
## ## The odds are
## 0.3898139/ (1-0.3898139)
## ## 0.6229509 and the log of the odds (logit) is
## log(0.6388443)
## ## -0.4480945, exactly what the regression intercept is
```

### Single Dichotomous Predictor

We include gender as an explanatory variable and expect being male reduces the probability of surviving (<u id='birkenhead'>Women and children first</u>).^[https://en.wikipedia.org/wiki/Women_and_children_first]

```{r, echo=FALSE}
tippy::tippy_this(elementId = "birkenhead", tooltip = "known to a lesser extent as the Birkenhead drill")
```

> “Passengers’ chances of surviving the sinking of the S.S. Titanic were related to their sex and their social class: females were more likely to survive than males, and the chances of survival declined with social class as measured by the class in which the passenger travelled.”</b>
>
> `r tufte::quote_footer('@hall1986social')`

$$logit (p) = \beta_0 + \beta_1 \cdot sex$$  

```{r, eval=TRUE, echo=TRUE}
logistic1 = glm(Survived ~ Sex, family = binomial(link = 'logit'), data = titanic)

modelsummary(list("Empty model" = logistic1), gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F')
```

The dummy variable behaves as in the linear model in the sense that it either adds to the intercept when it is 1 or does not add anything when it is 0. 

- The logodds of survival for women are: `r logistic1$coefficients[1]`
- The logodds of survival for men are: `r logistic1$coefficients[1] + logistic1$coefficients[2]`

We calculate the logodds for men based on the frequencies of male and female survival: 

```{r, eval=TRUE, echo=TRUE}
table(titanic$Survived, titanic$Sex)
```

In our dataset, what are the odds of a male being in the survival class and what are the odds of a female being in the survival class?

- The odds for males are 93 to 564 is `r 93/564`.
- The odds for females are 322 to 64 `r 322/64`.

The odds female are about 30 times higher than the odds for males. The *ratio of the odds* for female to the odds for male is 

$$odds ratio^{female} = \frac{odds^{female}}{odds^{male}} = \frac{\frac{322}{64}}{\frac{93}{564}} = \frac{322}{64} \frac{564}{93} = \frac{7567}{248} = 30.5121.$$
Where the odds ratio for males to survive is:

$$odds ratio^{male} = \frac{odds^{male}}{odds^{female}} = \frac{\frac{93}{564}}{\frac{322}{64}} = \frac{1}{30.5121} = 0.03277388.$$


We can get this figure more easily by exponentiating the regression coefficient, since: 

$$e^{-3.418} = 0.03277793$$

```{r, eval=TRUE, echo=FALSE}
structure(
  paste(
    "<div style='display:flex; flex-wrap: wrap;'>",
    "<div style='flex-basis: 50%;'>",
    modelsummary(list("Survival" = logistic1), 
             title = 'Default Coefficients.',
             gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F',
             exponentiate = FALSE),
    "</div>",
    "<div style='flex-basis: 50%;'>",
    modelsummary(list("Survival" = logistic1), 
             title = 'Odds ratio coefficients.',
             gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F',
             exponentiate = TRUE),
    "</div>",
    "</div>"
  ),
  class = "kableExtra"
)
```



:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
**Odds ratio** (OR) is a statistical measure used to describe the strength of the association between two events, such as the presence of a risk factor (being male) and the occurrence of the outcome (i.e. survival).
::::

We interpret OR as follows. An odds ratio greater than 1 indicates that the event is more likely to occur in the exposed group than in the non-exposed group, while an odds ratio less than 1 indicates that the event is less likely to occur in the exposed group than in the non-exposed group.



```{r, eval=F, echo=F}
## odds ratio: odds for female to the odds for male: 2.876543/0.2329059 = 12.35067 == (233*468)/(81*109)
## the odds for females are about 1235% higher than the odds for males

## odds ratio: odds for male to the odds for female: 0.2329059/2.876543 = 0.08096729
## the odds for male are about 8% of the odds for females

## The coefficient for female is the log of odds ratio between the female group and male group: log(12.35067) = 2.51371
## The coefficient for male is the log of odds ratio between the male group and female group: log(0.08096729) = -2.51371

###### titanic full data
table(titanic$Survived, titanic$Sex)

## male survival: (~ 14%)
table(titanic$Survived, titanic$Sex)[2,2]/(table(titanic$Survived, titanic$Sex)[2,2] + table(titanic$Survived, titanic$Sex)[1,2])

## ## female survival: (~ 83%)
table(titanic$Survived, titanic$Sex)[2,1]/(table(titanic$Survived, titanic$Sex)[2,1] + table(titanic$Survived, titanic$Sex)[1,1])

## male odds:
table(titanic$Survived, titanic$Sex)[2,2] / table(titanic$Survived, titanic$Sex)[1,2]

## female odds:
table(titanic$Survived, titanic$Sex)[2,1] / table(titanic$Survived, titanic$Sex)[1,1]

## logodds male
log(table(titanic$Survived, titanic$Sex)[2,2] / table(titanic$Survived, titanic$Sex)[1,2])
## logodds female
log(table(titanic$Survived, titanic$Sex)[2,1] / table(titanic$Survived, titanic$Sex)[1,1])

## odds ratio: odds for male to the odds for female: 0.03277389
(table(titanic$Survived, titanic$Sex)[2,2] / table(titanic$Survived, titanic$Sex)[1,2])/(table(titanic$Survived, titanic$Sex)[2,1] / table(titanic$Survived, titanic$Sex)[1,1])

## odds ratio: odds for female to the odds for male: 30.5121
(table(titanic$Survived, titanic$Sex)[2,1] / table(titanic$Survived, titanic$Sex)[1,1])/(table(titanic$Survived, titanic$Sex)[2,2] / table(titanic$Survived, titanic$Sex)[1,2])
```

<!-- The coefficient for male is the log of odds ratio between the male group and female group: -3.465955 -->
<!--  log(0.03124315) -->
 
<!-- https://data.library.virginia.edu/visualizing-the-effects-of-logistic-regression/ -->

### Effect plot 

We use a nested version of `plot()` on `allEffects()` on the `logistic1` model (`effects` package) to visualize the effect of gender:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(effects)
plot(allEffects(logistic1))
```

What kind of effect does this plot show? And how did the effects package make those estimates? Instead of odds ratio or odds the effect plot shows the plain male and female survival rates. 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## male survival: (~ 14%)
table(titanic$Survived, titanic$Sex)[2,2]/(table(titanic$Survived, titanic$Sex)[2,2] + table(titanic$Survived, titanic$Sex)[1,2])

## ## female survival: (~ 83%)
table(titanic$Survived, titanic$Sex)[2,1]/(table(titanic$Survived, titanic$Sex)[2,1] + table(titanic$Survived, titanic$Sex)[1,1])
```

Recap:

- The odds for males are 93 to 564 is 0.1648936.
- The male survival is 93 to (564+93) = 0.1415525.
- The odds for females are 322 to 64 5.03125.
- The female survival is 322 to (322+64) = 0.8341969.

<!-- Apply `invLogit()` on the logodds for males. Show that the results are equivalent to the numbers from the effect plot.  -->

```{r, eval=F, echo=F, message=FALSE, warning=FALSE}
invLogit <- function(x) exp(x)/(1 + exp(x))

## logodds male
invLogit(log(table(titanic$Survived, titanic$Sex)[2,2] / table(titanic$Survived, titanic$Sex)[1,2]))
## logodds female
invLogit(log(table(titanic$Survived, titanic$Sex)[2,1] / table(titanic$Survived, titanic$Sex)[1,1]))
```

<!-- ##https://stats.stackexchange.com/questions/181996/logistic-regression-interpretation -->

<!-- ## First off, you need to understand that, because the "regression function" is S-shaped, -->
<!-- ## the first derivative is not constant. You therefore need to calculate the marginal effect at a certain point. -->
<!-- ## Comparatively, in a linear regression, we do not have to care about that since the slope of a line -->
<!-- ## is the same everywhere on this line. You have different possibilities: calculate the marginal effect at the mean -->
<!-- ## (in general the default option), at the median, or at particular data points. -->

<!-- ##https://www.displayr.com/how-to-interpret-logistic-regression-coefficients/ -->

<!-- ## In some areas it is common to use odds rather than probabilities when thinking about risk -->
<!-- ## (e.g., gambling, medical statistics). If you are working in one of these areas, -->
<!-- ## it is often necessary to interpret and present coefficients as odds ratios. If you are -->
<!-- ## not in one of these areas, there is no need to read the rest of this post, as the -->
<!-- ## concept of odds ratios is of sociological rather than logical importance (i.e., using -->
<!-- ## odds ratios is not particularly useful except when communicating with people that require them). -->

### Chi Square 

<!-- https://stats.stackexchange.com/questions/177816/logistic-regression-vs-chi-square-in-a-2x2-and-ix2-single-factor-binary-respo -->

Logistic regression checked the relationship between survival (binary) and gender (binary). So can  $\chi^2$.

:::: {.defbox}
::: {.titeldefbox}
<h2> Definition </h2>
:::
The **Chi Square Test for Independence** is a statistical test used to determine whether there is a significant association between two categorical variables.
::::

$$X^2 = \sum_i \frac{(O_i-E_i)^2}{E_i}$$

Where $O_i$ is the observed frequency and $E_i$ is the expected frequency. It turns out that the 2 X 2 contingency analysis with chi-square is really just a special case of logistic regression, and this is analogous to the relationship between ANOVA and regression. 

<!-- With chi-square contingency analysis, the independent variable is dichotomous and the dependent variable is dichotomous. We can also conduct an equivalent logistic regression analysis with a dichotomous independent variable predicting a dichotomous dependent variable. Logistic regression is a more general analysis, however, because the independent variable (i.e., the predictor) is not restricted to a dichotomous variable. Nor is logistic regression limited to a single predictor. -->

```{r, eval=TRUE, echo=TRUE}
chisq.test(titanic$Survived, titanic$Sex)
```

In the standard output of the test we usually conclude from the p-value (being less than 0.05) that there's is a relationship between the two variables (survival on Titanic is related to gender.)

We derive the test statistic. Everything starts from the contingency table of survival by gender. The design is improved by the `epitab` package.

```{r, eval=TRUE, echo=F, warning=FALSE, message=FALSE}
library(epitab)
titanic <- titanic %>%
  mutate(Survived_fct = factor(Survived, labels=c("Deceased", "Survived")),
         Sex_fct = factor(Sex, labels=c("Female", "Male")))

contingency_table(independents=list("Sex"='Sex_fct'),
                  outcomes=list('Survival'='Survived_fct'), ## need to be factors  
                  crosstab_funcs=list(freq()),
                  data=titanic) %>% 
  neat_table()
```

:::: {.dedicated}
::: {.titeldedicated}
<h2> Chi Square Test Statistic </h2>
:::

We derive the test statistic of Chi.

```{r, eval=TRUE, echo=TRUE}
tab_margins <- addmargins(table(titanic$Survived, titanic$Sex))
tab_obs <- table(titanic$Survived, titanic$Sex)

## Step 1: Expected absolute frequency
library("DescTools")
tab_exp <- ExpFreq(table(titanic$Survived, titanic$Sex), freq ="abs")

## Step 2: Subtract the expected value from the observed value (and square differences)
(tab_obs-tab_exp)^2

## Step 3: Divide by expected values
(tab_obs-tab_exp)^2/tab_exp

## Step 4: Sum up all
tab_temp <- (tab_obs-tab_exp)^2/tab_exp
tab_temp[1,1] + tab_temp[1,2] + tab_temp[2,1] + tab_temp[2,2]
```
::::

<!-- https://web.pdx.edu/~newsomj/pa551/lectur21.htm -->
<!-- https://www.theanalysisfactor.com/chi-square-test-vs-logistic-regression-is-a-fancier-test-better/ -->
<!-- https://stats.stackexchange.com/questions/570544/chi-square-test-vs-logistic-regression-result -->

### Some Are More Equal Than Others

Let's extend the group analysis to multiple categories using the `Pclass` variable. The passenger class represents the socio-economic status of people. We hypothesize that people with more money used their influence to get into one of the lifeboats. First, the contingency table of the situation: 

```{r, eval=T, echo=TRUE, warning=FALSE, message=FALSE}
titanic <- titanic %>%
  mutate(Pclass_fct = factor(Pclass))

contingency_table(independents=list("Passenger Class"='Pclass_fct'),
                  outcomes=list('Survival'='Survived_fct'), ## need to be factors  
                  crosstab_funcs=list(freq()),
                  data=titanic) %>% 
  neat_table()
```

In first class, more people survived (n=168) than died (n=114) whereas this is not true for the third class (365 died and 135 survived). [All men may be created equal](https://en.wikipedia.org/wiki/All_men_are_created_equal), but some are more equal than others. The logistic regression with one categorical variable is straightforward. Note that we tell R to handle `Pclass` like a factor (group variable): 

```{r, eval=TRUE, echo=TRUE}
logistic2 = glm(Survived ~ factor(Pclass), 
                family = binomial(link = 'logit'), 
                data = titanic)
```

```{r, eval=TRUE, echo=F}
cm <- c('(Intercept)' = 'Constant',
        'factor(Pclass)2'    = 'Class 2',
        'factor(Pclass)3'    = 'Class 3')

structure(
  paste(
    "<div style='display:flex; flex-wrap: wrap;'>",
    "<div style='flex-basis: 50%;'>",
    modelsummary(list("Survival" = logistic2), 
                 gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F',
                 coef_map = cm),
    "</div>",
    "<div style='flex-basis: 50%;'>",
    modelsummary(list("Survival (OR)" = logistic2), 
                 gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F',
                 coef_map = cm, exponentiate = TRUE),
    "</div>",
    "</div>"
  ),
  class = "kableExtra"
)
```

The lower your socio-economic status, the less survival can you expect.  

### The Cheaper The Deader? 

We have more information on the effect of socio-economic class, the `Fare` variable (ticket price). In the data the `Fare` variable is like this:

```{r, eval=TRUE, echo=TRUE}
datasummary_skim(titanic$Fare)
```

<u id='currency'>The prices of tickets on the Titanic in 1912</u> ranged from £870 or \$4,350 for a first-class parlor suite to a maximum of £8 or \$40 for a third-class passage. 

<!-- A century later, in 2012, those ticket prices equaled a range of \$50,000 to \$460. -->

```{r, echo=FALSE}
tippy::tippy_this(elementId = "currency", tooltip = "Before Britain’s currency was in terms of pounds and pence, it was in terms of pounds, shillings and pennies e.g. £18 15s 9d.  There were: 12 pennies per shilling and 20 shillings per pound.")
```







```{r, eval=TRUE, echo=TRUE}
logistic3 = glm(Survived ~ Fare, family = binomial(link = 'logit'), data = titanic)
```

```{r, eval=TRUE, echo=F}
structure(
  paste(
    "<div style='display:flex; flex-wrap: wrap;'>",
    "<div style='flex-basis: 50%;'>",
    modelsummary(list("Survival" = logistic3), 
                 gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F'),
    "</div>",
    "<div style='flex-basis: 50%;'>",
    modelsummary(list("Survival (OR)" = logistic3), 
                 gof_omit = 'R2|AIC|BIC|RMSE|Log.Lik.|F',
                 exponentiate = TRUE),
    "</div>",
    "</div>"
  ),
  class = "kableExtra"
)
```

The coefficient and intercept estimates give us the following equation:

$$ \text{logit}(p) = -0.80666  + 0.01109 \cdot \text{fare} $$

In this case, the estimated coefficient for the **intercept** is the log odds of a passenger with a fare of zero surviving the accident. Of course, there is nothing like a free lunch. So the intercept in this model corresponds to the log odds of survival when fare is at the hypothetical value of zero. 

How do we interpret the **slope** coefficient for `Fare`? Let's fix `Fare` at some value, e.g. the mean which roughly equals `r round(mean(titanic$Fare), 0)` (rounded to 0 decimals). Then the conditional logit of surviving when the fare is equal to the mean is: 

$$\begin{aligned} 
\text{logit}(p | \text{fare}=`r round(mean(titanic$Fare), 0) `) &= `r logistic3$coefficients[1] + logistic3$coefficients[2] * round(mean(titanic$Fare), 0)`  \\
\text{logit}(p | \text{fare}=`r round(mean(titanic$Fare), 0) + 1`) &= `r logistic3$coefficients[1] + logistic3$coefficients[2] * (round(mean(titanic$Fare), 0)+1)`  \\
\end{aligned}$$

We can examine the effect of a **one-unit increase** in fare by taking the difference of the two equations: 

$$\text{logit}(p | \text{fare}=`r round(mean(titanic$Fare), 0) `) - \text{logit}(p | \text{fare}=`r round(mean(titanic$Fare), 0)+1 `) = `r logistic3$coefficients[2] * (round(mean(titanic$Fare), 0)+1) - logistic3$coefficients[2] * round(mean(titanic$Fare), 0)`  $$

In other words, for a one-unit increase in the fare price, the expected change in log odds is `r logistic3$coefficients[2] * (round(mean(titanic$Fare), 0)+1) - logistic3$coefficients[2] * round(mean(titanic$Fare), 0)`. Can we translate this change in log odds to the change in odds? Indeed, we can. Recall that logarithm converts multiplication and division to addition and subtraction. Its inverse, the exponentiation converts addition and subtraction back to multiplication and division. If we exponentiate both sides of our last equation, we have the following:

$$ e^{\text{logit}(p | \text{fare}=`r round(mean(titanic$Fare), 0)+1`) - \text{logit}(p | \text{fare}=`r round(mean(titanic$Fare), 0)`)} = e^{`r logistic3$coefficients[2] * (round(mean(titanic$Fare), 0)+1) - logistic3$coefficients[2] * round(mean(titanic$Fare), 0)`} =  1.011148  $$

So we can say for a one-unit increase in fare price, we expect to see about 1.1% increase in the odds of surviving. This 1.1% of increase does not depend on the value that fare is held at.

Can we get this information more directly? Yes, we can. 

```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
require(MASS)
exp(cbind(coef(logistic3), confint(logistic3)))  
```


<!-- ## We can say now that the coefficient for fare is the difference in the log odds. -->
<!-- ## In other words, for a one-unit increase in the fare, the expected change in log odds is 0.011493. -->

<!-- ## exp[log(p/(1-p))(fare=34)-log(p/(1-p))(fare=33)] = -->
<!-- ## exp(log(p/(1-p))(fare=34)) / exp(log(p/(1-p))(fare=33)) = -->
<!-- ## odds(fare=34)/odds(fare=33) = -->
<!-- ## exp(0.011493) = 1.011559 -->

<!-- ## So we can say for a one-unit increase in fare, -->
<!-- ## we expect to see about 2% increase in the odds of surviving. -->
<!-- ## This 2% of increase does not depend on the value that fare is held at. -->
<!-- ## (model is linear in parameters) -->

#### The Bigger Picture 

First, we take another subset, since `PassengerId`, `Name` and `Ticket` are not good predictors. 

```{r, eval=TRUE, echo=TRUE}
logistic4 = glm(Survived ~ ., family = binomial(link = 'logit'), data = titanic) 
logistic4
```

For example, increase one unit in age will decrease the log odd of survival by 0.0257; being a male will decrease the log odd of survival by 3.78 compared to female; and being in class2 will decrease the log odd of survival by 1.17, being in class3 will decrease the log odd of survival by 1.97.

Moreover, exponentiate the model coefficients can look at the result and interpret its meaning at a different angle. Below are table of the “odd ratio” value for each predictor coefficient relative to the survival and their respective 95% confident interval odd ratio value.

```{r, eval=TRUE, echo=TRUE}
exp(cbind(OR = coef(logistic4)))  
```

Now, the result can be interpreted as: for a unit increase in age, the odds of surviving from the incident decrease by a factor of 0.96; and being a third class (class3), the odds of surviving decrease by a factor of 0.12, etc.

<!-- The overall survival rate for men was 20%. For women, it was 74%, and for children, 52% -->

::: {.infobox2 .information data-latex="warning"}
The logit model coefficients can be presented as **odds ratios**. But odds-ratios are often misinterpreted as if they were relative risks/probabilities. Nonetheless presenting odds-ratios is standard practice in the medical literature.
:::

#### Marginal Effects

<!-- https://stats.stackexchange.com/questions/147612/interpretation-of-marginal-effects-in-logit-model-with-log-timesindependent-va -->
<!-- https://www.youtube.com/watch?v=lJMO5kqWwIo -->

Marginal effects is a way of presenting results as differences in probabilities, which is more informative than odds-ratios and relative risk. For continuous variables this represents the instantaneous change given that the ‘unit’ may be very small. For binary variables, the change is from 0 to 1, so one ‘unit’ as it is usually thought. 

In order to express the more intuitive change in the predicted probability that the outcome equals 1 requires conditioning on all other included variables (i.e., selecting a set of values for all righthand-side variables) and running that set of values through the link function to convert log-odds to probabilities, thus making the marginal effect (in probability terms) of one variable a function of all other variables included in the model. 

::: {.infobox2 .information data-latex="warning"}
**Marginal effects** show the change in probability when the predictor or independent variable increases by one unit.
:::

 <!-- This is the definition of semi-elasticity, and can be interpreted as the -->
 <!-- change in probability for a 1% change in x. -->

 <!-- So we can say for a one-percent increase in fare, -->
 <!-- we expect to see about 0.002544 increase in the probability of surviving -->
 <!-- on a scale from 0 to 1. -->
 <!-- We could also infer that a 10% price increase increases survival probability -->
 <!-- by about 0.025 -->

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(margins)
summary(margins(logistic4))
```





<!-- #### Odds Ratio -->

<!-- ::: {.infobox2 .information data-latex="warning"} -->
<!-- The **Odds Ratio** (OR) is a measure of association between an exposure and an outcome. -->
<!-- ::: -->

<!-- <!-- https://www.theanalysisfactor.com/why-use-odds-ratios/ -->
<!-- <!-- The problem is that probability and odds have different properties that give odds some advantages in statistics. For example, --> 

<!-- In logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur. The key phrase here is constant effect. In regression models, we often want a measure of the unique effect of each X on Y. If we try to express the effect of X on the likelihood of a categorical Y having a specific value through probability, the effect is not constant. What that means is there is no way to express in one number how X affects Y in terms of probability. The effect of X on the probability of Y has different values depending on the value of X. -->

<!-- So we can get the odds ratio by exponentiating the coefficient for female. Most statistical packages display both the raw regression coefficients and -->
<!--  the exponentiated coefficients for logistic regression models. -->

<!-- ```{r, eval=TRUE, echo=TRUE} -->
<!-- exp(coef(logistic1)) -->
<!-- ``` -->






