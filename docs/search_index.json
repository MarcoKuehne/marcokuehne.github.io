[["index.html", "Becoming Fluent in Data A Personal Journey – Every Time. Preface", " Becoming Fluent in Data A Personal Journey – Every Time. Marco Kühne 2024-02-11 Preface "],["about-the-book.html", "About the book Aims of this book Structure Why R? Why Tidyverse?", " About the book I welcome you to join us on our way to become fluent in data. Aims of this book This project is for everyone. For students You go from zero you hero in data analysis and data science and will become data fluent and learn major skills that you can use in your academic and business career. The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades... Hal Varian, 2009. Googles chief economist. In: McKinsey Quarterly 2009 In addition and independent of a specific career, I would like to foster people's data literacy. Definition Data literacy is the ability to read, understand, create, and communicate data as information. I would like to enable and empower all people to understand the data work of others. Don't take numbers for granted. It's a long journey to get them. Don't be satisfied with a summary or conclusion from someone else. It's worthwhile checking data sources and data work. Either to replicate and validate the data work of others or to form your own opinion. Not to mention, coding is fun. You might be under the impression, to code, your favorite thing must be computers. Or I've heard I'm bad a math, I can't code. None of this is true. Think about it, what is your passion. Learn to code, is something that you can do. And something that may just expand the way you approach and think about the passions in your life. Be their personal or professional.   A wordcloud of 25 students answers (2020) on the question: What do you expect to learn in this seminar? The animation is created by the gifski package. For instructors Becoming Fluent In Data is planned to become an open educational resource (OER) and provide freely accessible educational content related to data analysis. It offers resources, tutorials, and information aimed at helping individuals become proficient in working with data. As an OER, the website allows users to access and utilize its materials without any cost, enabling widespread dissemination and promoting equitable access to knowledge and learning opportunities. The licensing is yet to be finalized. Parts of the OER can be used flexibly, as they are modular and can be used independently of each other. This enables users to select and use those parts that are relevant to their specific needs. They can extract individual chapters, modules or exercises from the OER resource and integrate them into their own learning environments. The site is hosted on GitHub, the entire source code is available. The project is accompanied by a data repository that can be used for a variety of teaching and learning scenarios (available as .txt, .csv or .xlsx). The educational videos created for the OER can be used as a standalone introduction to central concepts. Structure Every chapter covers the content of a week. The first half of the course introduces all the data basics from scratch. What is data? Why do we measure? How do we measure? How do we make comparisons? Most decisions are complex, costly and have long-term implications. It is therefore vital that such decisions are based on the best available evidence. The second half of the course focuses on the analysis of relationships. The most interesting research questions in social science are about relationships. What is the relationship between beauty and employment chances? What is the connection between money and happiness? How does remote work change ones productivity? Is social support related to longevity of marriages? The workhorse procedure is regression, a statistical technique that relates variables to each other. Color Colored paragraphs give you a visual overview of things to watch out: Definition A definition is a statement of the meaning of a term. Amazing Fact Bazinga highlights a memorable fact. Reading A precious resource. Your Turn It's your turn. Truly Dedicated Heavy stuff to think about. Components The tippy package allows underlined text to display tooltips. The webexercises package allows interactive web pages that you can use in ballroom dancingself-guided learningvegan cookingyour stamp collection. What is the Answer to the Ultimate Question of Life . The most powerful interaction comes from the web annotation tool https://web.hypothes.is/. You may annotate or highlight all text by selecting it with the cursor and then click the on the pop-up menu. You can also see the annotations of others: click the in the upper right hand corner of the page. Create a free account for this feature. Why R? This book uses R. All concepts could have been implemented in Python as well and there is a future plan to translate some examples to Python. Both are versatile programming languages with an active community and a lot of free online resources. My main recommendation is to use a programming language instead of a WYSIWYG statistical software (e.g. SPSS) and to use open-source software instead of proprietary software. Why Tidyverse? The debate regarding how to teach R centers around whether to introduce the base R programming language or the tidyverse ecosystem. Proponents of base R argue that it provides a solid foundation for understanding R's core principles and functions. It emphasizes learning the fundamentals of R programming, which can be beneficial for more complex data manipulation tasks and working with large datasets. On the other hand, advocates for the tidyverse approach argue that it offers a more user-friendly and intuitive way to work with data. The tidyverse packages, such as dplyr and ggplot2, provide a consistent and streamlined syntax for data manipulation and visualization, making it easier for beginners to grasp and apply data analysis techniques. This book uses tidyverse predominantly. "],["about-the-author.html", "About the author Dust and Dark Teach – Learn – Repeat", " About the author   Welcome! My name is Marco Kühne. The very first thing I want to do is to invite you to call me Marco. That is, if we meet on the street, you come talk to me during office hours, you ask some question; Marco’s the name that I respond to. Web: http://marco-kuehne.com/ Twitter: https://twitter.com/marco_kuhne GitHub: https://github.com/MarcoKuehne I am a PhD candidate in Economics at the European University Viadrina. I am generally keen on teaching topics related to research design, quantitative methods, and statistical software. My main methodological interests in quantitative social science include panel data modelling, causal inference with observational data and R programming. I am also a gardening fanatic, a coffee enthusiast, a committed ballroom and Discofox dancer, a (vegetarian) food lover. I enjoy cutting down big trees and practicing new languages in its own sake. Feel free to contact me! Dust and Dark A dusty lecture hall. The light cuts through the darkness from the left side of the room. A dozen of seats in each bench, only few occupied by small groups of students who were trying to make sure that they sit far from each other and as far as possible from the lecturer. The bearish but competent assistant professor explained how to analyze and evaluate the results of various memory and cognition experiments through boxplots, t-test and the like in that software. My creaky, slow but loyal laptop in front of me. That's where R was introduced in my psychology undergraduate studies. – The Times They Are A-Changin'. This eBook is done in R. Are you eady to join the journey? Lecture Hall. Melanchthonianum. MLU University of Halle-Wittenberg The following animation is created with the gganimate package. It shows past course data. Which graphical feature is used to display 3 data dimensions in a two-dimensional graph: ColorSizeShapeOpacity? How many students participated in summer 2021? Answer: . Teach – Learn – Repeat Teaching and learning are strongly connected. I fell in love with learning by teaching the moment I came across this concept. It put the experiences I made into scientific context. Studying for the undergraduate math classes, I soon became head of the study group, than a private tutor, than a student assistant and a doctoral student, now, teaching stuff for over a decade. Still, I feel that (trying) to teach stuff is the best way of learning it myself. By writing the gitbook I hope to force myself to pinpoint exactly what I know and don't know about data and how to fill the gaps. Luckily, I am not alone with the approach of creating classes or writing books to learn: I could feel that econometrics was indispensable, and yet I was missing something. But what? It was a theory of causality […]. So, desperate, I did what I always do when I want to learn something new — I developed a course on causality to force myself to learn all the things I didn’t know. Cunningham (2021) This project helped me to learn more about R, RStudio, R Markdown, R Bookdown, HTML/CSS, Git and Github, empirical research, causal inference, statistics, math, frustration tolerance and fun. Teaching in 2014. Learning by teaching was originally defined by Jimmy WalesLinus TorvaldsJean-Pol MartinRichard E. Pattis in the 1980s. Did you know that some people who have Wikipedia articles also have user accounts on Wikipedia? "],["intro-to-r.html", "Intro to R R is a calculator R is more than a calculator Define objects Plots", " Intro to R An .R file is a text document. Open (and edit) it with a text editor, a web browser or a more sophisticated software like RStudio.1 Click this link that opens an R script in your browser. Using .R files or scripts offers efficiency, reproducibility, scalability, collaboration, documentation, and flexibility. They allow you to automate tasks, handle large datasets, collaborate with others, document your work, and customize solutions. R is a calculator R is a calculator. Use this R demo in the browser to explore basic features of R. Commands in the script.R tab are executed by the Run bottom. It runs the entire script and prints out results in the R Console. This setting is simplified but reflects the procedure in a more complex integrated developer environment (IDE) like RStudio. Test it. Definition Basic arithmetic operators are: + Addition - Subtraction * Multiplication / Division ^ Exponent R is more than a calculator Your Turn: Adjust the code. If you never saw R before, change the main title to what ever suits you or change the color option col from lightblue to aliceblue. If you have some experience, order the bars using the sort() command. Define objects Define R objects for later use. Objects are case-sensitive (X is different from x). Objects can take any name, but its best to use something that makes sense to you, and will likely make sense to others who may read your code. Numeric Variables The standard assignment operator is &lt;-, the equal sign = works as well. The code assigns the object a the value 2 and b the value 3. The sum is 5. a &lt;- 2 b = 3 a + b #&gt; [1] 5 Logical Variables Logical values are TRUE and FALSE. Abbreviations work. You can write T instead of TRUE. &gt; harvard &lt;- TRUE # spacing doesn&#39;t matter &gt; yale &lt;- FALSE &gt; princeton &lt;- F # short for FALSE &gt; &gt; # Attention: FALSE=0, TRUE=1 &gt; harvard + 1 #&gt; [1] 2 Although spacing technically doesn't matter in R, there are some best practices to consider. “Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read.” -- Hadley Wickam -- Style guide Reading Place spaces around all binary operators (=, +, -, &lt;-, etc.). Do not place a space before a comma, but always place one after a comma. Read more in Google's R Style Guide at Uni Stanford. String Variables Text is stored as string or character. emily &lt;- &#39;She is a friend.&#39; # string / character class / plain text libby &lt;- &quot;she is a coworker&quot; # use &#39; and &quot; interchangeably other &lt;- &quot;people&quot; # prefer &quot; Factor Variables A factor is an ordered categorical variable. c() is a generic function which combines its arguments. fruit &lt;- factor(c(&quot;banana&quot;, &quot;apple&quot;) ) # The default ordering is alphabetic fruit #&gt; [1] banana apple #&gt; Levels: apple banana dose &lt;- factor(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) ) # The default ordering is alphabetic dose #&gt; [1] low medium high #&gt; Levels: high low medium Factor levels inform about the order of the components, i.e. apple comes before banana and high comes comes before low, than comes medium. Of course, the apple-banana order does not makes any sense, and the high-low-medium order is just wrong. Software cannot know whether an ordering makes sense, that's job of the data scientist. Use the levels option inside the factor() function to tell R the ordering. dose &lt;- factor(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) ) dose #&gt; [1] low medium high #&gt; Levels: low medium high Combine objects # Declare new objects using other variables c &lt;- a + b + 10 # Open z object or put everything in parentheses (c &lt;- a + b + 10) #&gt; [1] 15 Vectors Think of a vector as a single column in a spreadsheet. vectorA &lt;- c(1,2,b) vectorB &lt;- c(TRUE,TRUE,FALSE) vectorC &lt;- c(emily, libby, other) # Vector Operations vectorA - vectorB # Vector operation AND auto-change TRUE =1, FALSE=0 #&gt; [1] 0 1 3 Data frame # think of it conceptually like a spreadsheet dataDF &lt;- data.frame(numberVec = vectorA, trueFalseVec = vectorB, stringsVec = vectorC) # Examine an entire data frame dataDF #&gt; numberVec trueFalseVec stringsVec #&gt; 1 1 TRUE She is a friend. #&gt; 2 2 TRUE she is a coworker #&gt; 3 3 FALSE people # Declare a new column dataDF$NewCol &lt;- c(10,9,8) # Examine with new column dataDF #&gt; numberVec trueFalseVec stringsVec NewCol #&gt; 1 1 TRUE She is a friend. 10 #&gt; 2 2 TRUE she is a coworker 9 #&gt; 3 3 FALSE people 8 # Examine a single column dataDF$numberVec # by name #&gt; [1] 1 2 3 dataDF[,1] # by index...remember ROWS then COLUMNS #&gt; [1] 1 2 3 # Examine a single row dataDF[2,] # by index position #&gt; numberVec trueFalseVec stringsVec NewCol #&gt; 2 2 TRUE she is a coworker 9 # Examine a single value dataDF$numberVec[2] # column name, then position (2) #&gt; [1] 2 dataDF[1,2] #by index row 1, column 2 #&gt; [1] TRUE Plots There are base R graphs. There are ggplot2 plots. # Create some variables x &lt;- 1:10 y1 &lt;- x*x y2 &lt;- 2*y1 # Create a first line plot(x, y1, type = &quot;b&quot;, frame = FALSE, pch = 19, col = &quot;red&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) # Add a second line lines(x, y2, pch = 18, col = &quot;blue&quot;, type = &quot;b&quot;, lty = 2) # Add a legend to the plot legend(&quot;topleft&quot;, legend=c(&quot;Line 1&quot;, &quot;Line 2&quot;), col=c(&quot;red&quot;, &quot;blue&quot;), lty = 1:2, cex=0.8) Download RStudio https://posit.co/downloads/.↩︎ "],["intro-to-tidyverse.html", "Intro to Tidyverse Data with readr Verbs of dplyr Graphs with ggplot2", " Intro to Tidyverse The tidyverse is a collection of R packages for data science that share a common philosophy and grammar.2 Once the package tidyverse is installed on your system via the command install.packages(tidyverse), it is loaded via library(tidyverse) in a session. Then you have access to all components like readr (for reading data), dplyr (for manipulating data), ggplot2 (for data visualization) and many more. Data with readr The readr package reads data into what is called a tibble. A tibble is similar to a dataframe. When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type, and uses font styles and color for highlighting. So to say, the default behavior is excellent. # load the entire tidyverse library(tidyverse) # read_csv is a tidyverse (readr) function coursedata &lt;- read_csv(&quot;https://raw.githubusercontent.com/MarcoKuehne/marcokuehne.github.io/main/data/Course/GF_2022_57.csv&quot;) # print a tibble coursedata #&gt; # A tibble: 57 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 8 2 #&gt; 2 Bachelor Female 22 10 4 #&gt; 3 Master Male 23 9 4 #&gt; 4 Master Male 24 2 3 #&gt; 5 Master Male 27 10 2 #&gt; 6 Master Male 23 7 3 #&gt; 7 Bachelor Female 20 5 2 #&gt; 8 Bachelor Female 22 8 2 #&gt; 9 Master Male 27 13 4 #&gt; 10 Master Female 22 10 4 #&gt; # ℹ 47 more rows #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; When you use the base R read.csv() instead, it reads data into a dataframe. When you print the dataframe, it displays all data at once (output not shown in the book). In order to show first entries, another command like head() is necessary. # use base R utilities coursedata &lt;- read.csv(&quot;https://raw.githubusercontent.com/MarcoKuehne/marcokuehne.github.io/main/data/Course/GF_2022_57.csv&quot;) # print a dataframe (all data) coursedata # print first 6 observations of the data head(coursedata) Reading There are three key differences between tibbles and data frames: printing, subsetting, and recycling rules. Read more about those difference in the vignette of tibble. Verbs of dplyr The first verbs you learn for data inspection are glimpse(), select(), arrange() and filter(). Those are classic operators that you also find in Microsoft Excel (via clicking the correct menu options). Glimpse glimpse() tells the number of rows and columns, the first variable names, the class of the variables, i.e. chr for character (like text) or int for integer (like whole numbers). Another kind of variables are dbl, double, short for double-precision floating-point format. This data set contains 57 rows (observations) and 9 columns (variables). glimpse() also shows the first observations for each variable. glimpse(coursedata) #&gt; Rows: 57 #&gt; Columns: 8 #&gt; $ Academic.level &lt;chr&gt; &quot;Bachelor&quot;, &quot;Bachelor&quot;, &quot;Master&quot;, &quot;Mast… #&gt; $ Gender &lt;chr&gt; &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Ma… #&gt; $ Age &lt;dbl&gt; 23, 22, 23, 24, 27, 23, 20, 22, 27, 22,… #&gt; $ Total.Semesters &lt;dbl&gt; 8, 10, 9, 2, 10, 7, 5, 8, 13, 10, 8, 8,… #&gt; $ Background.in.Statistics &lt;dbl&gt; 2, 4, 4, 3, 2, 3, 2, 2, 4, 4, 2, 2, 2, … #&gt; $ Background.in.R &lt;dbl&gt; 2, 3, 2, 1, 1, 1, 3, 1, 4, 4, 1, 2, 2, … #&gt; $ Background.in.Academic.Writing &lt;dbl&gt; 2, 2, 4, 3, 4, 2, 3, 2, 4, 3, 1, 2, 3, … #&gt; $ Expectations &lt;chr&gt; &quot;I want to be efficient in my knowledge… Select Columns are selected by name or column index. Thus, the outcome of select(coursedata, Gender, Age) and select(coursedata, 2, 3) is identical. select(coursedata, Gender, Age) #&gt; # A tibble: 57 × 2 #&gt; Gender Age #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Female 23 #&gt; 2 Female 22 #&gt; 3 Male 23 #&gt; 4 Male 24 #&gt; 5 Male 27 #&gt; 6 Male 23 #&gt; 7 Female 20 #&gt; 8 Female 22 #&gt; 9 Male 27 #&gt; 10 Female 22 #&gt; # ℹ 47 more rows We can use a minus - to get rid of a column and leave the rest of the columns: select(coursedata, -Total.Semesters, -Background.in.Statistics, -Background.in.R, -Background.in.Academic.Writing) Arrange Often we are interested in the maximum or minimum age, thus arrange() a numerical value. arrange(coursedata, Age) # from low to high Age #&gt; # A tibble: 57 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 20 5 2 #&gt; 2 Master Female 21 7 2 #&gt; 3 Bachelor Male 21 2 3 #&gt; 4 Bachelor Female 21 4 2 #&gt; 5 Bachelor Female 21 3 2 #&gt; 6 Bachelor Male 21 1 2 #&gt; 7 Bachelor Female 21 1 3 #&gt; 8 Bachelor Female 22 10 4 #&gt; 9 Bachelor Female 22 8 2 #&gt; 10 Master Female 22 10 4 #&gt; # ℹ 47 more rows #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; The default is from low to high values, the desc() options reverses the order. arrange(coursedata, desc(Age)) # reverse Rename Sometimes default variables names are too long or too complicated, thus we like to rename() them. coursedata %&gt;% rename(Degree = Academic.level, Semesters = Total.Semesters) #&gt; # A tibble: 57 × 8 #&gt; Degree Gender Age Semesters Background.in.Statistics Background.in.R #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 8 2 2 #&gt; 2 Bachelor Female 22 10 4 3 #&gt; 3 Master Male 23 9 4 2 #&gt; 4 Master Male 24 2 3 1 #&gt; 5 Master Male 27 10 2 1 #&gt; 6 Master Male 23 7 3 1 #&gt; 7 Bachelor Female 20 5 2 3 #&gt; 8 Bachelor Female 22 8 2 1 #&gt; 9 Master Male 27 13 4 4 #&gt; 10 Master Female 22 10 4 4 #&gt; # ℹ 47 more rows #&gt; # ℹ 2 more variables: Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; This change is only temporarily and shown in the console output. In order to keep the new name of a variable, we can overwrite the old R object or create a new one. # overwrite the old dataframe coursedata &lt;- coursedata %&gt;% rename(Degree = Academic.level, Semesters = Total.Semesters) The pipe operator As in base R, we often like to combine commands, e.g. select the Age variable and sort its values. dplyr verbs can be nested as in base R. arrange(select(coursedata, Age), Age) But there is something else that is used in tidyverse logic, the so called pipe operator %&gt;% (percentage sign, relation larger than, another percentage sign). You can read this as \"then, please do the following\". coursedata %&gt;% # start with this data select(Age) %&gt;% # then select only the Age variable arrange(Age) # then arrange the values #&gt; # A tibble: 57 × 1 #&gt; Age #&gt; &lt;dbl&gt; #&gt; 1 20 #&gt; 2 21 #&gt; 3 21 #&gt; 4 21 #&gt; 5 21 #&gt; 6 21 #&gt; 7 21 #&gt; 8 22 #&gt; 9 22 #&gt; 10 22 #&gt; # ℹ 47 more rows Filter It is reasonable to filter() specific values of variables. All filters use conditional expression based on relational operators. Definition Use relational operators to build your filter: == equal to != not equal to &gt; more or &lt; less then Here are some examples: # filter students who have more than 10 semesters in total coursedata %&gt;% filter(Total.Semesters &gt; 10) # filter female students coursedata %&gt;% filter(Gender == &quot;Female&quot;) Combinations of filters are possible via logical operators &amp; (and) and | (or). We are looking for females who study in a master program. coursedata %&gt;% filter(Gender == &quot;Female&quot; &amp; Academic.level == &quot;Master&quot;) #&gt; # A tibble: 12 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Master Female 22 10 4 #&gt; 2 Master Female 21 7 2 #&gt; 3 Master Female 29 9 3 #&gt; 4 Master Female 23 8 4 #&gt; 5 Master Female 26 8 2 #&gt; 6 Master Female 25 10 2 #&gt; 7 Master Female 24 11 1 #&gt; 8 Master Female 25 10 3 #&gt; 9 Master Female 33 3 1 #&gt; 10 Master Female 25 14 4 #&gt; 11 Master Female 25 11 3 #&gt; 12 Master Female 26 4 2 #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; We are looking for females or anybody who reports more than 10 semesters. coursedata %&gt;% filter(Gender == &quot;Female&quot; | Total.Semesters &gt; 10) #&gt; # A tibble: 33 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 8 2 #&gt; 2 Bachelor Female 22 10 4 #&gt; 3 Bachelor Female 20 5 2 #&gt; 4 Bachelor Female 22 8 2 #&gt; 5 Master Male 27 13 4 #&gt; 6 Master Female 22 10 4 #&gt; 7 Bachelor Female 22 8 2 #&gt; 8 Master Female 21 7 2 #&gt; 9 Master Male 27 15 3 #&gt; 10 Master Female 29 9 3 #&gt; # ℹ 23 more rows #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; Mutate mutate() is the most frequent used command you will come across. It changes the data. We create a new variable Background_Knowledge by taking the average of the three background variables. All background variables have the same range from 1 to 5. coursedata %&gt;% mutate(Background_Knowledge = (Background.in.Statistics + Background.in.R + Background.in.Academic.Writing)/3) %&gt;% select(Academic.level, Gender, Age, Background_Knowledge) #&gt; # A tibble: 57 × 4 #&gt; Academic.level Gender Age Background_Knowledge #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 2 #&gt; 2 Bachelor Female 22 3 #&gt; 3 Master Male 23 3.33 #&gt; 4 Master Male 24 2.33 #&gt; 5 Master Male 27 2.33 #&gt; 6 Master Male 23 2 #&gt; 7 Bachelor Female 20 2.67 #&gt; 8 Bachelor Female 22 1.67 #&gt; 9 Master Male 27 4 #&gt; 10 Master Female 22 3.67 #&gt; # ℹ 47 more rows Summarize Would you like to know the average age of course participants? It is 24.7192982 There are two ways in order to achieve this. # calculate mean age with summarize() What is the difference between them? mutate() creates a new variable mean_age in the data set for all 57 observations. But there is only 1 mean value. Thus, mutate() repeats this mean value 57 times. The result is a 57x9 tibble. summarize() collapses the tibble to a single value. The result is a 1x1 tibble. The question is, what do you plan to do next with your results. After summarize() all other information is gone. We will see this in the next graph. Graphs with ggplot2 ggplot() follows the Grammar of Graphics. The first argument is the data, the second is aes() aesthetics (that define the x- and y-variable). In order to add more to the graph, use the + operator (instead a pipe). Add layers, so called geoms, like geom_point() to create points in a coordinate system, a.k.a the scatter plot. theme_minimal() is a particular set of options that controls non-data display. ggplot(coursedata, aes(x = Age, y = Total.Semesters)) + geom_point() + theme_minimal() Alternatively, data can be piped into a ggplot(). In the second version of the graph, we added axis labels inside the labs() command and another layer geom_smooth() for a trend line of the relationship. Inside we define the method to be a linear model and the standard errors to be deactivated. Play around with those options, what other methods are available? What happens when we turn standard errors on? coursedata %&gt;% ggplot(aes(x = Age, y = Total.Semesters)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + labs(title = &quot;Relationship between Age and Semester of course participants.&quot;, x = &quot;Age&quot;, y = &quot;Semesters&quot;) See https://www.tidyverse.org/.↩︎ "],["data-is-everywhere.html", "Chapter 1 Data is everywhere 1.1 Why we measure 1.2 Means of measuring 1.3 Types of data 1.4 Can we measure everything? 1.5 The reality behind the data", " Chapter 1 Data is everywhere Data is ubiquitous in today's world and its importance is growing rapidly, especially in social science. With the increasing availability of data, researchers can gain insights into human behavior, social trends, and other important phenomena. The use of data analysis tools and techniques allows researchers to extract meaningful insights from the vast amounts of data that are being generated every day, and these insights can be used to inform policies, strategies, and decisions that impact society. It is therefore crucial for social scientists to have the skills and knowledge to effectively manage and analyze data. 1.1 Why we measure Two true stories. 1.1.1 Women are having far fewer children. Figure 1.1: Global fertility rate. Figure 1.1 shows the global total fertility rate according to Gapminder.3 There is a dramatic change in the number of babies per woman in the last 50 years. The maintenance of a stable human population requires that the mean number of children women should have by the completion of the fertile part of their life is 2.1.4 We cannot know this without measurement. We may have an impression that families are smaller, but that could just be the people we know directly – and most of us know directly at most a couple of hundred households.5 We have to measure to know the big picture and we have to keep measuring to see how the picture is changing. Size matters. Change matters. Without measurement we can describe neither current condition nor the history of current condition. 1.1.2 Global surface temperature is rising. Figure 1.2: Global temperature in the common era. The 2011–2020 decade warmed to an average 1.09 °C [0.95–1.20 °C] compared to the pre-industrial baseline (1850–1900). Figure 1.2 shows a global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue. Directly observed data is in red (Wikipedia contributors 2022). Data is required to make informed decisions. Decisions about climate change are complex, costly and have long-term implications. It is therefore vital that such decisions are based on the best available evidence. We need to understand the quality and provenance of that evidence, and whether any assumptions have been made in generating it. 1.2 Means of measuring Data collection is the process of gathering and measuring information. As social scientists we rarely count tree rings or analyse corals and ice cores. Social science is concerned about human behavior, attitudes, opinions, and characteristics to understand social phenomena. Social science researchers use a variety of data collection methods, including surveys, interviews, observations, and experiments, to collect data that can be analyzed and used to test hypotheses and answer research questions. Surveys are a common data collection method in social science research. They involve administering questionnaires to a sample of individuals to collect data about their attitudes, opinions, beliefs, and behaviors. Surveys can be conducted through various means, including online, telephone, and face-to-face interviews. Interviews are another method used in social science research to collect data. Interviews involve asking individuals questions about their experiences, attitudes, and opinions in a one-on-one or group setting. Interviews can be structured, semi-structured, or unstructured, depending on the research question. Observations are a method used to collect data about human behavior by observing individuals in natural or controlled settings. Researchers can collect data through direct observation or by using technology to capture behavior, such as video or audio recordings. Experiments involve manipulating one or more variables to observe their effect on a dependent variable. Experiments can be conducted in a controlled laboratory setting or in the natural environment. Data scraping is a method of data collection that involves using software or code to extract information from websites or other online sources. Data scraping can be a useful tool for gathering large amounts of data quickly, and it can be used for a variety of purposes, including market research, sentiment analysis, and trend analysis. In social science research, data collection must be conducted ethically and with informed consent from participants. Researchers must also consider issues of bias and sampling to ensure that their data collection methods produce accurate and representative data. 1.3 Types of data 1.3.1 Origin of data Primary and secondary data are two types of data used in research, and they differ in their origin and collection method. Primary data is original data that is collected by the researcher or research team through direct observation, experimentation, surveys, interviews, or other methods. Primary data is often collected specifically for the research project at hand, and it is tailored to the research question and objectives. Primary data is generally more expensive and time-consuming to collect compared to secondary data, but it is often more accurate and reliable since the researcher has more control over the data collection process. Secondary data, on the other hand, is data that has already been collected and compiled by others for other purposes. This can include data from sources such as government reports, academic journals, newspapers, and industry reports. Secondary data can be accessed easily and is usually less expensive and less time-consuming to obtain compared to primary data. However, the accuracy and reliability of secondary data can be a concern, as it may not have been collected with the specific research question or objectives in mind, or it may be outdated or biased. A lot of data comes ready for analysis and free for research purposes. Make us of it. 1.3.2 Analysis of data Qualitative and quantitative data are two types of data used in research, and they differ in their nature and analysis methods. Qualitative data is non-numerical data that is collected through open-ended questions, observations, or other non-structured methods. This data can be text, audio or visual. Qualitative data is often descriptive and subjective, and it provides insight into how individuals perceive and interpret the world. Qualitative data is often analyzed using methods such as thematic analysis, content analysis, or discourse analysis, and it can provide rich and detailed insights into complex phenomena. There are quantitative approaches to analyse text (text mining, e.g. sentiment analysis) and visual data (machine learning, e.g. image classification) as well. Quantitative data, on the other hand, is numerical data that is collected through structured methods such as surveys or experiments. Quantitative data is often used to test hypotheses and to measure the magnitude and frequency of a particular phenomenon. Quantitative data is analyzed using statistical methods, such as regression analysis or hypothesis testing, and it provides objective and standardized results. That being said, quantitative data is usually expressed in numerical form and can represent size, length, duration, amount, price, and so on. Sometimes quantitative data is understood as metric continuous as opposed to qualitative data in the form of categorical data. 1.3.3 Structure of data Rectangular data is a type of data structure that is commonly used to organize and store data in tables or spreadsheets. In rectangular data, the rows represent individual observations or cases, while the columns represent variables or attributes that describe the observations. Each cell in the table represents a single value for a particular observation and variable. Rectangular data is also known as \"tabular data\" or \"relational data,\" and it is the most common type of data used in quantitative research. Rectangular data is used to store various types of data, including demographic data, survey responses, financial data, and experimental data. Two common types of tabular data are cross-sectional and panel data that differ in their nature and the research question they address. Cross-sectional data is collected at a single point in time, from a sample of individuals, organizations, or other units of analysis. Cross-sectional data provides a snapshot of a particular phenomenon at a specific point in time, and it can be used to analyze differences and similarities between groups. Cross-sectional data can be collected through surveys, experiments, or other methods, and it is often analyzed using descriptive statistics, such as means, medians, or percentages. Panel data, on the other hand, is longitudinal data that is collected from the same individuals, organizations, or other units of analysis over time. Panel data provides information on how a particular phenomenon changes over time, and it allows for the analysis of individual-level changes and the identification of causal relationships. Panel data can be collected through surveys, experiments, or other methods, and it is often analyzed using methods such as regression analysis or difference-in-differences. 1.3.4 The level of access Open data refers to data that is made available to the public without restrictions or limitations on its use, reuse, and redistribution. This means that anyone can access, use, and share the data without needing permission or paying fees. One example is the official portal for European data is called the European Data Portal (EDP). It is a comprehensive platform that provides access to public datasets from various European Union (EU) institutions and other sources. The EDP aims to promote the sharing and use of open data across Europe by offering a centralized platform for finding, accessing, and using data. Open data is licensed under an open license. An open license is a type of license that allows others to access, use, and share a work or product while also providing certain freedoms and protections to the creator or owner of the work. Open licenses are often used for software, content, and data, and they typically include conditions that allow for free distribution and modification of the work. The statistical office in Germany provides open data under the Data Licence Germany 2.0. Most Wikipedia texsts are licensed under Creative Commons Attribution-ShareAlike 3.0. The Creative Commons Attribution-ShareAlike 3.0 (CC BY-SA 3.0) license is a type of open license that allows others to share and adapt a work, as long as they give credit to the original creator and distribute any derivative works under the same or a similar license. 1.4 Can we measure everything? In order to conduct meaningful measurement we need to make sure that we have a good understanding about the concept in question and its units of measurement. Some concepts are easy to grasp and there is a broad consensus on how to measure them. Remember Figure 1.2 that shows a global surface temperature. Temperature can be measured in Celsius or Fahrenheit (SI units). There is an accepted translation between the Celsius and Fahrenheit, i.e. 0°C correspond to 32°F and 0°F correspond to -17.8°C. We have a good understanding of what 10°C means and even know how 10°C \"feels\". Still, sometimes (or most of the time?) we just don't have the data we like to have. Thus the graph shows a temperature reconstruction using indirect proxy data from tree rings, corals, and ice cores as well as directly observed data (when available). Definition Variables are manifest or observed when they are directly measurable or hidden or latent when they are \"idealized constructs at best only indirectly measurable\". -- Encyclopaedia of Statistical Sciences (1999) The number of Twitter follower at a given time is technically determined and can be counted. It is a natural number. Age is indisputable measured in years but it could also be measured in month or days. For most analytically purposes years will be fine. Exact birth dates may not be available due to data protection aspects. Some concepts are harder to grasp and require a specific argument. Think about intelligence, populism, happiness or humor. What exactly are they and how can they be measured? Definition Operationalization means turning abstract concepts into measurable observations. It involves clearly defining your variables and indicators.6 See this joke: Q: “What’s the difference between England and a tea bag? “ A: “The tea bag stays in the cup longer.” What would be the best way of measuring how funny the joke is? We could measure physiological responses to jokes, such as heart rate, respiration rate, or facial expressions. Researchers may use brain imaging techniques, such as functional magnetic resonance imaging (fMRI), to measure brain activity in response to jokes. Many times, for many analytically purposes this will be an overkill. It will be expensive in terms of money and time. Thus most of the time, we ask people how funny they rate this joke on a scale from 1 to 10 where 1 refers to \"not at all funny\" and 10 refers to \"extremely funny\"? It is tempting to measure all social phenomena on a scale from 1 to 10. Wechsler Adult Intelligence Scale (version IV, released 2008) measures intelligence. Scores from intelligence tests are estimates of intelligence. Unlike, for example, distance and mass, a concrete measure of intelligence cannot be achieved given the abstract nature of the concept of \"intelligence\". Once measured, concepts can be related to each other: What is the relation between age and Twitter usage? (See chapter Relationships) What is the effect of intelligence on happiness? (See chapter Regression) 1.5 The reality behind the data ... it is important not to lose sight of the individuals whose lives provide the data for the models. Although variables rather than individual people may become the subjects of the statistician’s narrative, it is individuals rather than variables who have the capacity to act and reflect on society. Elliott, 1999. In: Byrne (2002) Sometimes, statisticians may become so focused on the data and the patterns they observe that they forget about the individuals behind the data. But it's important to keep in mind that it is ultimately individuals who are affected by the decisions and policies that are informed by the data. People have the ability to act and reflect on society, and understanding their experiences and perspectives is critical to building models and making decisions that truly reflect the needs and values of society as a whole. Data from 1800 to 1950 comes from Gapminder v6, for 1950 to 2014 from UN estimates and 2015 to 2099 from UN forecasts of future fertility.↩︎ It is 2.1 rather than 2 to allow for the failure of some female children themselves to live through the fertile years of adult life.↩︎ According to Tian Zheng (Columbia College), the average American knows about 600 people. NY times https://www.nytimes.com/2013/02/19/science/the-average-american-knows-how-many-people.html↩︎ Read more: https://www.scribbr.com/methodology/operationalization/↩︎ "],["stories-and-visuals.html", "Chapter 2 Stories and Visuals 2.1 Facts 2.2 Visualization 2.3 Telling a story 2.4 Man's best friend 2.5 Less is more 2.6 Grammar of Graphics", " Chapter 2 Stories and Visuals Stories make data and numbers memorable. Stories are everywhere. When you think about the context of the data, stories evolve naturally. How does the data connect to you, your friends and family, the work environment or society as a whole. 2.1 Facts A good story is based on facts and reliable sources. When evaluating facts, it's important to consider the source of the information and the evidence supporting it. According to OECD data on part-time employment rate (2021):7 36 % of women worked part-time in Germany whereas only 10 % of men did. 2.2 Visualization Good data visualization helps to convey complex information in a way that is easily understandable and accessible to a wide range of people. By presenting data in a visually appealing and intuitive way, it can help people to identify patterns, trends, and relationships that might not be immediately apparent from a simple data table or text-based analysis. Why not just tell the numbers as is? An important aspect of data science is to communicate information clearly and efficiently. Complex data is made more accessible. Data visualization reveals the data. 2.3 Telling a story Data storytelling is the practice of using data and visuals to communicate a narrative or message to an audience. It involves combining data, analysis, and storytelling techniques to create a compelling and engaging narrative that can inform, persuade, and inspire action. Data storytelling is necessary and good because it helps people make sense of complex data and information. By presenting data in a clear and visually appealing way, data storytelling can help people understand the meaning behind the numbers, identify patterns and trends, and gain insights into important issues and problems. By weaving a compelling narrative around the data, we can help our audience to understand the insights that we have discovered and why they matter. A data story can also help to make the data more memorable and emotionally resonant, which can help to further engage our audience and increase their interest in the subject matter. Once upon a time ... in a land not too far away, there were two siblings named Alex and Jamie. Alex and Jamie were very close in age and grew up in the same household with the same parents, but they had very different personalities and interests. As they reached adulthood, Alex decided to pursue a career in finance and secured a full-time job at a large investment bank. Jamie, on the other hand, decided to focus on their passion for art and took on a part-time job as a freelance graphic designer while also working on personal creative projects. Over time, Alex and Jamie noticed a stark difference in the way their work and career choices were perceived by society. Alex was praised for their ambition and dedication to their career, while Jamie was often questioned or criticized for not having a full-time job with benefits and stability. Alex was also more likely to receive promotions and higher salaries, while Jamie struggled to make ends meet and was sometimes overlooked for opportunities because of their part-time status. It became clear to Alex and Jamie that there was a gendered expectation for men to pursue full-time, high-paying careers while women were expected to prioritize caregiving or creative pursuits over financial stability. Despite these societal expectations, Alex and Jamie continued to pursue their individual paths and support each other's choices. They hoped that someday, society would recognize the value and importance of all types of work and careers, regardless of gender or perceived societal norms. 2.4 Man's best friend Humans love dogs. Dogs were domesticated by humans over 15,000 years ago. They can be perfect companions for singles, for couples for families. They differ in behavior, longevity and appetite. Figure ?? combines 6 dog characteristics in a dog score and compares this with the popularity of different breeds. From this scatterplot the authors define four categories of dog breads, e.g. the hot dogs and overlooked treasures (similar to a BCG matrix). Figure 2.1: The Ultimate Dog Data by informationisbeautiful. On the one hand, this is an awesome chart that transforms a huge data table in one graph, a scatter plot on two dimensions lightened by the individual dog icons for each data point. On the other hand, this graph is so complex and does not offer a major takeaway. Looking more closely to the graph, raises more questions then the graph answers, i.e. how was grooming and appetite measured? How are the 6 factors combined in the data score? When you look even more closely, you notice the easter egg.8 2.5 Less is more Figure ?? is an awesome data aggregate. Still, \"less is more\" in data visualization because too much information or visual clutter can overwhelm and confuse the audience, making it harder for them to understand the key insights and trends in the data. The complex scatterplot demands a lot of attention. Follow the slide presentation: The concept of data-ink ratio was introduced by data visualization expert Edward Tufte. It refers to the proportion of ink or pixels used to represent actual data in a visualization, as opposed to non-data elements like gridlines, borders, or labels. The reason why we should care about data-ink ratio is that it directly affects the clarity and effectiveness of the visualization. The more ink or pixels we use to represent non-data elements, the less space we have to represent actual data, which can make it harder for the audience to discern the insights and trends in the data. Definition The data-ink ratio is the proportion of Ink that is used to present actual data compared to the total amount of ink (or pixels) used in the entire display. Good graphics should include only data-Ink. Non-Data-Ink is to be deleted everywhere where possible. 2.6 Grammar of Graphics ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. ggplot2 is now over 10 years old and is used by hundreds of thousands of people to make millions of plots. Search OECD database https://stats.oecd.org/index.aspx?queryid=54746#↩︎ The frightened cat among all dogs.↩︎ "],["tabular-data.html", "Chapter 3 Tabular Data 3.1 Types of Tabular Data", " Chapter 3 Tabular Data Tabular data is the most common type of data. The ability to organize information systematically into rows and columns has been a cornerstone of data-driven decision-making for centuries. From handwritten ledgers to digital spreadsheets, tabular data has served as the foundation for understanding, managing, and extracting insights from a wide range of datasets. Since tabular data is easy to understand and easy to handle, there are approaches to transform non-tabular data like text data into a tabular format. In the R realm this has been termed the tidy data principle. Each variable must have its own column. Each observation must have its own row. Each value must have its own cell.9 In this chapter, we embark on a journey through the world of tabular data, exploring its diverse facets and applications. We will delve into the art of manipulating, analyzing, and visualizing data in this familiar format, unlocking the potential it holds for uncovering hidden patterns and informing critical decisions. But our exploration goes further, as we introduce you to a special and dynamic subset of tabular data that adds an extra layer of complexity and depth to your analytical toolkit — panel data. Definition Tidy data principles are: Every column is a variable. Every row is an observation. Every cell is a single value. 3.1 Types of Tabular Data How can there be different types of tabular data when a table always consists of rows and columns? 3.1.1 Cross-section Look at the following example. This data is collected in 2024. Each row represents a different person (or unit), i.e. there are 6 women at different age and their respective income. Cross-sectional data is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the one point or period of time. It can answer questions about levels: \"How many people are poor in 2023 in Germany?\" and questions about differences: \"How are men and women affected by poverty?\". 3.1.2 Repeated cross-section Cross-sectional survey data are data for a single point in time. Repeated cross-sectional data are created where a survey (or measurement) is administered to a new sample of interviewees at successive time points. For an annual survey, this means that respondents in one year will be different people to those in a prior year. Such data can either be analysed cross-sectionally, by looking at one survey year, or combined for analysis over time. This type of data can answer questions about trends: \"Has poverty increased or decreased?\". 3.1.3 Time series Time series is data on a single subject at multiple points in time. Most commonly, data is collected at successive equally spaced points in time e.g. daily, annually. If data is collected annually, it's likely to be a survey study. If data is collected more frequently, e.g. daily, it's likely to be meteorology or finance. A time series is very frequently plotted via a run chart (which is a temporal line chart). Time series data can answer questions about trends: \"Is there a seasonal component in unemployment?\". 3.1.4 Panel data Panel data are observations for the same subjects over time. Subjects can be people, households, firms or countries. Panel data are a subset of longitudinal data. Key components are the panel identifier: person (id) and time (year). Every row is a person-year combination (so called long format). With panel data we know the time-ordering of events. Panel data allow to identify causal effects under weaker assumptions (compared to cross-sectional data). Panel data can answer questions about change: \"How many people went in and out of poverty?\". Read more about tidy data.↩︎ "],["panel-data-1.html", "Chapter 4 Panel Data 4.1 Unemployment 4.2 Application 4.3 Panel Studies", " Chapter 4 Panel Data 4.1 Unemployment Unemployment occurs when someone is willing and able to work but does not have a paid job. Unemployment is measured by the unemployment rate. The unemployment rate is the most commonly used indicator for understanding conditions in the labour market. The personal and social costs of unemployment include severe financial hardship and poverty, debt, homelessness and housing stress, family tensions and breakdown, boredom, alienation, shame and stigma, increased social isolation, crime, erosion of confidence and self-esteem, the atrophying of work skills and ill-health. McClelland and Macdonald (1998) 4.1.1 On decline in Germany Figure 4.1: Unemployment rate in Germany Reading How Low Can Unemployment Really Go? Economists Have No Idea \"Here are two things most economists can agree upon: They want an economy where everyone who seeks a job can get one. Yet for the economy to be dynamic, some people will always be unemployed, at least temporarily as they move between jobs.\" Life after college. Panel data allows to analyze the level of unemployment in Germany as well as the changes and trajectories of individuals. We can separate a frictional unemployment component and a permanent unemployment share. Frictional unemployment is a form of unemployment reflecting the gap between someone voluntarily leaving a job and finding another. As such, it is sometimes called search unemployment. Is search unemployment acceptable? Is it different from long-term unemployment? What do you think. 4.1.2 Measurement The unemployment rate represents the proportion of the civilian labour force that is unemployed. Consequently, measuring the unemployment rate requires identifying who is in the labour force. The labour force consists of all employed and unemployed persons of working age. What exactly is defined as employment? Employment status can be defined via a threshold of working hours or income. Who is in the working age? Reading In Australia, the Australian Bureau of Statistics (ABS) conducts a survey each month – called the Labour Force Survey – in which it asks around 50,000 people. As part of this survey, the ABS groups people aged 15 years and over (the working-age population) into three broad categories: Employed – includes people who are in a paid job for one hour or more in a week. Unemployed – people who are not in a paid job, but who are actively looking for work. Not in the labour force – people not in a paid job, and who are not looking for work. Read More: Unemployment: Its Measurement and Types 4.2 Application 4.2.1 Data Inspection SOEP practice data (2015 - 2019) comes labeled and ready for analysis. SOEP provides a digital object identifier (DOI) for this data: https://doi.org/10.5684/soep.practice.v36.10 library(haven) soep &lt;- read_dta(&quot;https://github.com/MarcoKuehne/marcokuehne.github.io/blob/main/data/SOEP/practice_en/practice_dataset_eng.dta?raw=true&quot;) This practice data contains socio-economic information on children, education, job, health, satisfaction and income. It contains 15 variables and 23522 observations. Unique (#) Missing (%) Mean SD Min Median Max syear 5 0 2016.8 1.4 2015.0 2017.0 2019.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } sex 2 0 0.5 0.5 0.0 1.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } alter 86 0 48.3 18.3 17.0 48.0 102.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } anz_pers 13 0 2.9 1.5 1.0 2.0 13.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } anz_kind 12 0 0.7 1.1 0.0 0.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } bildung 17 7 12.4 2.8 7.0 11.5 18.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } erwerb 7 0 3.0 1.8 1.0 2.0 6.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } branche 84 42 60.5 25.3 1.0 64.0 99.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } gesund_org 6 0 2.6 1.0 1.0 2.0 5.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } lebensz_org 12 3 7.4 1.7 0.0 8.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenj1 13346 0 16775.8 22707.6 0.0 5786.1 269424.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenj2 1871 0 315.7 1887.6 0.0 0.0 79179.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenm1 14067 0 1645.6 2220.6 0.0 852.9 35260.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenm2 607 0 11.0 134.0 0.0 0.0 12033.2 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Column names are German, but attribute labels are in English. einkommenj1 contains Gross Income from Main Job/Year. The documentation (click the DOI https://doi.org/10.5684/soep.practice.v36) tells that there are 6.355 people in the data. Every individual is likely observed multiple times (i.e. panel data). Pipe soep into count() of personal id. The tibble output already contains the number of rows. To literally access the value, ask nrow(). Adding arrange() means sorting the data by a variable (i.e. the temporarily created variable n) either ascending or descending (from high to low). Ascending is the default. For descending order apply the desc() command. group_by() is a powerful command, especially when working with panel data. It can do any form of data manipulation or analysis with respect to the chosen variable. At this stage it's a mere alternative count(). # soep %&gt;% group_by(id) %&gt;% count() %&gt;% arrange(n) # try this alternative soep %&gt;% count(id) %&gt;% arrange(n) soep %&gt;% count(id) %&gt;% arrange(desc(n)) See result. #&gt; # A tibble: 6,355 × 2 #&gt; id n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 96 1 #&gt; 2 137 1 #&gt; 3 147 1 #&gt; 4 183 1 #&gt; 5 189 1 #&gt; 6 229 1 #&gt; 7 368 1 #&gt; 8 371 1 #&gt; 9 384 1 #&gt; 10 443 1 #&gt; # ℹ 6,345 more rows Remember that the observation period is between 2015 and 2019, i.e. the minimum number of observations per individual is 1 year, the maximum is 5 years. Over the years, observations get less and less (panel attrition). table(soep$syear) #&gt; #&gt; 2015 2016 2017 2018 2019 #&gt; 5527 4987 4720 4351 3937 How many people are observed in all years? Filter for a specific n and ask for the number of rows or observations (alternatively you can check the tibble size again). erwerb is the employment status in SOEP. Its labels range from -7 to 6. Use the attributes() command on a specific variable. It returns a set of information (object class is list). We can access elements of this list by the $ operator. Did you notice the small mistake in the labels? (Remember this is practice data.) attributes(soep$erwerb)$labels See result. #&gt; [-7] Only available in less restricted edition #&gt; -7 #&gt; [-6] Version of questionnaire with modified filtering #&gt; -6 #&gt; [-5] Not included in this version of the questionnaire #&gt; -5 #&gt; [-4] Inadmissable multiple response #&gt; -4 #&gt; [-3] not valid #&gt; -3 #&gt; [-2] does not apply #&gt; -2 #&gt; [-1] no answer #&gt; -1 #&gt; [-1] Employed full-time #&gt; 1 #&gt; [-2] Employed part-time #&gt; 2 #&gt; [3] Training, apprenticeship #&gt; 3 #&gt; [4] Irregular employment or in marginal #&gt; 4 #&gt; [5] Not employed #&gt; 5 #&gt; [6] Garage for disabled people #&gt; 6 Note that the output further tells you # A tibble: 3,550 x 2, i.e. there are 3550 ID-groups (or units or people). Negative values indicate several different forms of missing data in SOEP.11 Actually, there are no negative values in this dataset. As for levels of a factor variables, labels can be empty. table(soep$erwerb) #&gt; #&gt; 1 2 3 4 5 6 #&gt; 8700 3481 695 1446 9169 29 4.2.2 Data Preparation We summarize categories with a combination of mutate() and case_when(). For each value in erwerb conduct a logical comparison via == and assign a new value. In this case it combines Employed part-time with Irregular employment or in marginal. soep &lt;- soep %&gt;% mutate(erwerb = case_when(erwerb == 1 ~ &quot;fulltime&quot;, erwerb == 2 ~ &quot;parttime&quot;, erwerb == 3 ~ &quot;parttime&quot;, erwerb == 4 ~ &quot;parttime&quot;, erwerb == 5 ~ &quot;unemployed&quot;, erwerb == 6 ~ &quot;parttime&quot;, TRUE ~ &quot;NA&quot;)) The unemployment rate represents the proportion of the civilian labour force that is unemployed. The labour force consists of all employed and unemployed persons of working age. Filter for working age between 18 and 67 years. There are 472 observations younger than 18 and 4125 older than 67. soep &lt;- soep %&gt;% filter(alter %in% c(18:67)) The moment of glory has come for group_by(). It accepts multiple inputs. The following combination returns for each year and each employment status the number of observations with help of summarise() and n(). NA See result. From this table, the unemployment rate for 2015 can be calculated manually: \\[\\frac{1268}{2013+1261+1268} = 0.279172 = 27.92 \\%\\] Having done this, step back to focus on years again and use different n-values per year to figure out the unemployment. Relate unemployed to those working either fulltime or partime. mutate(unemployment_rate = n[3]/(n[1]+n[2]+n[3])) See result. The unemployment rate in this data is unreasonably high. 4.2.3 Data Visualization 4.2.3.1 Unemployment status Pick up the calculation of the unemployment rate and pipe it into a ggplot() call. Define the axes elements within aes() (for aesthetics) and ask for points that are connected by a line. soep %&gt;% group_by(syear, erwerb) %&gt;% summarise(n = n()) %&gt;% group_by(syear) %&gt;% mutate(unemployment_rate = n[3]/(n[1]+n[2]+n[3])) %&gt;% ggplot(aes(x = syear, y = unemployment_rate)) + geom_point() + geom_line() + labs(x=&quot;Year&quot;, y=&quot;Unemployment Rate&quot;, title = &quot;Unemployment Rate in Germany between 2015 and 2019&quot;, subtitle = &quot;SOEP practice data&quot;) 4.2.3.2 Obs per individual Pick up the number of observations per individual and pipe it in a barplot with geom_bar(). soep %&gt;% group_by(id) %&gt;% count() %&gt;% ggplot(aes(x = n)) + geom_bar() + labs(title = &quot;Number of observations per individual&quot;) + theme_classic() 4.3 Panel Studies Famous household panel data studies include: United States: Panel Study of Income Dynamics (PSID) since 1968 Germany: Socio-Economic Panel (SOEP) since 1984 United Kingdom: British Household Panel Survey (BHPS) since 1991 Australia: Household, Income and Labour Dynamics in Australia Survey (HILDA) since 2001 These scientific datasets can often be analyzed for research and student theses free of charge. Why are DOIs important? A DOI is a unique identifier for a digital document. DOIs are important in academic citation because they are more permanent than URLs, ensuring that your reader can reliably locate the source. Read More: What is a DOI? | Finding and Using Digital Object Identifiers↩︎ Read more on the SOEPcompanion Missing Conventions↩︎ "],["time-data.html", "Chapter 5 Time data 5.1 Measuring Time 5.2 Measuring Dates 5.3 Your First Time (in R) 5.4 Time Zones 5.5 Time Management in R 5.6 Coffee Spending 5.7 Run Chart Grouped Cleaned", " Chapter 5 Time data Writing a short introduction about time and time data is a challenge. Time is considered the fourth dimension, alongside the three spatial dimensions (physics). Does time have a beginning and end (philosophy)? How does perception of time change in moments of joy compared to moments of stress (psychology)? How do our cells \"keep time,\" and what are the molecular mechanisms that regulate biological rhythms at the cellular level (biology)? In economics, time preference explores if people prefer the present over the future, tied to interest rates and economic cycles. Economists use time data, like GDP or stock prices, to forecast. Time is a measure of the duration between events or the intervals during which things happen. 5.1 Measuring Time Measuring time data involves assessing durations, intervals, and sequences. Common units of measurement include seconds, minutes, hours, days, and beyond. The precision of time measurement varies based on the application, ranging from macro-level timeframes in months or years to micro-level measurements in milliseconds or nanoseconds. time stamps for point in time The time system in which 1 hour is divided into 60 minutes, and each minute is further divided into 60 seconds, is called the sexagesimal or base-60 system. This system has been widely used in measuring time and angles (see chapter Geo Data). Amazing Fact In everyday conversation, we often use a combination of clock time and spatial metaphors to convey specific times or durations. Expressions like \"Quarter to 2\" are colloquial ways of indicating time, and they are commonly understood in various cultures. In these expressions, the clock face is imagined as a circle, and the position of the clock hands is described using spatial terms. 5.2 Measuring Dates A calendar date is a specific day within a calendar system, typically identified by a combination of the day, month, and year. It is a standardized way of expressing and referencing points in time. Calendar dates are used globally for various purposes, including scheduling events, recording historical events, and organizing daily life. The current date system used globally is the Gregorian calendar. The Gregorian calendar is a solar calendar introduced by Pope Gregory XIII in October 1582 to reform the earlier Julian calendar. It is the calendar system most widely used today for civil purposes. The Gregorian calendar is based on a 365-day year divided into 12 months. It includes leap years to account for the fact that a year is not precisely 365.25 days long. 5.3 Your First Time (in R) Sys.time() and Sys.Date() return the time and date on your system and they come in date formats. Sys.time() #&gt; [1] &quot;2024-02-11 09:16:01 CET&quot; The default format for dates and date-times in R is the ISO 8601 format. This format is widely used and unambiguous, representing the date in the format YYYY-MM-DD and the date-time in the format YYYY-MM-DD HH:MM:SS. It looks like a character, but is not. CET stands for Central European Time. CET is a time zone that is 1 hour ahead of Coordinated Universal Time (UTC+1). 5.4 Time Zones There are 24 time zones, each representing 15 degrees of longitude (see chapter Geo Data). Time zones are centered around the Prime Meridian (0 degrees longitude). Current time zones Amazing Fact Historically, China used five time zones, corresponding to its geographical expanse. However, in 1949, after the establishment of the People's Republic of China, the government decided to unify the country under a single time zone. China's decision to use a single time zone is rooted in the desire for national unity and centralized governance. Adopting a single time zone simplifies administration and coordination across the country. In addition to the standard time zones, some regions may observe daylight saving time (DST), which involves adjusting the clocks forward by one hour during the warmer months. This practice can result in an effective difference of two hours between neighboring time zones during the DST period. In summary, the relationship between CET, UTC, and DST is dynamic: During standard time (not observing DST), CET is UTC+1. During daylight saving time (DST), CET becomes CEST (Central European Summer Time) and is UTC+2. Australia, positioned ahead of numerous countries in the global time zones, stands among the foremost nations to usher in the New Year. As the first place to celebrate the New Year is typically in the Pacific region, Australia's geographical location allows it to be among the early heralds of the new calendar year. Notably, the city of Sydney has gained international acclaim for its legendary New Year's Eve festivities. The iconic celebrations in Sydney feature a breathtaking fireworks display illuminating the night sky over the Sydney Harbour Bridge and the Sydney Opera House, creating a dazzling spectacle that marks the beginning of the New Year in grandeur. library(tidyverse) library(lubridate) new_year_berlin &lt;- &quot;2023-12-31 23:59:59&quot; %&gt;% with_tz(&quot;Europe/Berlin&quot;) new_year_berlin #&gt; [1] &quot;2023-12-31 23:59:59 CET&quot; new_year_sydney &lt;- new_year_berlin %&gt;% with_tz(&quot;Australia/Sydney&quot;) new_year_sydney #&gt; [1] &quot;2024-01-01 09:59:59 AEDT&quot; 5.5 Time Management in R 5.5.1 Decimal Time How can we code one hour in R? Well, we can put numbers in a numeric vector. Combinations of hours, minutes and seconds can be represented by decimals, e.g. one and a half hours are 1.5 and 1 hour and 10 min corresponds to 1.166 hours. # one hour time &lt;- 1 # half hours hours &lt;- c(1, 1.5, 2.5) # 1 hour 10 min = 7/6 (in hours) one_hour_10_min &lt;- c(1.1666667) Well, fractions are a complicated way of representing minutes. 5.5.2 Time Formats In R, time formats are essential for handling and representing date and time information. Two commonly used time-related classes in R are Date and POSIXct. In Germany, it is more common to have DD-MM-YYYY, e.g. 24.12.2023. format() (or the format option in as.Date()) can change the internal representation: German_Date &lt;- c(&quot;24.12.2023&quot;) # This does not work without the format option #as.Date(German_Date) as.Date(German_Date, format = &quot;%d.%m.%Y&quot;) #&gt; [1] &quot;2023-12-24&quot; Note that date variables often come as character and need to be converted to a time format. In base R, there is no specific as.time() function for converting objects to time class objects. However, there are functions like as.POSIXct() and as.POSIXlt() for working with date and time information. Well, what is the benefit of converting to a time and date format? For example date arithmetic such as calculating the difference between dates (e.g. difftime()), adding or subtracting days, months, or years, and other operations. This is more straightforward and accurate than performing such operations on raw character strings. # Calculate the difference between two times time1 &lt;- as.POSIXct(&quot;2023-12-25 14:30:00&quot;, format = &quot;%Y-%m-%d %H:%M:%S&quot;) time2 &lt;- as.POSIXct(&quot;2023-12-25 15:45:00&quot;, format = &quot;%Y-%m-%d %H:%M:%S&quot;) difftime(time2, time1, units = &quot;mins&quot;) #&gt; Time difference of 75 mins 5.6 Coffee Spending The coffee data comes from the app Money Manager. It allows to track your spendings and export data to .xlsx format. Variables and date information is German. library(tidyverse) Coffee &lt;- read_csv(&quot;./data/Coffee/Coffee.csv&quot;) %&gt;% select(Date = Datum, Category = Kategorie, Note = Notiz...5, Amount = Betrag) glimpse(Coffee) #&gt; Rows: 150 #&gt; Columns: 4 #&gt; $ Date &lt;chr&gt; &quot;24/05/2023 12:40:44&quot;, &quot;24/05/2023 12:12:20&quot;, &quot;23/05/2023 12:… #&gt; $ Category &lt;chr&gt; &quot;Lebensmittelkosten&quot;, &quot;Lebensmittelkosten&quot;, &quot;Lebensmittelkost… #&gt; $ Note &lt;chr&gt; &quot;Kaffee&quot;, &quot;Mensa&quot;, &quot;Getränke&quot;, &quot;Mensa&quot;, &quot;Getränke&quot;, &quot;Mensa&quot;, … #&gt; $ Amount &lt;dbl&gt; 1.60, 6.73, 3.85, 6.73, 3.85, 4.78, 6.80, 1.60, 6.40, 3.85, 5… Change the character to a date format: Coffee$Date &lt;- as.POSIXct(Coffee$Date, format = &quot;%d/%m/%Y %H:%M:%S&quot;) head(Coffee$Date) #&gt; [1] &quot;2023-05-24 12:40:44 CEST&quot; &quot;2023-05-24 12:12:20 CEST&quot; #&gt; [3] &quot;2023-05-23 12:31:46 CEST&quot; &quot;2023-05-23 12:01:52 CEST&quot; #&gt; [5] &quot;2023-05-22 12:25:08 CEST&quot; &quot;2023-05-22 12:00:26 CEST&quot; What is the time interval of available data: # First and Last Date range(Coffee$Date) #&gt; [1] &quot;2022-10-26 07:03:04 CEST&quot; &quot;2023-05-24 12:40:44 CEST&quot; # Range in Days diff(range(Coffee$Date)) #&gt; Time difference of 210.2345 days 5.6.1 Spending Time of Day To calculate the usual time of spendings, calculate the average of all observed times. First, extract the time of day of spending with format() to create Coffee$time_of_day. #&gt; [1] &quot;12:40:44&quot; &quot;12:12:20&quot; &quot;12:31:46&quot; &quot;12:01:52&quot; &quot;12:25:08&quot; &quot;12:00:26&quot; Then, use the chron package to convert these times to the times format with times() function. Then apply mean() on the times object. #&gt; [1] 12:32:58 5.6.2 Run Chart Use scale_x_datetime() to show each month on the x-axis. ggplot(Coffee, aes(x = Date, y = Amount)) + geom_line() + labs(title = &quot;Running Chart with Time on X-Axis and Spending on Y-Axis&quot;, x = &quot;Time&quot;, y = &quot;Spending (in €)&quot;) + theme_minimal() + scale_x_datetime(date_breaks = &quot;1 month&quot;, date_labels = &quot;%b %Y&quot;) 5.6.3 Run Chart Grouped Colour different forms of spending from Note. 5.7 Run Chart Grouped Cleaned Filter other beverages and correct the mensq typo. "],["web-data.html", "Chapter 6 Web Data 6.1 Most expensive paintings 6.2 Student numbers at Viadrina", " Chapter 6 Web Data “Over the last two years alone, 90 percent of the data in the world was generated.” Bernard Marr (2018) How Much Data Do We Create Every Day? The Mind-Blowing Stats Everyone Should Read Forbes. Data scraping is a technique where a computer program extracts data from human-readable output coming from another program. Data scraping often takes place in web scraping (also known as crawling or spidering). In this process, an application is used to extract valuable information from a website. PDF scraping or more general report mining is the extraction of data from human-readable computer reports. It is worth considering alternatives before start scraping data.12 6.1 Most expensive paintings What do The Card Players and Marshall Islands have in common? Figure 6.1: The Card Players.   Figure 6.2: Flag of Marshall Islands. Well, the first was sold for about 250.000.000 million USD in an auction in 2011 and the laters nominal gross domestic product is of similar size 220.000.000 million USD in 2019. About 60.000 people live in Marshall Islands. The most expensive paintings score similar to the gross domestic product of insular states. This is the English Wikipedia article list of highest prices ever paid for paintings. In a web browser use the keyboard shortcut CTRL + U (in Google Chrome, Firefox, Microsoft Edge, Opera) to view the source code of a webpage. Alternatively, right click and choose \"show source code\". border:1px solid black; One approach is this. Use read_html() from rvest package to download the webpage. Extract the node table.wikitable with the html_nodes() command. Convert this node into a table with html_table() setting header=TRUE. Use this data in a pipe %&gt;% with bind_rows() and as_tibble(). You can use any other scraping approach. Store the final table of expensive paintings in paintings. The following DT table should yield identical results. 6.2 Student numbers at Viadrina Did you notice less and less fellow students sitting next to you? We analyse enrollment numbers of Viadrina European University. Dezernat 1 administers and publishes student statistics. There is an overview page where they provide some time series data (overall student numbers from 1992 to 2020) and the number of study programs (between 1992 to 2014). Information is presented in two ways, as tables in PDF, e.g. the time series on total student numbers:13 And as tables in HTML. For each semester there is summary information on the webpage (and more comprehensive data in PDF), like for winter term 2022/2023: Most web browser allow to inspect the html source: view-source:https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2022-wintersemester/index.html When you search for a characteristic value like 4.797 it shows: &lt;div class=&quot;zeile&quot;&gt;&lt;div class=&quot;dreispaltig&quot;&gt; &lt;div class=&quot;text&quot;&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;width: 400px;&quot;&gt;Studierende gesamt&lt;/td&gt; &lt;td&gt;4.797&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;weiblich&lt;/td&gt; &lt;td&gt;2.785&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;m&amp;auml;nnlich&lt;/td&gt; &lt;td&gt;2.012&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Deutsche&lt;/td&gt; &lt;td&gt;3.201&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ausl&amp;auml;nder/innen&lt;/td&gt; &lt;td&gt;1.596&lt;/td&gt; Let's investigate the student numbers. 6.2.1 PDF scraping Apply a programmatic way to download a PDF Use the built in download.file() function and specify an url and a destination and file name. # Set URL for PDF url &lt;- &quot;https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/Entwicklung-der-Gesamtstudierendenzahl.pdf&quot; # Specify destination where file should be saved (and name) destfile &lt;- &quot;data/Viadrina/Gesamtstudierendenzahl.pdf&quot; # Apply download.file function in R download.file(url, destfile, mode = &quot;wb&quot;) To extract a table from a PDF we use the pdftables package.14 The package is a wrapper for the PDFTables API. It requires an API key from https://pdftables.com/. You can register and get one for free. The result is stored as .csv file. Definition An application programming interface (API) is a way for two or more computer programs to communicate with each other. It is a type of software interface, offering a service to other pieces of software. Be careful with your API keys. If you only use a file locally on your computer, you might be fine. Don't share this file. Don't upload it. If you upload an API key on GIT, you get a notification mail from https://www.gitguardian.com/. Instead, put your API in your environment. This can be done by a .Renviron file. Use usethis::edit_r_environ(scope = \"project\") in order to access and edit your information. 50 pages are for free. Read more * https://daattali.gitbooks.io/stat545-ubc-github-io/content/bit003_api-key-env-var.html * https://resources.numbat.space/using-rprofile-and-renviron.html * https://github.com/expersso/pdftables NA Scraped data often requires a lot of cleaning. library(tidyverse) # read csv file viadrina_1992_2020_before &lt;- read.csv(&quot;data/Viadrina/viadrina_students.csv&quot;, header = TRUE) viadrina_1992_2020 &lt;- viadrina_1992_2020_before # replace header names by first row names(viadrina_1992_2020) &lt;- viadrina_1992_2020[1,] # drop first row viadrina_1992_2020 &lt;- viadrina_1992_2020[-1,] # drop last row viadrina_1992_2020 &lt;- viadrina_1992_2020[-30,] # remove all \\n colnames(viadrina_1992_2020) &lt;- gsub(&quot;[\\r\\n]&quot;, &quot;&quot;, colnames(viadrina_1992_2020)) # readable column names colnames(viadrina_1992_2020) &lt;- c(&quot;Year&quot;, &quot;Total&quot;, &quot;Female&quot;, &quot;Female_Pct&quot;, &quot;German&quot;, &quot;German_Pct&quot;, &quot;Foreign&quot;, &quot;Foreign_Pct&quot;, &quot;Pole&quot;, &quot;Pole_Pct&quot;) # remove percentage sign viadrina_1992_2020 &lt;- viadrina_1992_2020 %&gt;% mutate(across(everything(), ~ ifelse(str_detect(.x, &quot;%&quot;), parse_number(.x) / 10, .x))) # convert all chr to numeric viadrina_1992_2020 &lt;- viadrina_1992_2020 %&gt;% mutate_if(is.character,as.numeric) library(DT) datatable(viadrina_1992_2020) 6.2.2 Share of female students The share of female students over time in a line plot. viadrina_1992_2020 %&gt;% ggplot(aes(x=Year)) + geom_line(aes(y = Total, colour = &quot;Total&quot;)) + geom_line(aes(y = Female, colour = &quot;Female&quot;)) + labs(title = &quot;Student numbers at Viadrina&quot;, subtitle = &quot;Share of female students&quot;) 6.2.3 Share of foreign students The share of foreign students over time in a stacked barplot. viadrina_1992_2020 %&gt;% ggplot(aes(x=Year)) + geom_col(aes(y = Total, fill = &quot;Total&quot;)) + geom_col(aes(y = German, fill = &quot;German&quot;)) + geom_col(aes(y = Foreign, fill = &quot;Foreign&quot;)) + labs(title = &quot;Student numbers at Viadrina&quot;, subtitle = &quot;Share of foreign students&quot;) 6.2.4 Web scraping To get more recent numbers, we rely on the webpage information. How can we access all semester data? Investigate the pattern of the URLs. https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2013-Wintersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2014-Sommersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2014-Wintersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2015-Sommersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2015-wintersemester/index.html Usually the pattern is year-semester. Some winter semesters are capitalized, some are not (Wintersemester vs. wintersemester). We use the rvest package to web scrape static html content. It always starts with reading in the html data. Then pipe and use html_table() to extract tabular information (lucky us, there is only one table). library(rvest) read_html(&quot;https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2013-Wintersemester/index.html&quot;) %&gt;% html_table() #&gt; [[1]] #&gt; # A tibble: 7 × 2 #&gt; X1 X2 #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Studierende gesamt 6645 #&gt; 2 weiblich 4206 #&gt; 3 männlich 2439 #&gt; 4 Deutsche 5001 #&gt; 5 Ausländer/innen 1644 #&gt; 6 1. Fachsemester 1783 #&gt; 7 1. Hochschulsemester 1110 Now, we would like to do this for every subpage (semester) and combine the data in one table. Focus on the variable component in the URL and create a vector of all semesters. # the manual way winters &lt;- seq(from=2013, to=2022) summers &lt;- seq(from=2014, to=2023) winters &lt;- paste0(winters, &quot;-wintersemester&quot;) summers &lt;- paste0(summers, &quot;-Sommersemester&quot;) all_terms &lt;- c(rbind(winters, summers)) all_terms[1] &lt;- &quot;2013-Wintersemester&quot; all_terms[3] &lt;- &quot;2014-Wintersemester&quot; # we can paste them together all_url &lt;- paste0(&quot;https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/&quot;, all_terms, &quot;/index.html&quot;) Create a for loop to repeat the rvest procedure for each element in the URL list. Definition A loop is a programming structure that repeats a sequence of instructions until a specific condition is met. There are two more components. We initialize a list to store each iteration of the loop. tables &lt;- list() index &lt;- 1 for(i in 1:length(all_url)){ table &lt;- all_url[i] %&gt;% read_html() %&gt;% html_table() tables[index] &lt;- table index &lt;- index + 1 } df &lt;- do.call(&quot;cbind&quot;, tables) More and more cleaning. df[,c(seq(from=3, to=40 , by=2))] &lt;- NULL colnames(df) &lt;- c(&quot;Variable&quot;, all_terms) # transpose dataframe viadrina_2013_2023 &lt;- as.data.frame(t(df)) # replace header names by first row names(viadrina_2013_2023) &lt;- viadrina_2013_2023[1,] # drop first row viadrina_2013_2023 &lt;- viadrina_2013_2023[-1,] Once more, there are slightly different formats for numbers in winter term 2019, 2020 and 2021. In these terms they use a . for digit grouping, e.g. 3.607 instead of 3607. # all chr to numeric # viadrina_2013_2022 &lt;- viadrina_2013_2022 %&gt;% mutate_if(is.character,parse_number) #viadrina_2013_2022 &lt;- viadrina_2013_2022 %&gt;% mutate_if(is.character,as_numeric) viadrina_2013_2023 &lt;- viadrina_2013_2023 %&gt;% mutate_if(is.character,as.numeric) # row 13, 15, 17, columns 1 to 6, multiply by 1000 viadrina_2013_2023[c(13,15,17,19), c(1:6)] &lt;- viadrina_2013_2023[c(13,15,17,19), c(1:6)] * 1000 # round all numbers # viadrina_2013_2022 &lt;- viadrina_2013_2022 %&gt;% mutate_if(is.numeric, round) # multiply some rows by 1000 # viadrina_2013_2022[c(13,15,17),] &lt;- viadrina_2013_2022[c(13,15,17),] %&gt;% # mutate_all(.funs = funs(. * 1000)) # fix last column and divide by 1000 # viadrina_2013_2022[c(13,15,17),7] &lt;- viadrina_2013_2022[c(13,15,17),7]/1000 # create a semester variable viadrina_2013_2023$year &lt;- substr(all_terms, start = 1, stop = 4) viadrina_2013_2023$term &lt;- rep(c(&quot;winter&quot;, &quot;summer&quot;), 10) viadrina_2013_2023$semester &lt;- paste0(viadrina_2013_2023$year, rep(c(&quot;-02&quot;, &quot;-01&quot;), 10)) # row names rownames(viadrina_2013_2023) &lt;- 1:nrow(viadrina_2013_2023) 6.2.5 Most recent student numbers There is a structural difference between summer and winter term. Most new enrollments are in winter. viadrina_2013_2023 %&gt;% ggplot(aes(x=semester, y = `Studierende gesamt`)) + geom_point(aes(col=term), size=2) + #geom_point(aes(y = `Studierende gesamt`, fill = &quot;Total&quot;)) + #geom_point(aes(y = `1. Fachsemester`, fill = &quot;First years&quot;)) + #geom_point(aes(y = `1. Hochschulsemester`, fill = &quot;First sem&quot;)) + labs(title = &quot;Student numbers at Viadrina&quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 6.2.6 The long run trend Combine old and new data. via_1992_2020 &lt;- viadrina_1992_2020 %&gt;% select(Year, Total) %&gt;% mutate(Term = &quot;winter&quot;) via_2013_2023 &lt;- viadrina_2013_2023 %&gt;% rename(Year = year, Term = term, Total = `Studierende gesamt`) %&gt;% select(Year, Total, Term) via_1992_2023 &lt;- rbind(via_1992_2020, via_2013_2023) via_1992_2023$Year &lt;- as.numeric(via_1992_2023$Year) Plot and polish. Assume the enrollment for winter 2020/2021 was not affected by Corona virus. via_1992_2023 %&gt;% #filter(Term == &quot;winter&quot;) %&gt;% ggplot(aes(x=Year, y=Total, col = Term)) + geom_point() + geom_smooth(method = &quot;gam&quot;) + scale_x_continuous(breaks = scales::pretty_breaks(n = 28)) + labs(title = &quot;Student numbers at Viadrina&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + geom_vline(xintercept=2020.5, colour=&quot;grey&quot;, linetype = &quot;solid&quot;, lwd = 1.3) + geom_text(aes(x=2021, y=4000, label=&quot;Corona&quot;), colour=&quot;red&quot;, angle=90, vjust = 1) (1) Look for a download button. (2) Search same or similar data somewhere else. (3) Check if there is an API. (4) Ask the website owner for the data.↩︎ Get the PDF https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/Entwicklung-der-Gesamtstudierendenzahl.pdf↩︎ You may use other free services. Search for Online converter PDF to csv/xlsx↩︎ "],["geo-data.html", "Chapter 7 Geo Data 7.1 Geo coordinates 7.2 Google Takeout 7.3 Blood Donation", " Chapter 7 Geo Data Geo data, short for geographic data, is a fundamental aspect of our digital world that captures information about locations on Earth. It encompasses a wide range of spatial data, including coordinates, addresses, and attributes tied to specific geographic points. Geo data plays a pivotal role in shaping our everyday experiences, influencing navigation, enhancing services, and providing valuable insights across various sectors. One of the most prevalent ways individuals interact with geo data is through the Global Positioning System (GPS) embedded in their mobile phones. When you use GPS on your smartphone, you're tapping into a vast network of satellites orbiting above. These satellites beam signals to your device, allowing it to pinpoint your exact location with remarkable accuracy. Analyzing and visualizing geospatial data is essential in various fields, including geography, environmental sciences, urban planning, epidemiology, and transportation not to mention social science. Google Route from Berlin to Munich. 7.1 Geo coordinates 7.1.1 Where Are You? Websites such as GPS Coordinates offer the capability to determine your precise location on Earth. By accessing these platforms, users can obtain a visual representation of their current surroundings through a map view, along with corresponding geo-coordinates. This valuable service not only provides a real-time snapshot of your location but also offers the convenience of easily retrieving accurate geographic coordinates, enhancing navigation and location-aware applications. Ask Google Maps for Viadrina European University returns two numbers 52.342977500409994, latitude and 14.555877070488613 latitude. 7.1.2 Latitude and longitude Latitude and longitude are geographic coordinates used to specify locations on the Earth's surface. They are used to precisely determine a point's position in terms of its north-south and east-west positions. (#fig:lon_lat)Longitude lines are perpendicular to and latitude lines are parallel to the Equator. Latitude measures the north-south position of a point on the Earth. The equator is defined as 0 degrees latitude, and it divides the Earth into the Northern Hemisphere (positive latitudes) and the Southern Hemisphere (negative latitudes). The range of latitude extends from -90 degrees (South Pole) to +90 degrees (North Pole). Longitude measures the east-west position of a point on the Earth. It is also measured in degrees, with the Prime Meridian serving as the reference point. The Prime Meridian, located at Greenwich, London, is defined as 0 degrees longitude. Longitude lines extend from the Prime Meridian to the International Date Line, which is roughly 180 degrees longitude. The range of longitude extends from -180 degrees to +180 degrees. Well, who invented latitude and longitude and why is latitude positive in the north and where is the North anyway? History The decision to have latitude positive in the north and negative in the south is essentially arbitrary. The convention was established to provide a consistent and universally accepted reference frame for geographic coordinates. It was likely influenced by the fact that most early civilizations and cartographers were based in the northern hemisphere. Regarding the choice of the prime meridian (0 degrees longitude) passing through Greenwich, England, it was largely due to historical reasons and the influence of the British Empire. The concept of establishing a prime meridian dates back to the 19th century when international cooperation in navigation and mapping was increasing. In 1884, at the International Meridian Conference held in Washington, D.C., representatives from various countries agreed to adopt the Greenwich Meridian as the Prime Meridian, mainly because the British Royal Observatory in Greenwich was already internationally recognized for its contributions to astronomy and navigation. Europe is a continent located entirely in the Northern Hemisphere and mostly in the Eastern Hemisphere. It is situated to the west of Asia, separated by the Ural Mountains, the Ural River, the Caspian Sea, the Caucasus Mountains, and the Black Sea. Here are approximate latitude and longitude ranges for Europe: Latitude Range: Approximately 35 degrees North to 71 degrees North. Longitude Range: Approximately 25 degrees West to 45 degrees East. (#fig:europe_lat_long)Europe c. 650. 7.1.3 Angles and Degrees The intuition is that latitude and longitude are angles. Angular measurements are commonly expressed in units of degrees, minutes, and seconds (DMS). 1 degree equals 60 minutes, and one minute equals 60 seconds. Berlin, the capitol of Germany is located: Latitude: 52° 31' 12\" N Longitude: 13° 24' 18\" E in DMS. In decimal degrees (DD) we get: \\[\\text{Decimal degree} = \\text{Degree} + \\frac{\\text{Minute}}{60} + \\frac{\\text{Minute}}{3600}\\] Latitude: 52.5200° N Longitude: 13.4050° E In general, coordinates with six decimal places (0.000001 degrees) can provide location accuracy to approximately within a few centimeters. Each additional decimal place adds further precision, narrowing down the location to smaller units of measurement. The entrance of Viadrina main building and the best coffee in town are about 40m away. Latitude is different in the 4th and longitude differs in the 3rd decimal. name address lat lon Viadrina Main Building Große Scharrnstr. 52.34228 14.55387 Best Coffee In Town Große Scharrnstr. 52.34212 14.55459 7.1.4 Coordinate Reference System A Coordinate Reference System (CRS) is a framework used to uniquely identify locations on the Earth's surface. It defines how geographic coordinates (latitude and longitude) are mapped to positions on a two-dimensional surface, whether it's a map or a computer screen. Common CRS formats include: Geographic CRS (GCS): Based on a three-dimensional ellipsoidal model of the Earth. Examples include WGS 84 (World Geodetic System 1984) and NAD83 (North American Datum 1983). Projected CRS: Represents locations on a two-dimensional plane (e.g., a map). Examples include UTM (Universal Transverse Mercator) and Web Mercator. The European Petroleum Survey Group Geodesy (EPSG) is best known for its system of spatial reference IDs for coordinate systems. An EPSG code is a unique ID that can be a simple way to identify a CRS. library(rnaturalearth) # Get geometries of countries (default CRS is WGS 84) world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) # Check the CRS in world st_crs(world)$input #&gt; [1] &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; The default CRS is WGS 84 (World Geodetic System 1984) which is a widely used global geodetic datum and coordinate system, and it is based on an ellipsoidal model of the Earth. Below there are two versions of Germany. The left version shows Germany from the WGS 84 (st_transform(germany, crs = 4326)). The right version shows Germany seen from Iceland (st_transform(germany, crs = 5325)). Test this yourself. Go to Google Maps or Google Earth and navigate to the middle of Iceland. Now, look how the shape of Germany changes. 7.1.5 Distance measurement Distances are determined by the choice of a distance formula, and considerations such as accuracy requirements, computational efficiency, and the specific shape of the Earth's surface in the area of interest come into play. There are various methods, each tailored to specific contexts: Euclidean distance (beeline distance in two dimensions) calculates the straight-line distance between two points in a Cartesian coordinate system (2D). Haversine distance is the distance between two points on a sphere given their longitudes and latitudes (3D). The earth is not perfectly round. Vincenty distance assumes an ellipsoid instead. library(geosphere) # Define the coordinates of New York City and London coord_nyc &lt;- c(40.7128, -74.0060) # New York City coord_ldn &lt;- c(51.5074, -0.1278) # London # Haversine Distance haversine_distance &lt;- distHaversine(coord_nyc, coord_ldn) # Vincenty Sphere Distance sphere_distance &lt;- distVincentySphere(coord_nyc, coord_ldn) # Vincenty Ellipsoid Distance ellipsoid_distance &lt;- distVincentyEllipsoid(coord_nyc, coord_ldn) # Create a data frame distance_data &lt;- data.frame( Method = c(&quot;Haversine&quot;, &quot;Vincenty Sphere&quot;, &quot;Vincenty Ellipsoid&quot;), Distance = c(haversine_distance, sphere_distance, ellipsoid_distance) ) # Display the data frame as a styled HTML table kable(distance_data, format = &quot;html&quot;) %&gt;% kable_styling(&quot;striped&quot;, full_width = FALSE) %&gt;% add_header_above(c(&quot;Distance between New York and London&quot; = 2)) Distance between New York and London Method Distance Haversine 8256431 Vincenty Sphere 8256431 Vincenty Ellipsoid 8234358 7.1.6 Points And Polygons Polygon data and geographical coordinates work together to represent and visualize spatial areas on a map. In a geographical context, polygons are commonly used to define the boundaries of regions, countries, cities, or any other geographic entities. # Load necessary libraries library(sf) library(ggplot2) # Define coordinates of a simple polygon representing Germany germany_polygon &lt;- st_polygon(list(cbind(c(5, 15, 15, 5, 5), c(47, 47, 55, 55, 47)))) # Create an sf object with the polygon germany_sf &lt;- st_sf(geometry = st_sfc(germany_polygon)) # Plot the polygon ggplot() + geom_sf(data = germany_sf, fill = &quot;lightblue&quot;, color = &quot;blue&quot;) + ggtitle(&quot;Abstract Polygon of Germany&quot;) + theme_minimal() Polygon data for countries typically comes from geospatial databases, GIS (Geographic Information System) data, or shapefiles maintained by government agencies, international organizations, or open-source initiatives. (#fig:ger_shape)Shape of Germany. 7.1.7 Shapefiles Shapefiles are commonly used to represent geographic data. Shapefiles are a popular geospatial vector data format that stores both geometric and attribute information about geographic features. They are widely used in geographic information system (GIS) applications and can be easily imported and manipulated in R using various packages such as sf, rgdal, or maptools. Truly Dedicated These files together make up a shapefile, a widely used format for storing geospatial vector data. The combination of geometric and attribute data, along with coordinate system information, allows for the representation of complex geographic features in a structured and interoperable manner. .dbf File .dbf File: This file is the attribute table file associated with the shapefile. It stores the attribute data for each geographic feature in a tabular format. The attributes can include information such as names, IDs, population, or any other relevant data associated with the features. The .dbf file follows the dBase file format and can be accessed using functions like `read.dbf()` or `read.dbf()` in R. .prj File .prj File: This file contains the coordinate reference system (CRS) information for the shapefile. It specifies the spatial reference system and projection details, such as the coordinate units, projection method, and datum used. The CRS information is crucial for correctly interpreting and aligning the spatial data in the shapefile. In R, the CRS information can be accessed or set using functions provided by the sf package, such as st_crs() or st_set_crs(). .shp File .shp File: This file stores the actual geometric data of the shapefile. It contains information about the shape, size, and location of each geographic feature, such as points, lines, or polygons. Each feature is represented by a set of vertices or coordinates. .shx File .shx File: This file is the shapefile index file. It provides a quick lookup or index of the geometric features in the shapefile. It helps in efficiently accessing specific features without reading the entire shapefile. Spatial Scale In determining the unit and scale for spatial analysis, it is crucial to align choices with research objectives and the analysis scope, whether at national, regional, or local levels. Considerations include evaluating data availability and granularity to match the resolution of spatial data. Geographical Entities Spatial or geographic entities, such as regions, countries, and cities, often refer to political boundaries. Shapefiles, typically sourced from official channels, primarily represent these boundaries. Political borders, administrative divisions, and state or province limits are commonly depicted. Additionally, shapefiles extend to transportation networks, capturing roads and railways for effective planning. These versatile files also cover topography, offering insights into elevation, land cover, and contour lines. Points of interest, like landmarks and businesses, are included, making shapefiles invaluable for applications in land management, urban planning, climate analysis, and demographic mapping. Overall, shapefiles serve as indispensable tools for visualizing and analyzing spatial data across diverse domains. 7.2 Google Takeout Google Takeout is a service provided by Google that allows users to export their data from various Google products. Users can request an archive of their data, which is then compiled into downloadable files. One of the types of data that users can request through Google Takeout is location data. The Google Takeout in JSON format comes with nest variables in R. The class of takeout is a data.frame. # Load JSON data json_data &lt;- fromJSON(&quot;https://raw.githubusercontent.com/yashsakhuja/Cleaning-Google-Takeout-JSON-Data-to-check-places-visited/main/2023_JANUARY.json&quot;) # Convert to data frame takeout &lt;- as.data.frame(json_data) # Filter NA takeout &lt;- takeout %&gt;% filter(!is.na(takeout$timelineObjects.placeVisit$location$name)) # Convert E7 format to numeric for latitude and longitude takeout$latitude &lt;- takeout$timelineObjects.placeVisit$location$latitudeE7 / 1e7 takeout$longitude &lt;- takeout$timelineObjects.placeVisit$location$longitudeE7 / 1e7 takeout$name &lt;- takeout$timelineObjects.placeVisit$location$name # Create a leaflet map library(htmlwidgets) leaflet(takeout) %&gt;% addTiles() %&gt;% addMarkers(~longitude, ~latitude, label = ~name) 7.3 Blood Donation Blood donation is a vital contribution to healthcare, saving countless lives every day. In Germany, the German Red Cross (Deutsches Rotes Kreuz) plays a crucial role in organizing and facilitating blood donation events across the country. You can find information about blood donation venues on their official website: https://www.blutspenden.de/blutspendedienste/ The code fetches, processes, and extracts information about blood donation locations in Germany from a website, organizing the data into a tidy tibble for further analysis and mapping. library(&#39;rvest&#39;) library(&#39;stringr&#39;) library(&#39;stringi&#39;) library(&#39;jsonlite&#39;) rubyhash &lt;- &quot;https://www.blutspenden.de/blutspendedienste/#&quot; %&gt;% read_html() %&gt;% html_nodes(&quot;body&quot;) %&gt;% html_nodes(&quot;script:first-of-type&quot;) %&gt;% html_text() %&gt;% as_tibble() %&gt;% slice(1) blood &lt;- rubyhash$value %&gt;% str_replace(&quot;var instituionsmap_data = &#39;\\\\[\\\\{&quot;, &quot;&quot;) %&gt;% str_replace(&quot;\\\\}\\\\]&#39;;\\n&quot;, &#39;&#39;) %&gt;% # Removes the javascript chars at the end strsplit(&#39;\\\\},\\\\{&#39;) %&gt;% # Split into component json strings getElement(1) %&gt;% sapply(function(x) paste0(&#39;{&#39;, x, &#39;}&#39;), USE.NAMES = FALSE) %&gt;% lapply(function(x) as.data.frame(fromJSON(x))) %&gt;% bind_rows() %&gt;% as_tibble() blood &lt;- blood %&gt;% mutate(lat = as.numeric(lat), lon = as.numeric(lon)) library(DT) #blood %&gt;% select(title, street, number, zip, city, lat, lon) %&gt;% datatable() blood %&gt;% select(title, street, number, zip, city, lat, lon) %&gt;% datatable( options = list( dom = &#39;t&#39;, # Display only the table scrollX = TRUE, # Enable horizontal scrolling pageLength = 10, # Number of rows per page lengthMenu = c(10, 25, 50) # Set custom page lengths ), callback = JS(&#39; table.rows().nodes().to$().css({&quot;font-size&quot;: &quot;10px&quot;, &quot;padding&quot;: &quot;5px&quot;}); &#39;) ) library(&quot;maps&quot;) # Retrieve the map data for Participant Countries Germany_map &lt;- map_data(&quot;world&quot;, region = &quot;German&quot;) ggplot(Germany_map, aes(x = long, y = lat, group = group)) + theme(plot.title = element_text(hjust = 0.5)) + geom_polygon(fill = &quot;lightgray&quot;, colour = &quot;white&quot;) + scale_fill_viridis_d() + theme_void() + scale_color_manual(name = &quot;Locations&quot;, values = c(&quot;red&quot;, &quot;blue&quot;)) + geom_point(data = blood, aes(x = lon, y = lat, group = title), shape = 16, col = &quot;red&quot;) + theme( plot.background = element_rect(fill = &quot;white&quot;), aspect.ratio = 1.2, # Adjust the aspect ratio as needed plot.margin = margin(0.5, 0.5, 0.5, 0.5, &quot;cm&quot;) # Adjust margins as needed ) "],["missing-data.html", "Chapter 8 Missing Data 8.1 Types of Missing Data 8.2 Causes of Missing Data 8.3 Causes of Missing Data 8.4 Causes of Missing Data 8.5 Causes of Missing Data 8.6 Missing Data in R", " Chapter 8 Missing Data Well, missing data refers to the absence of values in a dataset where information is expected. It can introduce bias and impact the validity of statistical analyses. 8.1 Types of Missing Data Understanding the nature of missing data is crucial for selecting appropriate methods to handle it in statistical analyses. Missing Completely at Random (MCAR) Missing at Random (MAR) Missing Not at Random (MNAR) 8.1.1 Missing Completely at Random (MCAR) Missing data is completely random if the likelihood of missing a value is the same for all observations, regardless of the observed values. Imagine you are conducting a survey on people's favorite colors, and some respondents accidentally skip the question due to distractions or oversights. If the likelihood of skipping the question is the same for all respondents, regardless of their actual favorite color, then the missing data is considered completely random (MCAR). 8.1.2 Missing at Random (MAR) Missing data is at random if the likelihood of missing a value depends on observed information but not on the unobserved (missing) values. In a study on income, respondents might be more likely to skip the income question if they are younger. However, once you know the respondent's age, the likelihood of missing the income information is the same for people with the same age. The missing data is at random because it depends on observed information (age) but not on the unobserved variable (income). 8.1.3 Missing Not at Random (MNAR) Missing data is not at random if the likelihood of missing a value is related to the unobserved (missing) values themselves. Consider a health survey where individuals with higher levels of stress are less likely to report their stress levels accurately. In this case, the missing data (unreported stress levels) is related to the unobserved variable (stress). This scenario is considered missing not at random (MNAR) because the likelihood of missing data depends on the unobserved variable. 8.2 Causes of Missing Data - Human error during data entry - Technical issues in data collection - Intentional non-response by participants 8.3 Causes of Missing Data Intentional Non-response by Participants: Example: Respondents may choose not to answer specific questions due to privacy concerns, sensitivity, or personal reasons. Impact: Intentional non-response can introduce bias and affect the representativeness of the collected data. 8.4 Causes of Missing Data Data Cleaning and Preprocessing Errors: Example: Mistakes made during data cleaning or preprocessing stages can inadvertently introduce missing values. Impact: Errors in data preparation can affect the accuracy of downstream analyses and interpretations. 8.5 Causes of Missing Data Changes in Measurement Protocols: Example: Modifications in measurement methods or instruments over time can lead to inconsistencies and missing data. Impact: Changes in protocols may result in data incompatibility, making it challenging to analyze trends over different periods. What happened in SOEP for firm size? See Paneldata.org or Documentation 8.6 Missing Data in R In R, missing values are represented by the symbol NA (not available). Impossible values (e.g., dividing by zero) are represented by the symbol NaN (not a number). Unlike SAS, R uses the same symbol for character and numeric data. "],["relationships.html", "Chapter 9 Relationships 9.1 Storks Deliver Babies 9.2 Statistics 9.3 Visualizations 9.4 Spurious Relationships Readings", " Chapter 9 Relationships “[M]y ally is the Force, and a powerful ally it is. Life creates it, makes it grow. Its energy surrounds us, binds us. Luminous beings are we, not this crude matter. You must feel the Force flow around you. Here, between you, me, the tree, the rock, yes, even between the land and the ship.” Yoda. -- Episode V: The Empire Strikes Back \"I'm not talking about pagan voodoo here - I'm talking about something REAL and measurable in the biology of the forest. What we think we know is that there's some kind of electrochemical communication between the roots of the trees, like the synapses between neurons. Each tree has ten to the fourth connections to the trees around it, and there are ten to the twelfth trees on Pandora. That's more connections than the human brain. It's a network - a global network. The Na'vi can access it, they can upload and download data - memories - at sites like the one you just destroyed.\" Dr. Grace Augustine. -- Avatar Welcome to a chapter where we explore the connections that make our social world tick. Think about how Yoda talks about the Force binding everything in the Star Wars galaxy and how Dr. Grace Augustine explains the real, measurable connections in Pandora's forest. Well, social scientists do something similar in our world – we study the relationships that shape our lives. A relationship is the way in which two or more concepts are connected. Consider these examples: Can money truly bring happiness (Easterlin 1973)? Intriguing examples abound, such as exploring the relationship between social media use and sleep quality (Alonzo et al. 2021), investigating the impact of attachment styles on adult romantic relationships (Feeney and Noller 1990), and studying the interplay between gender diversity and team performance (Schneid et al. 2015). Additionally, researchers investigate the connection between parental involvement and academic achievement (Wilder 2014), and scrutinize how perceived crime levels influence quality of life (Kitchen and Williams 2010). The study conducted by Heim and Heim (2023) further delves into the secrets behind enduring relationships, with younger couples seeking insights on factors like commitment, altruism, shared values, good communication, compromise, love, and persistence from older couples with over 40 years of marriage experience (Heim and Heim 2023). In this video I'll explain main parts of this chapter. 9.1 Storks Deliver Babies Figure 9.1: Stork bringing baby - Colmar, Alsace. The relationship between the number of storks and the human population is a classic example used to illustrate that correlation does not imply causation (Matthews 2000). It is based on the familiar folk tale that babies are delivered by storks. People noticed a positive correlation between the number of storks in an area or country and the number of human babies born. Stork populations and human populations seem to increase or decrease together. Although storks are not responsible for delivering babies, a careless interpretation of correlation and p-values can lead to unreliable conclusions. International White Stork Census The first International White Stork Census was initiated by Prof. Ernst Schüz in 1934 and thus has a long history. Since 1974, it has taken place at ten-yearly intervals. So far, it has been possible to enthuse countless ornithologists and people interested in protecting White Storks to record their numbers at regular intervals. We have raw data on stork population from the Results of the 6th International White Stork Census 2004/2005 for 28 countries in 2005 as well as figures for human population. ⬇ Download Stork Data (Excel) Storks is the number of pairs of storks in that country. The Area is in square kilometers. Population is the total population in million whereas UrbanPop is the population living in urban areas. Fertility refers to the total fertility rate, that is the average number of children that would be born to a female over their lifetime. How many storks have been observed in Ukraine in 2005? Answer: . Which country has more storks? GermanyGreeceHungary The following table shows some key characteristics of the data, i.e. the minimum and maximum (which define the range) and average value of each numeric variable. It further provides a tiny histogram of the variables distribution. Min Mean Max Histogram Storks 3.00 7718.36 52500.00 ▇▂▁ Area 20273.00 801740.96 17075200.00 ▇ Population 1.36 25.03 143.67 ▇▁▁▁▁ Fertility 1.15 1.44 2.37 ▄▇▁▂▁▁▁ UrbanPop 0.93 17.88 105.51 ▇▁▁▁▁ 9.2 Statistics Before venturing into specific inquiries, it is essential to establish a solid foundation by introducing key concepts such as variance, covariance, correlation, and levels of measurement. The understanding of variance is paramount as it provides valuable insights into the degree of variability within a variable. Covariance, the measure of how two variables change together, assumes a pivotal role in unveiling the direction of association between them. The standardized measure of association, correlation, plays a central role in quantifying the strength and direction of relationships. 9.2.1 Variance The map shows the number of stork pairs for the selected countries. Looks like storks like certain countries more than others. The range goes from 3 pairs in Albania to pairs of storks in Poland. There is high variability in the number of storks. The variance is a measure for variability of data. Definition The variance is defined as the average quadratic deviation from the mean. The term variance was first introduced by Ronald Fisher in his 1918 paper The Correlation Between Relatives on the Supposition of Mendelian Inheritance. \\[var(x) = \\frac{1}{n-1} \\sum (x_i - \\overline{x} )^2\\] To calculate the variance, we subtract each data point \\(x_i\\) from the mean \\(\\overline{x}\\). Then square those deviations and add them up. Finally, there is a scaling factor, we divide by the number of observations minus 1. The variance of Storks and can be calculated via var() in R. It is 154084389. 150 million of what? The variance is in squared unit, i.e. square storks and thus hard to interpret. Truly Dedicated: Population vs. Sample When you collect data from every member of the population that you’re interested in, you can get an exact value for population variance. When you collect data from a sample, the sample variance is used to make estimates or inferences about the population variance. Sample variance is divided by \\(n-1\\). Population variance is divided by \\(n\\). Please try mental calculation. What is the variance of 3, 5, 7, 9 and 11? Answer: . 9.2.2 Standard Deviation The standard deviation is derived from variance and tells, on average, how far each value lies from the mean. Variance and standard deviation both measure the variability of a variable. The standard deviation is the square root of variance. \\[sd(x) = \\sqrt{var(x)} = \\sqrt{ \\frac{\\sum (x_i - \\overline{x} )^2}{n-1} } \\] In R, the standard deviation is calculated by sd() like sd(storks$Storks) which yields 12413. The mean of storks mean(storks$Storks) is approximately 7718. The one-dimensional variable Storks is visualized as points on the line. Truly Dedicated: Empirical Rule In statistics, the empirical rule states that 68% of the observed data will occur within the first standard deviation, 95% will take place in the second deviation and 99.7% within the third standard deviation of the mean within a normal distribution. See 68–95–99.7 rule for more information. Well, the empirical rule assumes that the data follows a perfectly symmetric bell-shaped curve with no constraints on the range of possible values. In reality, many variables have natural constraints, such as a floor (minimum value) or a ceiling (maximum value). For example, age cannot be negative, so it has a natural floor of zero. Similarly, test scores might have a maximum possible value, such as 100%. Again, the standard deviation of storks is 12413. Given the graph and the table with raw data, can you tell how many percent of the stork data lies with one standard deviation on each side of the mean? Answer: . The following scatterplot shows the number of storks and the population with their respective means as bold red lines. Now, that we know how to describe the variation of each of the two variables, we look for a measure that reflects the co-variation of both variables, i.e. how they change in relation to each other. The new grid of means is a good starting point. When most data points fall into lower left and upper right quadrant, we call this a positive relationship. 9.2.3 Covariance Covariance is a measure of the joint variability of two variables. The main idea of covariance is to classify three types of relationships: positive, negative or no relationship. Definition The covariance between two variables is the product of the deviations of x and y from their respective means. \\[cov(x,y) = \\frac{1}{n-1} \\sum\\limits (x_i - \\bar{x})(y_i - \\bar{y})\\] For each data point, we multiply the differences with the respective mean. Geometrically, this results in several rectangular areas starting at the intersection of means as a new origin. The covariance sums up all these areas. Finally, the covariance is adjusted by the number of observations. When both values are smaller or greater than the mean, the result will be positive. In R, the covariance is calculated by cov(storks$Storks, storks$Population) and yields 89235. The positive covariance confirms what we saw in the first scatterplot, the positive association between storks and population. Let's try to visualize the calculation procedure of the covariance. Your Turn Can you validate the covariance result from cov(x,y) from the sum of squares from the figure? Covariance qualifies a relationship as positive or negative, i.e. the direction of the relationship. Covariance is expressed in units that vary with the data. Because the data are not standardized, you cannot use the covariance statistic to assess the strength of a linear relationship (a covariance of 117.5 can be very low for one relationship and 0.23 very high for another relationship). We need a measure independent of units in a fixed range, the correlation coefficient. 9.2.4 Correlation To assess the strength of a relationship between two variables a correlation coefficient is commonly used. It brings variation to a standardized scale between -1 to +1. Definition The correlation coefficient is a statistical measure of the strength and direction of the relationship between two variables. \\[r(x,y) = \\frac{\\sum\\limits (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits (x_i - \\bar{x})^2 \\sum\\limits (y_i - \\bar{y})^2}}\\] Does the numerator and denominator remind you of something? The formula is made of the components variance and covariance. Thus, the correlation coefficient formula is often expressed in short as: \\[r(x,y,) = \\frac{Cov(x,y)}{\\sqrt{Var(x) Var(y)}}\\] cor() is a basic function to calculate the correlation coefficient. # Basic function cor(storks$Storks, storks$Population) #&gt; [1] 0.2199176 The correlation coefficients confirms ones more, there is a positive relationship. You will find thresholds for different fields of research that classify the magnitude of the correlation coefficient as weak, moderate and strong. Social science usually accept lower correlation values to be meaningful. One possible classification could be: above 0.4 is strong between 0.2 and 0.4 is moderate, and those below 0.2 are considered weak. Thus we may consider the stork-population-relationship as weak to moderate. Keep in mind that these thresholds are not set in stone. Now, let's turn to cor.test(), a more sophisticated version including a hypothesis test. # Advanced function cor.test(storks$Storks, storks$Population) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: storks$Storks and storks$Population #&gt; t = 1.1495, df = 26, p-value = 0.2608 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.1668487 0.5480307 #&gt; sample estimates: #&gt; cor #&gt; 0.2199176 The correlation test is based on a t-value (t = 1.1495056) and returns a p-value (0.2608121) for statistical significance. Definition The p value is a statistical measure to determine whether the results of a statistical analysis are statistically significant or if they could have occurred due to random chance. Small p-values below 0.05 are usually considered to be statistically significant. This is not the case for our stork-population relationship. 9.2.5 An Early Glimpse into Regression Now, let's consolidate our understanding with a minimal example. Consider 5 observations for two variables, \\(x = (4, 13, 19, 25, 29)\\) and \\(y = (10, 12, 28, 32, 38)\\). To compute the variance for each variable, we follow a simple process: subtract the mean, square the difference, and sum the results. The mean of \\(x\\) is while the mean of \\(y\\) is . \\[ \\begin{aligned} \\text{var}(x) &amp;= \\frac{1}{4} ( (4 - 18)^2 + (13 - 18)^2 + (19 - 18)^2 + (25 - 18)^2 + (29 - 18)^2) \\\\ \\text{var}(x) &amp;= \\frac{1}{4} ( (-14)^2 + (-5)^2 + (1)^2 + (7)^2 + (11)^2) \\\\ \\text{var}(x) &amp;= \\frac{1}{4} ( 196 + 25 + 1 + 49 + 121) = \\frac{1}{4} (196 + 75 + 121) = \\frac{1}{4} (392) \\\\ \\text{var}(x) &amp;= \\frac{1}{4} (392) = 98 \\end{aligned} \\] We then calculate the covariance of \\(x\\) and \\(y\\). Given the variances and the covariance, we can calculate the correlation coefficient of the two variables. Now, there is another concept for measuring the connection between two variables: The regression coefficient. Don't worry, for the moment you just need to know that there is an interesting connection between the two. \\[\\beta_{x \\rightarrow y} = r_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\] The correlation coefficient is the same value no matter the order of the variables. It is symmetric, i.e. the correlation between variables \\(x\\) and \\(y\\) is the same as the correlation between \\(y\\) and \\(x\\). However, the regression coefficient is a bit picky. It does care about the order of things. If you swap the roles of the dependent (\\(y\\)) and independent variables (\\(x\\)), the regression coefficient changes. So, it's like having a favorite direction – it matters which way you're looking when interpreting the relationship. Observation x y \\(\\text{var}(x)\\) \\(\\text{var}(y)\\) \\(\\text{cov}(x, y)\\) \\(r_{xy}\\) \\(\\beta_{x \\rightarrow y}\\) 1 4 10 98 154 118 0.956 1.2 2 13 12 98 154 118 0.956 1.2 3 19 28 98 154 118 0.956 1.2 4 25 32 98 154 118 0.956 1.2 5 29 38 98 154 118 0.956 1.2 Notably, if the standard deviations (\\(\\sigma\\)) of the two variables are equal, the correlation coefficient and the simple regression coefficient coincide. We can achieve this when variables are on the same scale. The most common way of achieving this is through standardization. Standardization is a crucial process in statistics that harmonizes variables by centering them around a mean of 0 and scaling them to have a standard deviation of 1. This transformation ensures that variables operate on a uniform scale, facilitating meaningful comparisons. The standardization formula is expressed as: \\[ z = \\frac{x - \\bar{x}}{\\sigma} \\] By subtracting each data point from the mean and dividing by the standard deviation, we create a standardized variable that simplifies the interpretation and comparison of different datasets. The same calculation based on scaled data: Observation \\(z_x\\) \\(z_y\\) \\(\\text{var}(z_x)\\) \\(\\text{var}(z_y)\\) \\(\\text{cov}(z_x, z_y)\\) \\(r_{z_x z_y}\\) \\(\\beta_{z_x \\rightarrow z_y}\\) 1 -1.414 -1.128 1 1 0.956 0.956 0.956 2 -0.505 -0.967 1 1 0.956 0.956 0.956 3 0.101 0.322 1 1 0.956 0.956 0.956 4 0.707 0.645 1 1 0.956 0.956 0.956 5 1.111 1.128 1 1 0.956 0.956 0.956 9.3 Visualizations In our example, we explored the stork-baby relationship by measuring the number of stork pairs and human population as continuous variables. Using a scatter plot, we depict their connection as dots on a graph. We also try another way by changing one of the continuous things into groups, like putting them in boxes. We then use a barplot to show this connection. This helps us see both the detailed view of the continuous things and the simpler picture when we group them. It gives readers a good understanding of how things are related in different ways. 9.3.1 Storks and Population in a Scatterplot The scatterplot is a two-dimensional instrument that shows the number of storks on the x-axis and the population on the y-axis. The blue line illustrates the linear trend between the variables. Since its slope is increasing, it suggests a positive connection between storks and population, i.e. countries with more pairs of storks also tend to have a higher population. The data doesn't cluster in a nice dot cloud but is spread out in both directions. While some countries only have a hand full of storks, others have tens of thousands. Let's explore this in more detail. 9.3.2 Storks and Population in a Barplot A barplot is a graphical representation of categorical data where individual bars correspond to different categories, and the length of each bar represents the frequency or value associated with that category. To illustrate the relationship in a barplot, the number of storks is transformed from a continuous variable to a categorical ordered variable. Stork numbers are grouped into four equal parts (quartiles). For each part, the mean number of population is calculated. In the first quartile are countries with a range of stork numbers between 3 and 198. For all those countries the average population is about 14 million. 9.3.3 Storks and Area we should also consider area when talking about the stork population relationship. since Additionally, considerations of geographical area size reveal potential confounding factors, where larger areas might exhibit both more storks and higher human populations. Logarithmic transformations are particularly useful for variables that exhibit skewed distributions, large ranges, or multiplicative relationships. Examples include income, where logarithmic transformation can help normalize the data, making it more symmetric. Population growth rates are often better visualized on a log scale due to their multiplicative nature. Stock prices, influenced by exponential growth or decay, benefit from log scales to highlight percentage changes and identify trends. This compression can be particularly useful when you have extreme values that would otherwise make the visualization challenging. ### Storks and Urban Population Examining the stork-baby relationship in light of the share of urban population provides a more nuanced understanding of the observed correlation. The positive link between stork population and human population may be influenced by several factors. Storks, as wild creatures, are significantly affected by shifts in habitat due to urbanization. This correlation analysis helps unravel the intricate dynamics of how changes in human habitat impact stork habitats. Urbanization emerges as another influential factor, with urban areas typically hosting fewer storks but higher human populations, potentially confounding the relationship. Moreover, environmental changes, such as deforestation or urban development, may impact both stork habitats and human populations, introducing confounding variables that could mislead interpretations, making it appear as if storks influence human births. This comprehensive exploration highlights the complexity of the stork-baby relationship and the importance of considering multiple factors for a more accurate interpretation. Integrating information on the share of urban population into the analysis of the stork-baby relationship offers a more nuanced perspective on the observed correlation. The positive link between stork population and the proportion of urban residents may stem from various contributing factors: Storks, being wild creatures, can be significantly affected by shifts in habitat resulting from urbanization. Evaluating the correlation between the share of urban population and stork populations aids in understanding the impact of changes in human habitat on stork habitats. 9.4 Spurious Relationships Figure 9.2: The first known pair in Finland (2015), representing a northward expansion compared to the species' historical breeding range. In the stork and baby relationship, the observed correlation between stork populations and birth rates could be influenced by common causes, such as geographical area and urbanization. Let's delve into these common causes: Larger geographical areas might exhibit both more storks and higher human populations. This is not necessarily because storks directly influence birth rates, but rather because larger areas can accommodate larger stork populations and, simultaneously, support higher human populations. Urban areas tend to have fewer storks but higher human populations. Urbanization can alter habitats and make urban areas less conducive to stork populations, while it concentrates human populations in the same regions. Reading Skill and Shoe Size: - Variables: Reading skill and shoe size. - Confounding Variable: Age. - Correlation Explanation: Age serves as a confounding variable, influencing both reading skill development and physical growth (shoe size). Ice Cream Sales and Drowning Incidents: - Variables: Ice cream sales and drowning incidents. - Confounding Variable: Temperature. - Correlation Explanation: Temperature is a confounding variable, affecting both ice cream sales and the number of drowning incidents. Number of TVs per Household and Life Expectancy: - Variables: Number of TVs per household and life expectancy. - Confounding Variable: Socioeconomic status. - Correlation Explanation: Socioeconomic status is a confounding variable, affecting both the number of TVs owned and life expectancy. Readings Matthews, R. (2000). Storks deliver babies (p= 0.008). Teaching Statistics, 22(2), 36-38. "],["regression.html", "Chapter 10 Regression 10.1 Old but Gold 10.2 Data is everywhere 10.3 For the truly dedicated 10.4 Survival of the Fittest Line 10.5 On the Shoulders of Giants", " Chapter 10 Regression This chapter introduces the workhorse of empirical research in the social science: Regression. “As an undergraduate I studied economics, which meant I studied a lot of regressions. It was basically 90% of the curriculum (when we’re not discussing supply and demand curves, of course). The effect of corruption on sumo wrestling? Regression. Effect of minimum wage changes on a Wendy’s in NJ? Regression. Or maybe The Zombie Lawyer Apocalypse is more your speed (O.K., not a regression, but the title was cool).” -- Amanda West (2020) -- A Beginner’s Guide to Instrumental Variables 10.1 Old but Gold Are older artists are better than younger artists? While experience and maturity can certainly contribute to an artist's skill and creativity, there are many factors that can influence the quality of an artist's work, such as natural talent, dedication, training, and access to resources. Is there an optimum age for artistic performance as compared to athletic performances which reaches a peak in youth? Do artists improve their skills and performance over an entire lifetime step-by-step such that the longer you live, the more you have time to practice? Does exceptional art happens randomly? Perhaps it takes time to become more well-known. You need time to travel and show or sell your art in different places. Thus when you produce \"more art\" you increase the chance to be discovered by the public or a patron? Have you ever heard of an artist who exactly created one piece of art? There seems to be something to the story. \"Paul Cezanne died in October 1906 at the age of 67. In time he would be generally regarded as the most influential painter who had worked in the nineteenth century (e.g., Clive Bell, 1982; Clement Greenberg, 1993). Art historians and critics would also agree that his greatest achievement was the work he did late in his life.\" -- Galenson, D. W., &amp; Weinberg, B. A. (2001). Creating modern art: The changing careers of painters in France from impressionism to cubism. American Economic Review, 91(4), 1063-1071. What does better mean? In this chapter the research question is: What is the relationship between the age of an artist and his productivity? Definition A research question is focused on a single issue, specific enough to answer thoroughly and feasible to answer within the given time frame or practical constraints not to mention relevant to your field of study. But what exactly is productivity and how can we measure it? To keep things simple we follow the literature and measure productivity via auction prices for paintings. That's a very economic perspective on art. Definition Operationalization is the process of defining the measurement of a phenomenon that is not directly measurable. This is what we gonna explore: What is the relationship between the age of artists and auction price for their paintings? 10.2 Data is everywhere Researchers use auction price data for which they have to pay. We use free information from a Wikipedia List of most expensive paintings of all time. The auction prices are inflation-adjusted by consumer price index in millions of United States dollars in 2019. That's another interesting economic procedure, that we take for given at this analysis. 10.2.1 Data in a table The table is created with the DT package in datatable format. This exploits the full potential of html documents, i.e. the data is searchable and sortable. The first rows are displayed, but in principle it can include the entire dataset. Definition Tabular data is common in data analysis. You can create a table in Word or Excel. 10.2.2 Data in a graph Two continuous variables are plotted in a scatterplot. The x-axis is called abscissa and the y-axis is called ordinate. Note that the axis beginning is not zero. The decision where axes start was made by the ggplot package for this data. Remember, every unit on the y-axis represents a million US dollars. Do we need to show the age between 0 and 20? How many famous artists died before 20 and sold paintings for a hundred million US dollars? When It’s OK to NOT Start Your Axis at Zero. When the data really don’t fluctuate very much but a rise of small values like 1.4 or 1.4% is a big deal. With a graph that starts at zero, these changes can't be detected. The data scientist has to decide. 10.2.3 The trend The graph suggests a positive trend between price and age. There is an increase in price for older artists. The older the artist, the higher the auction prices. 10.2.4 The blackbox The mission is to find a mathematical function that describes the trend. In other words, we are looking for the black box that transforms the input into the output: Definition A mathematical function is an expression, rule, or law that defines a relationship between one variable (the independent variable, on the x-axis) and another variable (the dependent variable, on the y-axis). From looking at the graph, here are two suggestions: \\[\\begin{align} \\text{price} = 80 + 0.5 \\cdot \\text{age} \\tag{Suggestion 1} \\\\ \\text{price} = 90 + 0.2 \\cdot \\text{age} \\tag{Suggestion 2} \\\\ \\end{align}\\] Definition A linear function is defined by two components, intercept (with the y-axis) and slope. How can we compare the two suggested lines? Which linear function represents the relationship best? 10.2.5 Nobody's perfect We all make mistakes. So do the linear functions: \\[ \\begin{align} \\text{price} &amp;= 80 + 0.5 \\cdot \\text{age} \\tag{Suggestion 1} \\\\ 102.5 &amp;= 80 + 0.5 \\cdot 45 \\tag{Age for Holbein} \\\\ \\end{align}\\] The equation tells (or predicts) that for any artist at the age of 45 it expects a auction price for a painting of 102.5 million US Dollar. Darmstadt Madonna was sold for 85 million US dollar. The linear function overestimated the true value. When you look at the graph, you see some predictions are more accurate (close to the true values) than others. All are either above or below the line. Definition A residual (or error) is the vertical distance between the actual and the predicted value. 10.2.6 Vocab Wrap-Up Let's wrap up regression vocab! Find an equation that describes the phenomenon of interest. Equation I shows a generic statistical model; equation II a generic linear model. \\[ \\begin{align} \\text{outcome} &amp;= f(\\text{explanatory}) + \\text{noise} \\tag{I} \\\\ \\text{outcome} &amp;= \\text{intercept} + \\text{slope} \\cdot \\text{explanatory} + \\text{noise} \\tag{II} \\\\ \\end{align} \\] A regression model is suggested by the researcher. A more concrete regression model looks like this: \\[Y = \\beta_1 + \\beta_2 X + \\epsilon\\] A model can be easy or complicated. It definitely contains variables and parameters. Variables: Things that we measure (or have data). Parameters: Constant values we believe to represent some fundamental truth about the relationship between the variables. The calculation is called an estimation. In textbooks the same equation can be found with hats: \\[ \\widehat{Y} = \\widehat{\\beta}_1 + \\widehat{\\beta}_2 \\cdot X \\] \\(\\widehat{Y}\\) are called the fitted or predicted values. \\(\\widehat{\\beta}\\) are regression coefficients (this is the estimate of the unknown population parameter). As we have seen in the graph before, the differences between the actual and the predicted values are the residuals \\(e = Y - \\widehat{Y}\\). The fitting procedure is called ordinary least squares (OLS). 10.3 For the truly dedicated The overall goal is to make as little as possible mistakes! What kind of mistake? The deviation from the observed values! What could come to your mind is to minimize the sum of all errors: \\[\\sum e \\rightarrow \\min\\] But wait, there is more. Is it fair to say that the sum should be small? Compare The Scream and Meules, their deviations are \\(+17.5\\) and \\(-13.6\\) (very similar). So taken these two together, there's almost not mistake! That is to say, positive and negative deviations cancel each other out. Thus we need one more twist in the story: \\[\\sum e^2 \\rightarrow \\min\\] The goal of OLS is to minimize the residual sum of squares or the sum of squared residuals. 10.3.1 Algebra Amazing Fact Algebra comes from Arabic, meaning \"reunion of broken parts\". Let's introduce matrix notation. We began with \\(X\\) and \\(Y\\) being variables in the equation: \\[Y = \\beta_0 + \\beta_1 X + \\epsilon \\] We turn this into: \\[y = X \\beta + \\epsilon \\] Capital letters like \\(X\\) represent a matrix (a table with rows and column), and small letters like \\(y\\) and \\(e\\) represent vectors. Since there are 6 observations and two parameters in our model we get: \\[ \\begin{align} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_5 \\\\ Y_6 \\end{pmatrix} &amp;= \\begin{pmatrix} 1 &amp; X_{11} \\\\ 1 &amp; X_{12} \\\\ 1 &amp; X_{13} \\\\ 1 &amp; X_{14} \\\\ 1 &amp; X_{15} \\\\ 1 &amp; X_{16} \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\\\ \\epsilon_6 \\\\ \\end{pmatrix} \\end{align}\\] Let's turn to the goal, minizing the residual sum of squares (RSS). By convention, the normal version of a vector is a vertical list of numbers in big parentheses (i.e. a column vector). To transpose a vector means change between the row and column format. Squaring a vector thus means the row version of the vector times the column version of the vector: \\[\\sum e^2 = e^T \\cdot e \\rightarrow \\min\\] Notice that the sum operator is gone. Matrix multiplication requires multiplying all elements pairwise with each other and summing them up. Plug in the residuals \\(e = y - X \\beta\\) in the equation: \\[\\begin{align} \\sum e^2 &amp;= e^T \\cdot e \\\\ &amp;= (y - X \\beta )^T (y - X \\beta) \\tag{$(A+B)^T = A^T + B^T$}\\\\ &amp;= (y^T - X^T \\beta^T) (y - X \\beta) \\\\ &amp;= y^T y - y^T X \\beta - X^T \\beta^T y + X^T \\beta^T X \\beta \\\\ &amp;= y^2 \\underbrace{- 2 \\beta^T X^T y}_{??} + \\beta^2 X^2 \\\\ \\end{align}\\] Did you notice what happened in the middle? The transpose of the first term is equal to the second: \\[\\begin{align} (y^T X \\beta)^T = y X^T \\beta^T \\end{align}\\] 10.3.2 Analysis Amazing Fact From Medieval Latin, analysis means \"resolution of anything complex into simple elements\" (opposite of synthesis). Next, we are ready to optimize. Optimization (in math and economics) is done by differentiation: \\[\\begin{align} \\frac{\\partial RSS}{\\partial \\beta} &amp;= -2 X^T y + 2 \\beta X^T X = 0 \\tag{first derivative = zero} \\\\ 2 \\beta X^T X &amp;= 2 X^T y \\tag{rearrange terms}\\\\ \\beta X^T X &amp;= X^T y \\tag{the &quot;normal equation&quot;} \\\\ \\beta &amp;= (X^T X)^{-1} X^T y \\tag{Bam}\\\\ \\end{align}\\] Those \\(\\beta\\) coefficients are the first and most important regression results. Retrieve them step by step to enhance your understanding of the math and coding as the same time. 10.3.3 Take the Long Way Home First, retrieve matrix \\(X\\) from the data set: X #&gt; [,1] [,2] #&gt; [1,] 1 91 #&gt; [2,] 1 86 #&gt; [3,] 1 67 #&gt; [4,] 1 45 #&gt; [5,] 1 80 #&gt; [6,] 1 71 Second, the transpose of \\(X\\) has two rows and six columns (use t()): t(X) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] 1 1 1 1 1 1 #&gt; [2,] 91 86 67 45 80 71 Next, calculate the square of the matrix (transpose times original): t(X)%*%X #&gt; [,1] [,2] #&gt; [1,] 6 440 #&gt; [2,] 440 33632 The inverse of the matrix product can be calculated by solve(): solve(t(X)%*%X) #&gt; [,1] [,2] #&gt; [1,] 4.10546875 -0.0537109375 #&gt; [2,] -0.05371094 0.0007324219 Next, multiply the inverse with the transpose from the right: solve(t(X)%*%X) %*% t(X) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] -0.78222656 -0.513671875 0.506835938 1.68847656 -0.191406250 #&gt; [2,] 0.01293945 0.009277344 -0.004638672 -0.02075195 0.004882813 #&gt; [,6] #&gt; [1,] 0.291992188 #&gt; [2,] -0.001708984 Finally, multiply the vector \\(y\\): solve(t(X)%*%X) %*% t(X) %*% y #&gt; [,1] #&gt; [1,] 22.345215 #&gt; [2,] 1.165747 It's the \\(\\beta\\) vector! The first entry is the intercept and the second is the slope of the linear function. The following graph shows the line created from intercept and slope in a scatter plot. 10.4 Survival of the Fittest Line The linear equation that best describes the data is this: \\[Price = 22.3452 + 1.1657 \\cdot Age\\] 10.5 On the Shoulders of Giants Fortunately, we are standing on the shoulders of giants. Clever people implemented the linear regression (command lm()) and all kinds of regressions and statistical tests in R. lm(Price ~ Age.at.Death, data = artists) #&gt; #&gt; Call: #&gt; lm(formula = Price ~ Age.at.Death, data = artists) #&gt; #&gt; Coefficients: #&gt; (Intercept) Age.at.Death #&gt; 22.345 1.166 The workhorse packs up work. "],["references.html", "References 10.6 Resources", " References 10.6 Resources https://informationisbeautiful.net/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
