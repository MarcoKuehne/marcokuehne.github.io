[["index.html", "Becoming Fluent in Data A Personal Journey – Every Time. Preface", " Becoming Fluent in Data A Personal Journey – Every Time. Marco Kühne 2023-08-25 Preface "],["about-the-book.html", "About the book Aims of this book Structure Why R? Why Tidyverse?", " About the book I welcome you to join us on our way to become fluent in data. Aims of this book This project is for everyone. For students You go from zero you hero in data analysis and data science and will become data fluent and learn major skills that you can use in your academic and business career. The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades... Hal Varian, 2009. Googles chief economist. In: McKinsey Quarterly 2009 In addition and independent of a specific career, I would like to foster people's data literacy. Definition Data literacy is the ability to read, understand, create, and communicate data as information. I would like to enable and empower all people to understand the data work of others. Don't take numbers for granted. It's a long journey to get them. Don't be satisfied with a summary or conclusion from someone else. It's worthwhile checking data sources and data work. Either to replicate and validate the data work of others or to form your own opinion. Not to mention, coding is fun. You might be under the impression, to code, your favorite thing must be computers. Or I've heard I'm bad a math, I can't code. None of this is true. Think about it, what is your passion. Learn to code, is something that you can do. And something that may just expand the way you approach and think about the passions in your life. Be their personal or professional.   A wordcloud of 25 students answers (2020) on the question: What do you expect to learn in this seminar? The animation is created by the gifski package. For instructors Becoming Fluent In Data is planned to become an open educational resource (OER) and provide freely accessible educational content related to data analysis. It offers resources, tutorials, and information aimed at helping individuals become proficient in working with data. As an OER, the website allows users to access and utilize its materials without any cost, enabling widespread dissemination and promoting equitable access to knowledge and learning opportunities. The licensing is yet to be finalized. Parts of the OER can be used flexibly, as they are modular and can be used independently of each other. This enables users to select and use those parts that are relevant to their specific needs. They can extract individual chapters, modules or exercises from the OER resource and integrate them into their own learning environments. The site is hosted on GitHub, the entire source code is available. The project is accompanied by a data repository that can be used for a variety of teaching and learning scenarios (available as .txt, .csv or .xlsx). The educational videos created for the OER can be used as a standalone introduction to central concepts. Structure Every chapter covers the content of a week. The first half of the course introduces all the data basics from scratch. What is data? Why do we measure? How do we measure? How do we make comparisons? Most decisions are complex, costly and have long-term implications. It is therefore vital that such decisions are based on the best available evidence. The second half of the course focuses on the analysis of relationships. The most interesting research questions in social science are about relationships. What is the relationship between beauty and employment chances? What is the connection between money and happiness? How does remote work change ones productivity? Is social support related to longevity of marriages? The workhorse procedure is regression, a statistical technique that relates variables to each other. Color Colored paragraphs give you a visual overview of things to watch out: Definition A definition is a statement of the meaning of a term. Amazing Fact Bazinga highlights a memorable fact. Reading A precious resource. Your Turn It's your turn. Truly Dedicated Heavy stuff to think about. Components The tippy package allows underlined text to display tooltips. The webexercises package allows interactive web pages that you can use in ballroom dancingself-guided learningvegan cookingyour stamp collection. What is the Answer to the Ultimate Question of Life . The most powerful interaction comes from the web annotation tool https://web.hypothes.is/. You may annotate or highlight all text by selecting it with the cursor and then click the on the pop-up menu. You can also see the annotations of others: click the in the upper right hand corner of the page. Create a free account for this feature. Why R? This book uses R. All concepts could have been implemented in Python as well and there is a future plan to translate some examples to Python. Both are versatile programming languages with an active community and a lot of free online resources. My main recommendation is to use a programming language instead of a WYSIWYG statistical software (e.g. SPSS) and to use open-source software instead of proprietary software. Why Tidyverse? The debate regarding how to teach R centers around whether to introduce the base R programming language or the tidyverse ecosystem. Proponents of base R argue that it provides a solid foundation for understanding R's core principles and functions. It emphasizes learning the fundamentals of R programming, which can be beneficial for more complex data manipulation tasks and working with large datasets. On the other hand, advocates for the tidyverse approach argue that it offers a more user-friendly and intuitive way to work with data. The tidyverse packages, such as dplyr and ggplot2, provide a consistent and streamlined syntax for data manipulation and visualization, making it easier for beginners to grasp and apply data analysis techniques. This book uses tidyverse predominantly. "],["about-the-author.html", "About the author Dust and Dark Teach – Learn – Repeat", " About the author   Welcome! My name is Marco Kühne. The very first thing I want to do is to invite you to call me Marco. That is, if we meet on the street, you come talk to me during office hours, you ask some question; Marco’s the name that I respond to. Web: http://marco-kuehne.com/ Twitter: https://twitter.com/marco_kuhne GitHub: https://github.com/MarcoKuehne I am a PhD candidate in Economics at the European University Viadrina. I am generally keen on teaching topics related to research design, quantitative methods, and statistical software. My main methodological interests in quantitative social science include panel data modelling, causal inference with observational data and R programming. I am also a gardening fanatic, a coffee enthusiast, a committed ballroom and Discofox dancer, a (vegetarian) food lover. I enjoy cutting down big trees and practicing new languages in its own sake. Feel free to contact me! Dust and Dark A dusty lecture hall. The light cuts through the darkness from the left side of the room. A dozen of seats in each bench, only few occupied by small groups of students who were trying to make sure that they sit far from each other and as far as possible from the lecturer. The bearish but competent assistant professor explained how to analyze and evaluate the results of various memory and cognition experiments through boxplots, t-test and the like in that software. My creaky, slow but loyal laptop in front of me. That's where R was introduced in my psychology undergraduate studies. – The Times They Are A-Changin'. This eBook is done in R. Are you eady to join the journey? Lecture Hall. Melanchthonianum. MLU University of Halle-Wittenberg The following animation is created with the gganimate package. It shows past course data. Which graphical feature is used to display 3 data dimensions in a two-dimensional graph: ColorSizeShapeOpacity? How many students participated in summer 2021? Answer: . Teach – Learn – Repeat Teaching and learning are strongly connected. I fell in love with learning by teaching the moment I came across this concept. It put the experiences I made into scientific context. Studying for the undergraduate math classes, I soon became head of the study group, than a private tutor, than a student assistant and a doctoral student, now, teaching stuff for over a decade. Still, I feel that (trying) to teach stuff is the best way of learning it myself. By writing the gitbook I hope to force myself to pinpoint exactly what I know and don't know about data and how to fill the gaps. Luckily, I am not alone with the approach of creating classes or writing books to learn: I could feel that econometrics was indispensable, and yet I was missing something. But what? It was a theory of causality […]. So, desperate, I did what I always do when I want to learn something new — I developed a course on causality to force myself to learn all the things I didn’t know. Cunningham (2021) This project helped me to learn more about R, RStudio, R Markdown, R Bookdown, HTML/CSS, Git and Github, empirical research, causal inference, statistics, math, frustration tolerance and fun. Teaching in 2014. Learning by teaching was originally defined by Jimmy WalesLinus TorvaldsJean-Pol MartinRichard E. Pattis in the 1980s. Did you know that some people who have Wikipedia articles also have user accounts on Wikipedia? "],["intro-to-r.html", "Intro to R R is a calculator R is more than a calculator Define objects Plots", " Intro to R An .R file is a text document. Open (and edit) it with a text editor, a web browser or a more sophisticated software like RStudio.1 Click this link that opens an R script in your browser. Using .R files or scripts offers efficiency, reproducibility, scalability, collaboration, documentation, and flexibility. They allow you to automate tasks, handle large datasets, collaborate with others, document your work, and customize solutions. R is a calculator R is a calculator. Use this R demo in the browser to explore basic features of R. Commands in the script.R tab are executed by the Run bottom. It runs the entire script and prints out results in the R Console. This setting is simplified but reflects the procedure in a more complex integrated developer environment (IDE) like RStudio. Test it. Definition Basic arithmetic operators are: + Addition - Subtraction * Multiplication / Division ^ Exponent R is more than a calculator Your Turn: Adjust the code. If you never saw R before, change the main title to what ever suits you or change the color option col from lightblue to aliceblue. If you have some experience, order the bars using the sort() command. Define objects Define R objects for later use. Objects are case-sensitive (X is different from x). Objects can take any name, but its best to use something that makes sense to you, and will likely make sense to others who may read your code. Numeric Variables The standard assignment operator is &lt;-, the equal sign = works as well. The code assigns the object a the value 2 and b the value 3. The sum is 5. a &lt;- 2 b = 3 a + b #&gt; [1] 5 Logical Variables Logical values are TRUE and FALSE. Abbreviations work. You can write T instead of TRUE. &gt; harvard &lt;- TRUE # spacing doesn&#39;t matter &gt; yale &lt;- FALSE &gt; princeton &lt;- F # short for FALSE &gt; &gt; # Attention: FALSE=0, TRUE=1 &gt; harvard + 1 #&gt; [1] 2 Although spacing technically doesn't matter in R, there are some best practices to consider. “Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read.” -- Hadley Wickam -- Style guide Reading Place spaces around all binary operators (=, +, -, &lt;-, etc.). Do not place a space before a comma, but always place one after a comma. Read more in Google's R Style Guide at Uni Stanford. String Variables Text is stored as string or character. emily &lt;- &#39;She is a friend.&#39; # string / character class / plain text libby &lt;- &quot;she is a coworker&quot; # use &#39; and &quot; interchangeably other &lt;- &quot;people&quot; # prefer &quot; Factor Variables A factor is an ordered categorical variable. c() is a generic function which combines its arguments. fruit &lt;- factor(c(&quot;banana&quot;, &quot;apple&quot;) ) # The default ordering is alphabetic fruit #&gt; [1] banana apple #&gt; Levels: apple banana dose &lt;- factor(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) ) # The default ordering is alphabetic dose #&gt; [1] low medium high #&gt; Levels: high low medium Factor levels inform about the order of the components, i.e. apple comes before banana and high comes comes before low, than comes medium. Of course, the apple-banana order does not makes any sense, and the high-low-medium order is just wrong. Software cannot know whether an ordering makes sense, that's job of the data scientist. Use the levels option inside the factor() function to tell R the ordering. dose &lt;- factor(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) ) dose #&gt; [1] low medium high #&gt; Levels: low medium high Combine objects # Declare new objects using other variables c &lt;- a + b + 10 # Open z object or put everything in parentheses (c &lt;- a + b + 10) #&gt; [1] 15 Vectors Think of a vector as a single column in a spreadsheet. vectorA &lt;- c(1,2,b) vectorB &lt;- c(TRUE,TRUE,FALSE) vectorC &lt;- c(emily, libby, other) # Vector Operations vectorA - vectorB # Vector operation AND auto-change TRUE =1, FALSE=0 #&gt; [1] 0 1 3 Data frame # think of it conceptually like a spreadsheet dataDF &lt;- data.frame(numberVec = vectorA, trueFalseVec = vectorB, stringsVec = vectorC) # Examine an entire data frame dataDF #&gt; numberVec trueFalseVec stringsVec #&gt; 1 1 TRUE She is a friend. #&gt; 2 2 TRUE she is a coworker #&gt; 3 3 FALSE people # Declare a new column dataDF$NewCol &lt;- c(10,9,8) # Examine with new column dataDF #&gt; numberVec trueFalseVec stringsVec NewCol #&gt; 1 1 TRUE She is a friend. 10 #&gt; 2 2 TRUE she is a coworker 9 #&gt; 3 3 FALSE people 8 # Examine a single column dataDF$numberVec # by name #&gt; [1] 1 2 3 dataDF[,1] # by index...remember ROWS then COLUMNS #&gt; [1] 1 2 3 # Examine a single row dataDF[2,] # by index position #&gt; numberVec trueFalseVec stringsVec NewCol #&gt; 2 2 TRUE she is a coworker 9 # Examine a single value dataDF$numberVec[2] # column name, then position (2) #&gt; [1] 2 dataDF[1,2] #by index row 1, column 2 #&gt; [1] TRUE Plots There are base R graphs. There are ggplot2 plots. # Create some variables x &lt;- 1:10 y1 &lt;- x*x y2 &lt;- 2*y1 # Create a first line plot(x, y1, type = &quot;b&quot;, frame = FALSE, pch = 19, col = &quot;red&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) # Add a second line lines(x, y2, pch = 18, col = &quot;blue&quot;, type = &quot;b&quot;, lty = 2) # Add a legend to the plot legend(&quot;topleft&quot;, legend=c(&quot;Line 1&quot;, &quot;Line 2&quot;), col=c(&quot;red&quot;, &quot;blue&quot;), lty = 1:2, cex=0.8) Download RStudio https://posit.co/downloads/.↩︎ "],["intro-to-tidyverse.html", "Intro to Tidyverse Data with readr Verbs of dplyr Graphs with ggplot2", " Intro to Tidyverse The tidyverse is a collection of R packages for data science that share a common philosophy and grammar.2 Once the package tidyverse is installed on your system via the command install.packages(tidyverse), it is loaded via library(tidyverse) in a session. Then you have access to all components like readr (for reading data), dplyr (for manipulating data), ggplot2 (for data visualization) and many more. Data with readr The readr package reads data into what is called a tibble. A tibble is similar to a dataframe. When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type, and uses font styles and color for highlighting. So to say, the default behavior is excellent. # load the entire tidyverse library(tidyverse) # read_csv is a tidyverse (readr) function coursedata &lt;- read_csv(&quot;https://raw.githubusercontent.com/MarcoKuehne/marcokuehne.github.io/main/data/Course/GF_2022_57.csv&quot;) # print a tibble coursedata #&gt; # A tibble: 57 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 8 2 #&gt; 2 Bachelor Female 22 10 4 #&gt; 3 Master Male 23 9 4 #&gt; 4 Master Male 24 2 3 #&gt; 5 Master Male 27 10 2 #&gt; 6 Master Male 23 7 3 #&gt; 7 Bachelor Female 20 5 2 #&gt; 8 Bachelor Female 22 8 2 #&gt; 9 Master Male 27 13 4 #&gt; 10 Master Female 22 10 4 #&gt; # ℹ 47 more rows #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; When you use the base R read.csv() instead, it reads data into a dataframe. When you print the dataframe, it displays all data at once (output not shown in the book). In order to show first entries, another command like head() is necessary. # use base R utilities coursedata &lt;- read.csv(&quot;https://raw.githubusercontent.com/MarcoKuehne/marcokuehne.github.io/main/data/Course/GF_2022_57.csv&quot;) # print a dataframe (all data) coursedata # print first 6 observations of the data head(coursedata) Reading There are three key differences between tibbles and data frames: printing, subsetting, and recycling rules. Read more about those difference in the vignette of tibble. Verbs of dplyr The first verbs you learn for data inspection are glimpse(), select(), arrange() and filter(). Those are classic operators that you also find in Microsoft Excel (via clicking the correct menu options). Glimpse glimpse() tells the number of rows and columns, the first variable names, the class of the variables, i.e. chr for character (like text) or int for integer (like whole numbers). Another kind of variables are dbl, double, short for double-precision floating-point format. This data set contains 57 rows (observations) and 9 columns (variables). glimpse() also shows the first observations for each variable. glimpse(coursedata) #&gt; Rows: 57 #&gt; Columns: 8 #&gt; $ Academic.level &lt;chr&gt; &quot;Bachelor&quot;, &quot;Bachelor&quot;, &quot;Master&quot;, &quot;Mast… #&gt; $ Gender &lt;chr&gt; &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Ma… #&gt; $ Age &lt;dbl&gt; 23, 22, 23, 24, 27, 23, 20, 22, 27, 22,… #&gt; $ Total.Semesters &lt;dbl&gt; 8, 10, 9, 2, 10, 7, 5, 8, 13, 10, 8, 8,… #&gt; $ Background.in.Statistics &lt;dbl&gt; 2, 4, 4, 3, 2, 3, 2, 2, 4, 4, 2, 2, 2, … #&gt; $ Background.in.R &lt;dbl&gt; 2, 3, 2, 1, 1, 1, 3, 1, 4, 4, 1, 2, 2, … #&gt; $ Background.in.Academic.Writing &lt;dbl&gt; 2, 2, 4, 3, 4, 2, 3, 2, 4, 3, 1, 2, 3, … #&gt; $ Expectations &lt;chr&gt; &quot;I want to be efficient in my knowledge… Select Columns are selected by name or column index. Thus, the outcome of select(coursedata, Gender, Age) and select(coursedata, 2, 3) is identical. select(coursedata, Gender, Age) #&gt; # A tibble: 57 × 2 #&gt; Gender Age #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Female 23 #&gt; 2 Female 22 #&gt; 3 Male 23 #&gt; 4 Male 24 #&gt; 5 Male 27 #&gt; 6 Male 23 #&gt; 7 Female 20 #&gt; 8 Female 22 #&gt; 9 Male 27 #&gt; 10 Female 22 #&gt; # ℹ 47 more rows We can use a minus - to get rid of a column and leave the rest of the columns: select(coursedata, -Total.Semesters, -Background.in.Statistics, -Background.in.R, -Background.in.Academic.Writing) Arrange Often we are interested in the maximum or minimum age, thus arrange() a numerical value. arrange(coursedata, Age) # from low to high Age #&gt; # A tibble: 57 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 20 5 2 #&gt; 2 Master Female 21 7 2 #&gt; 3 Bachelor Male 21 2 3 #&gt; 4 Bachelor Female 21 4 2 #&gt; 5 Bachelor Female 21 3 2 #&gt; 6 Bachelor Male 21 1 2 #&gt; 7 Bachelor Female 21 1 3 #&gt; 8 Bachelor Female 22 10 4 #&gt; 9 Bachelor Female 22 8 2 #&gt; 10 Master Female 22 10 4 #&gt; # ℹ 47 more rows #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; The default is from low to high values, the desc() options reverses the order. arrange(coursedata, desc(Age)) # reverse Rename Sometimes default variables names are too long or too complicated, thus we like to rename() them. coursedata %&gt;% rename(Degree = Academic.level, Semesters = Total.Semesters) #&gt; # A tibble: 57 × 8 #&gt; Degree Gender Age Semesters Background.in.Statistics Background.in.R #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 8 2 2 #&gt; 2 Bachelor Female 22 10 4 3 #&gt; 3 Master Male 23 9 4 2 #&gt; 4 Master Male 24 2 3 1 #&gt; 5 Master Male 27 10 2 1 #&gt; 6 Master Male 23 7 3 1 #&gt; 7 Bachelor Female 20 5 2 3 #&gt; 8 Bachelor Female 22 8 2 1 #&gt; 9 Master Male 27 13 4 4 #&gt; 10 Master Female 22 10 4 4 #&gt; # ℹ 47 more rows #&gt; # ℹ 2 more variables: Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; This change is only temporarily and shown in the console output. In order to keep the new name of a variable, we can overwrite the old R object or create a new one. # overwrite the old dataframe coursedata &lt;- coursedata %&gt;% rename(Degree = Academic.level, Semesters = Total.Semesters) The pipe operator As in base R, we often like to combine commands, e.g. select the Age variable and sort its values. dplyr verbs can be nested as in base R. arrange(select(coursedata, Age), Age) But there is something else that is used in tidyverse logic, the so called pipe operator %&gt;% (percentage sign, relation larger than, another percentage sign). You can read this as \"then, please do the following\". coursedata %&gt;% # start with this data select(Age) %&gt;% # then select only the Age variable arrange(Age) # then arrange the values #&gt; # A tibble: 57 × 1 #&gt; Age #&gt; &lt;dbl&gt; #&gt; 1 20 #&gt; 2 21 #&gt; 3 21 #&gt; 4 21 #&gt; 5 21 #&gt; 6 21 #&gt; 7 21 #&gt; 8 22 #&gt; 9 22 #&gt; 10 22 #&gt; # ℹ 47 more rows Filter It is reasonable to filter() specific values of variables. All filters use conditional expression based on relational operators. Definition Use relational operators to build your filter: == equal to != not equal to &gt; more or &lt; less then Here are some examples: # filter students who have more than 10 semesters in total coursedata %&gt;% filter(Total.Semesters &gt; 10) # filter female students coursedata %&gt;% filter(Gender == &quot;Female&quot;) Combinations of filters are possible via logical operators &amp; (and) and | (or). We are looking for females who study in a master program. coursedata %&gt;% filter(Gender == &quot;Female&quot; &amp; Academic.level == &quot;Master&quot;) #&gt; # A tibble: 12 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Master Female 22 10 4 #&gt; 2 Master Female 21 7 2 #&gt; 3 Master Female 29 9 3 #&gt; 4 Master Female 23 8 4 #&gt; 5 Master Female 26 8 2 #&gt; 6 Master Female 25 10 2 #&gt; 7 Master Female 24 11 1 #&gt; 8 Master Female 25 10 3 #&gt; 9 Master Female 33 3 1 #&gt; 10 Master Female 25 14 4 #&gt; 11 Master Female 25 11 3 #&gt; 12 Master Female 26 4 2 #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; We are looking for females or anybody who reports more than 10 semesters. coursedata %&gt;% filter(Gender == &quot;Female&quot; | Total.Semesters &gt; 10) #&gt; # A tibble: 33 × 8 #&gt; Academic.level Gender Age Total.Semesters Background.in.Statistics #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 8 2 #&gt; 2 Bachelor Female 22 10 4 #&gt; 3 Bachelor Female 20 5 2 #&gt; 4 Bachelor Female 22 8 2 #&gt; 5 Master Male 27 13 4 #&gt; 6 Master Female 22 10 4 #&gt; 7 Bachelor Female 22 8 2 #&gt; 8 Master Female 21 7 2 #&gt; 9 Master Male 27 15 3 #&gt; 10 Master Female 29 9 3 #&gt; # ℹ 23 more rows #&gt; # ℹ 3 more variables: Background.in.R &lt;dbl&gt;, #&gt; # Background.in.Academic.Writing &lt;dbl&gt;, Expectations &lt;chr&gt; Mutate mutate() is the most frequent used command you will come across. It changes the data. We create a new variable Background_Knowledge by taking the average of the three background variables. All background variables have the same range from 1 to 5. coursedata %&gt;% mutate(Background_Knowledge = (Background.in.Statistics + Background.in.R + Background.in.Academic.Writing)/3) %&gt;% select(Academic.level, Gender, Age, Background_Knowledge) #&gt; # A tibble: 57 × 4 #&gt; Academic.level Gender Age Background_Knowledge #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bachelor Female 23 2 #&gt; 2 Bachelor Female 22 3 #&gt; 3 Master Male 23 3.33 #&gt; 4 Master Male 24 2.33 #&gt; 5 Master Male 27 2.33 #&gt; 6 Master Male 23 2 #&gt; 7 Bachelor Female 20 2.67 #&gt; 8 Bachelor Female 22 1.67 #&gt; 9 Master Male 27 4 #&gt; 10 Master Female 22 3.67 #&gt; # ℹ 47 more rows Summarize Would you like to know the average age of course participants? It is 24.7192982 There are two ways in order to achieve this. # calculate mean age with mutate() coursedata %&gt;% mutate(mean_age = mean(Age)) # calculate mean age with summarize() coursedata %&gt;% summarize(mean_age = mean(Age)) What is the difference between them? mutate() creates a new variable mean_age in the data set for all 57 observations. But there is only 1 mean value. Thus, mutate() repeats this mean value 57 times. The result is a 57x9 tibble. summarize() collapses the tibble to a single value. The result is a 1x1 tibble. The question is, what do you plan to do next with your results. After summarize() all other information is gone. We will see this in the next graph. Graphs with ggplot2 ggplot() follows the Grammar of Graphics. The first argument is the data, the second is aes() aesthetics (that define the x- and y-variable). In order to add more to the graph, use the + operator (instead a pipe). Add layers, so called geoms, like geom_point() to create points in a coordinate system, a.k.a the scatter plot. theme_minimal() is a particular set of options that controls non-data display. ggplot(coursedata, aes(x = Age, y = Total.Semesters)) + geom_point() + theme_minimal() Alternatively, data can be piped into a ggplot(). In the second version of the graph, we added axis labels inside the labs() command and another layer geom_smooth() for a trend line of the relationship. Inside we define the method to be a linear model and the standard errors to be deactivated. Play around with those options, what other methods are available? What happens when we turn standard errors on? coursedata %&gt;% ggplot(aes(x = Age, y = Total.Semesters)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + labs(title = &quot;Relationship between Age and Semester of course participants.&quot;, x = &quot;Age&quot;, y = &quot;Semesters&quot;) See https://www.tidyverse.org/.↩︎ "],["data-is-everywhere.html", "Chapter 1 Data is everywhere 1.1 Why we measure 1.2 Means of measuring 1.3 Types of data 1.4 Can we measure everything? 1.5 The reality behind the data", " Chapter 1 Data is everywhere Data is ubiquitous in today's world and its importance is growing rapidly, especially in social science. With the increasing availability of data, researchers can gain insights into human behavior, social trends, and other important phenomena. The use of data analysis tools and techniques allows researchers to extract meaningful insights from the vast amounts of data that are being generated every day, and these insights can be used to inform policies, strategies, and decisions that impact society. It is therefore crucial for social scientists to have the skills and knowledge to effectively manage and analyze data. 1.1 Why we measure Two true stories. 1.1.1 Women are having far fewer children. Figure 1.1: Global fertility rate. Figure 1.1 shows the global total fertility rate according to Gapminder.3 There is a dramatic change in the number of babies per woman in the last 50 years. The maintenance of a stable human population requires that the mean number of children women should have by the completion of the fertile part of their life is 2.1.4 We cannot know this without measurement. We may have an impression that families are smaller, but that could just be the people we know directly – and most of us know directly at most a couple of hundred households.5 We have to measure to know the big picture and we have to keep measuring to see how the picture is changing. Size matters. Change matters. Without measurement we can describe neither current condition nor the history of current condition. 1.1.2 Global surface temperature is rising. Figure 1.2: Global temperature in the common era. The 2011–2020 decade warmed to an average 1.09 °C [0.95–1.20 °C] compared to the pre-industrial baseline (1850–1900). Figure 1.2 shows a global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue. Directly observed data is in red (Wikipedia contributors 2022). Data is required to make informed decisions. Decisions about climate change are complex, costly and have long-term implications. It is therefore vital that such decisions are based on the best available evidence. We need to understand the quality and provenance of that evidence, and whether any assumptions have been made in generating it. 1.2 Means of measuring Data collection is the process of gathering and measuring information. As social scientists we rarely count tree rings or analyse corals and ice cores. Social science is concerned about human behavior, attitudes, opinions, and characteristics to understand social phenomena. Social science researchers use a variety of data collection methods, including surveys, interviews, observations, and experiments, to collect data that can be analyzed and used to test hypotheses and answer research questions. Surveys are a common data collection method in social science research. They involve administering questionnaires to a sample of individuals to collect data about their attitudes, opinions, beliefs, and behaviors. Surveys can be conducted through various means, including online, telephone, and face-to-face interviews. Interviews are another method used in social science research to collect data. Interviews involve asking individuals questions about their experiences, attitudes, and opinions in a one-on-one or group setting. Interviews can be structured, semi-structured, or unstructured, depending on the research question. Observations are a method used to collect data about human behavior by observing individuals in natural or controlled settings. Researchers can collect data through direct observation or by using technology to capture behavior, such as video or audio recordings. Experiments involve manipulating one or more variables to observe their effect on a dependent variable. Experiments can be conducted in a controlled laboratory setting or in the natural environment. Data scraping is a method of data collection that involves using software or code to extract information from websites or other online sources. Data scraping can be a useful tool for gathering large amounts of data quickly, and it can be used for a variety of purposes, including market research, sentiment analysis, and trend analysis. In social science research, data collection must be conducted ethically and with informed consent from participants. Researchers must also consider issues of bias and sampling to ensure that their data collection methods produce accurate and representative data. 1.3 Types of data 1.3.1 Origin of data Primary and secondary data are two types of data used in research, and they differ in their origin and collection method. Primary data is original data that is collected by the researcher or research team through direct observation, experimentation, surveys, interviews, or other methods. Primary data is often collected specifically for the research project at hand, and it is tailored to the research question and objectives. Primary data is generally more expensive and time-consuming to collect compared to secondary data, but it is often more accurate and reliable since the researcher has more control over the data collection process. Secondary data, on the other hand, is data that has already been collected and compiled by others for other purposes. This can include data from sources such as government reports, academic journals, newspapers, and industry reports. Secondary data can be accessed easily and is usually less expensive and less time-consuming to obtain compared to primary data. However, the accuracy and reliability of secondary data can be a concern, as it may not have been collected with the specific research question or objectives in mind, or it may be outdated or biased. A lot of data comes ready for analysis and free for research purposes. Make us of it. 1.3.2 Analysis of data Qualitative and quantitative data are two types of data used in research, and they differ in their nature and analysis methods. Qualitative data is non-numerical data that is collected through open-ended questions, observations, or other non-structured methods. This data can be text, audio or visual. Qualitative data is often descriptive and subjective, and it provides insight into how individuals perceive and interpret the world. Qualitative data is often analyzed using methods such as thematic analysis, content analysis, or discourse analysis, and it can provide rich and detailed insights into complex phenomena. There are quantitative approaches to analyse text (text mining, e.g. sentiment analysis) and visual data (machine learning, e.g. image classification) as well. Quantitative data, on the other hand, is numerical data that is collected through structured methods such as surveys or experiments. Quantitative data is often used to test hypotheses and to measure the magnitude and frequency of a particular phenomenon. Quantitative data is analyzed using statistical methods, such as regression analysis or hypothesis testing, and it provides objective and standardized results. That being said, quantitative data is usually expressed in numerical form and can represent size, length, duration, amount, price, and so on. Sometimes quantitative data is understood as metric continuous as opposed to qualitative data in the form of categorical data. 1.3.3 Structure of data Rectangular data is a type of data structure that is commonly used to organize and store data in tables or spreadsheets. In rectangular data, the rows represent individual observations or cases, while the columns represent variables or attributes that describe the observations. Each cell in the table represents a single value for a particular observation and variable. Rectangular data is also known as \"tabular data\" or \"relational data,\" and it is the most common type of data used in quantitative research. Rectangular data is used to store various types of data, including demographic data, survey responses, financial data, and experimental data. Two common types of tabular data are cross-sectional and panel data that differ in their nature and the research question they address. Cross-sectional data is collected at a single point in time, from a sample of individuals, organizations, or other units of analysis. Cross-sectional data provides a snapshot of a particular phenomenon at a specific point in time, and it can be used to analyze differences and similarities between groups. Cross-sectional data can be collected through surveys, experiments, or other methods, and it is often analyzed using descriptive statistics, such as means, medians, or percentages. Panel data, on the other hand, is longitudinal data that is collected from the same individuals, organizations, or other units of analysis over time. Panel data provides information on how a particular phenomenon changes over time, and it allows for the analysis of individual-level changes and the identification of causal relationships. Panel data can be collected through surveys, experiments, or other methods, and it is often analyzed using methods such as regression analysis or difference-in-differences. 1.3.4 The level of access Open data refers to data that is made available to the public without restrictions or limitations on its use, reuse, and redistribution. This means that anyone can access, use, and share the data without needing permission or paying fees. One example is the official portal for European data is called the European Data Portal (EDP). It is a comprehensive platform that provides access to public datasets from various European Union (EU) institutions and other sources. The EDP aims to promote the sharing and use of open data across Europe by offering a centralized platform for finding, accessing, and using data. Open data is licensed under an open license. An open license is a type of license that allows others to access, use, and share a work or product while also providing certain freedoms and protections to the creator or owner of the work. Open licenses are often used for software, content, and data, and they typically include conditions that allow for free distribution and modification of the work. The statistical office in Germany provides open data under the Data Licence Germany 2.0. Most Wikipedia texsts are licensed under Creative Commons Attribution-ShareAlike 3.0. The Creative Commons Attribution-ShareAlike 3.0 (CC BY-SA 3.0) license is a type of open license that allows others to share and adapt a work, as long as they give credit to the original creator and distribute any derivative works under the same or a similar license. 1.4 Can we measure everything? In order to conduct meaningful measurement we need to make sure that we have a good understanding about the concept in question and its units of measurement. Some concepts are easy to grasp and there is a broad consensus on how to measure them. Remember Figure 1.2 that shows a global surface temperature. Temperature can be measured in Celsius or Fahrenheit (SI units). There is an accepted translation between the Celsius and Fahrenheit, i.e. 0°C correspond to 32°F and 0°F correspond to -17.8°C. We have a good understanding of what 10°C means and even know how 10°C \"feels\". Still, sometimes (or most of the time?) we just don't have the data we like to have. Thus the graph shows a temperature reconstruction using indirect proxy data from tree rings, corals, and ice cores as well as directly observed data (when available). Definition Variables are manifest or observed when they are directly measurable or hidden or latent when they are \"idealized constructs at best only indirectly measurable\". -- Encyclopaedia of Statistical Sciences (1999) The number of Twitter follower at a given time is technically determined and can be counted. It is a natural number. Age is indisputable measured in years but it could also be measured in month or days. For most analytically purposes years will be fine. Exact birth dates may not be available due to data protection aspects. Some concepts are harder to grasp and require a specific argument. Think about intelligence, populism, happiness or humor. What exactly are they and how can they be measured? Definition Operationalization means turning abstract concepts into measurable observations. It involves clearly defining your variables and indicators.6 See this joke: Q: “What’s the difference between England and a tea bag? “ A: “The tea bag stays in the cup longer.” What would be the best way of measuring how funny the joke is? We could measure physiological responses to jokes, such as heart rate, respiration rate, or facial expressions. Researchers may use brain imaging techniques, such as functional magnetic resonance imaging (fMRI), to measure brain activity in response to jokes. Many times, for many analytically purposes this will be an overkill. It will be expensive in terms of money and time. Thus most of the time, we ask people how funny they rate this joke on a scale from 1 to 10 where 1 refers to \"not at all funny\" and 10 refers to \"extremely funny\"? It is tempting to measure all social phenomena on a scale from 1 to 10. Wechsler Adult Intelligence Scale (version IV, released 2008) measures intelligence. Scores from intelligence tests are estimates of intelligence. Unlike, for example, distance and mass, a concrete measure of intelligence cannot be achieved given the abstract nature of the concept of \"intelligence\". Once measured, concepts can be related to each other: What is the relation between age and Twitter usage? (See chapter Relationships) What is the effect of intelligence on happiness? (See chapter Regression) 1.5 The reality behind the data ... it is important not to lose sight of the individuals whose lives provide the data for the models. Although variables rather than individual people may become the subjects of the statistician’s narrative, it is individuals rather than variables who have the capacity to act and reflect on society. Elliott, 1999. In: Byrne (2002) Sometimes, statisticians may become so focused on the data and the patterns they observe that they forget about the individuals behind the data. But it's important to keep in mind that it is ultimately individuals who are affected by the decisions and policies that are informed by the data. People have the ability to act and reflect on society, and understanding their experiences and perspectives is critical to building models and making decisions that truly reflect the needs and values of society as a whole. Data from 1800 to 1950 comes from Gapminder v6, for 1950 to 2014 from UN estimates and 2015 to 2099 from UN forecasts of future fertility.↩︎ It is 2.1 rather than 2 to allow for the failure of some female children themselves to live through the fertile years of adult life.↩︎ According to Tian Zheng (Columbia College), the average American knows about 600 people. NY times https://www.nytimes.com/2013/02/19/science/the-average-american-knows-how-many-people.html↩︎ Read more: https://www.scribbr.com/methodology/operationalization/↩︎ "],["stories-and-visuals.html", "Chapter 2 Stories and Visuals 2.1 Facts 2.2 Visualization 2.3 Telling a story 2.4 Man's best friend 2.5 Less is more 2.6 Grammar of Graphics", " Chapter 2 Stories and Visuals Stories make data and numbers memorable. Stories are everywhere. When you think about the context of the data, stories evolve naturally. How does the data connect to you, your friends and family, the work environment or society as a whole. 2.1 Facts A good story is based on facts and reliable sources. When evaluating facts, it's important to consider the source of the information and the evidence supporting it. According to OECD data on part-time employment rate (2021):7 36 % of women worked part-time in Germany whereas only 10 % of men did. 2.2 Visualization Good data visualization helps to convey complex information in a way that is easily understandable and accessible to a wide range of people. By presenting data in a visually appealing and intuitive way, it can help people to identify patterns, trends, and relationships that might not be immediately apparent from a simple data table or text-based analysis. Why not just tell the numbers as is? An important aspect of data science is to communicate information clearly and efficiently. Complex data is made more accessible. Data visualization reveals the data. 2.3 Telling a story Data storytelling is the practice of using data and visuals to communicate a narrative or message to an audience. It involves combining data, analysis, and storytelling techniques to create a compelling and engaging narrative that can inform, persuade, and inspire action. Data storytelling is necessary and good because it helps people make sense of complex data and information. By presenting data in a clear and visually appealing way, data storytelling can help people understand the meaning behind the numbers, identify patterns and trends, and gain insights into important issues and problems. By weaving a compelling narrative around the data, we can help our audience to understand the insights that we have discovered and why they matter. A data story can also help to make the data more memorable and emotionally resonant, which can help to further engage our audience and increase their interest in the subject matter. Once upon a time ... in a land not too far away, there were two siblings named Alex and Jamie. Alex and Jamie were very close in age and grew up in the same household with the same parents, but they had very different personalities and interests. As they reached adulthood, Alex decided to pursue a career in finance and secured a full-time job at a large investment bank. Jamie, on the other hand, decided to focus on their passion for art and took on a part-time job as a freelance graphic designer while also working on personal creative projects. Over time, Alex and Jamie noticed a stark difference in the way their work and career choices were perceived by society. Alex was praised for their ambition and dedication to their career, while Jamie was often questioned or criticized for not having a full-time job with benefits and stability. Alex was also more likely to receive promotions and higher salaries, while Jamie struggled to make ends meet and was sometimes overlooked for opportunities because of their part-time status. It became clear to Alex and Jamie that there was a gendered expectation for men to pursue full-time, high-paying careers while women were expected to prioritize caregiving or creative pursuits over financial stability. Despite these societal expectations, Alex and Jamie continued to pursue their individual paths and support each other's choices. They hoped that someday, society would recognize the value and importance of all types of work and careers, regardless of gender or perceived societal norms. 2.4 Man's best friend Humans love dogs. Dogs were domesticated by humans over 15,000 years ago. They can be perfect companions for singles, for couples for families. They differ in behavior, longevity and appetite. Figure ?? combines 6 dog characteristics in a dog score and compares this with the popularity of different breeds. From this scatterplot the authors define four categories of dog breads, e.g. the hot dogs and overlooked treasures (similar to a BCG matrix). Figure 2.1: The Ultimate Dog Data by informationisbeautiful. On the one hand, this is an awesome chart that transforms a huge data table in one graph, a scatter plot on two dimensions lightened by the individual dog icons for each data point. On the other hand, this graph is so complex and does not offer a major takeaway. Looking more closely to the graph, raises more questions then the graph answers, i.e. how was grooming and appetite measured? How are the 6 factors combined in the data score? When you look even more closely, you notice the easter egg.8 2.5 Less is more Figure ?? is an awesome data aggregate. Still, \"less is more\" in data visualization because too much information or visual clutter can overwhelm and confuse the audience, making it harder for them to understand the key insights and trends in the data. The complex scatterplot demands a lot of attention. Follow the slide presentation: The concept of data-ink ratio was introduced by data visualization expert Edward Tufte. It refers to the proportion of ink or pixels used to represent actual data in a visualization, as opposed to non-data elements like gridlines, borders, or labels. The reason why we should care about data-ink ratio is that it directly affects the clarity and effectiveness of the visualization. The more ink or pixels we use to represent non-data elements, the less space we have to represent actual data, which can make it harder for the audience to discern the insights and trends in the data. Definition The data-ink ratio is the proportion of Ink that is used to present actual data compared to the total amount of ink (or pixels) used in the entire display. Good graphics should include only data-Ink. Non-Data-Ink is to be deleted everywhere where possible. 2.6 Grammar of Graphics ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. ggplot2 is now over 10 years old and is used by hundreds of thousands of people to make millions of plots. Search OECD database https://stats.oecd.org/index.aspx?queryid=54746#↩︎ The frightened cat among all dogs.↩︎ "],["panel-data.html", "Chapter 3 Panel Data 3.1 Types of Data 3.2 Unemployment 3.3 Application 3.4 Panel Studies", " Chapter 3 Panel Data Panel data are observations for the same subjects over time. Subjects can be people, households, firms or countries. Panel data are a subset of longitudinal data. Key components are the panel identifier: person (id) and time (year). Every row is a person-year combination (so called long format). 3.1 Types of Data 3.1.1 Cross-section Cross-sectional data is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the one point or period of time. Questions about levels: \"How many people are poor in 2016 in Germany?\" Questions about differences: \"How are men and women affected by poverty?\" 3.1.2 Repeated cross-section Cross-sectional survey data are data for a single point in time. Repeated cross-sectional data are created where a survey is administered to a new sample of interviewees at successive time points. For an annual survey, this means that respondents in one year will be different people to those in a prior year. Such data can either be analysed cross-sectionally, by looking at one survey year, or combined for analysis over time. Questions about trends: \"Has poverty increased or decreased?\" 3.1.3 Time series Time series is data on a single subject at multiple points in time. Most commonly, data is collected at successive equally spaced points in time e.g. daily, annually. If data is collected annually, it's likely to be a survey study. If data is collected more frequently, e.g. daily, it's likely to be meteorology or finance. A time series is very frequently plotted via a run chart (which is a temporal line chart). An example of a temporal line chart is total number of students per year at Viadrina in the next chapter. Questions about trends: \"Is there a seasonal component in unemployment?\" 3.1.4 Panel data With panel data we know the time-ordering of events. Panel data allow to identify causal effects under weaker assumptions (compared to cross-sectional data). Especially, panel data allows for certain statistical analyses, e.g. [fixed effects regression][Fixed effects]. * Questions about change: \"How many people went in and out of poverty?\" 3.2 Unemployment Unemployment occurs when someone is willing and able to work but does not have a paid job. Unemployment is measured by the unemployment rate. The unemployment rate is the most commonly used indicator for understanding conditions in the labour market. The personal and social costs of unemployment include severe financial hardship and poverty, debt, homelessness and housing stress, family tensions and breakdown, boredom, alienation, shame and stigma, increased social isolation, crime, erosion of confidence and self-esteem, the atrophying of work skills and ill-health. McClelland and Macdonald (1998) 3.2.1 On decline in Germany Figure 3.1: Unemployment rate in Germany Reading How Low Can Unemployment Really Go? Economists Have No Idea \"Here are two things most economists can agree upon: They want an economy where everyone who seeks a job can get one. Yet for the economy to be dynamic, some people will always be unemployed, at least temporarily as they move between jobs.\" Life after college. Panel data allows to analyze the level of unemployment in Germany as well as the changes and trajectories of individuals. We can separate a frictional unemployment component and a permanent unemployment share. Frictional unemployment is a form of unemployment reflecting the gap between someone voluntarily leaving a job and finding another. As such, it is sometimes called search unemployment. Is search unemployment acceptable? Is it different from long-term unemployment? What do you think. 3.2.2 Measurement The unemployment rate represents the proportion of the civilian labour force that is unemployed. Consequently, measuring the unemployment rate requires identifying who is in the labour force. The labour force consists of all employed and unemployed persons of working age. What exactly is defined as employment? Employment status can be defined via a threshold of working hours or income. Who is in the working age? Reading In Australia, the Australian Bureau of Statistics (ABS) conducts a survey each month – called the Labour Force Survey – in which it asks around 50,000 people. As part of this survey, the ABS groups people aged 15 years and over (the working-age population) into three broad categories: Employed – includes people who are in a paid job for one hour or more in a week. Unemployed – people who are not in a paid job, but who are actively looking for work. Not in the labour force – people not in a paid job, and who are not looking for work. Read More: Unemployment: Its Measurement and Types 3.3 Application 3.3.1 Data Inspection SOEP practice data (2015 - 2019) comes labeled and ready for analysis. SOEP provides a digital object identifier (DOI) for this data: https://doi.org/10.5684/soep.practice.v36.9 library(haven) soep &lt;- read_dta(&quot;https://github.com/MarcoKuehne/marcokuehne.github.io/blob/main/data/SOEP/practice_en/practice_dataset_eng.dta?raw=true&quot;) This practice data contains socio-economic information on children, education, job, health, satisfaction and income. It contains 15 variables and 23522 observations. Unique (#) Missing (%) Mean SD Min Median Max syear 5 0 2016.8 1.4 2015.0 2017.0 2019.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } sex 2 0 0.5 0.5 0.0 1.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } alter 86 0 48.3 18.3 17.0 48.0 102.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } anz_pers 13 0 2.9 1.5 1.0 2.0 13.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } anz_kind 12 0 0.7 1.1 0.0 0.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } bildung 17 7 12.4 2.8 7.0 11.5 18.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } erwerb 7 0 3.0 1.8 1.0 2.0 6.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } branche 84 42 60.5 25.3 1.0 64.0 99.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } gesund_org 6 0 2.6 1.0 1.0 2.0 5.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } lebensz_org 12 3 7.4 1.7 0.0 8.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenj1 13346 0 16775.8 22707.6 0.0 5786.1 269424.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenj2 1871 0 315.7 1887.6 0.0 0.0 79179.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenm1 14067 0 1645.6 2220.6 0.0 852.9 35260.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } einkommenm2 607 0 11.0 134.0 0.0 0.0 12033.2 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Column names are German, but attribute labels are in English. einkommenj1 contains Gross Income from Main Job/Year. attributes(soep$einkommenj1)$label #&gt; [1] &quot;Gross Income from Main Job/Year&quot; The documentation (click the DOI https://doi.org/10.5684/soep.practice.v36) tells that there are 6.355 people in the data. Every individual is likely observed multiple times (i.e. panel data). Pipe soep into count() of personal id. The tibble output already contains the number of rows. To literally access the value, ask nrow(). # soep %&gt;% count(id) # try this as well soep %&gt;% count(id) %&gt;% nrow() #&gt; [1] 6355 Adding arrange() means sorting the data by a variable (i.e. the temporarily created variable n) either ascending or descending (from high to low). Ascending is the default. For descending order apply the desc() command. group_by() is a powerful command, especially when working with panel data. It can do any form of data manipulation or analysis with respect to the chosen variable. At this stage it's a mere alternative count(). # soep %&gt;% group_by(id) %&gt;% count() %&gt;% arrange(n) # try this alternative soep %&gt;% count(id) %&gt;% arrange(n) soep %&gt;% count(id) %&gt;% arrange(desc(n)) See result. #&gt; # A tibble: 6,355 × 2 #&gt; id n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 96 1 #&gt; 2 137 1 #&gt; 3 147 1 #&gt; 4 183 1 #&gt; 5 189 1 #&gt; 6 229 1 #&gt; 7 368 1 #&gt; 8 371 1 #&gt; 9 384 1 #&gt; 10 443 1 #&gt; # ℹ 6,345 more rows Remember that the observation period is between 2015 and 2019, i.e. the minimum number of observations per individual is 1 year, the maximum is 5 years. Over the years, observations get less and less (panel attrition). table(soep$syear) #&gt; #&gt; 2015 2016 2017 2018 2019 #&gt; 5527 4987 4720 4351 3937 How many people are observed in all years? Filter for a specific n and ask for the number of rows or observations (alternatively you can check the tibble size again). soep %&gt;% group_by(id) %&gt;% count() %&gt;% filter(n == 5) %&gt;% nrow() #&gt; [1] 3113 erwerb is the employment status in SOEP. Its labels range from -7 to 6. Use the attributes() command on a specific variable. It returns a set of information (object class is list). We can access elements of this list by the $ operator. Did you notice the small mistake in the labels? (Remember this is practice data.) attributes(soep$erwerb)$labels See result. #&gt; [-7] Only available in less restricted edition #&gt; -7 #&gt; [-6] Version of questionnaire with modified filtering #&gt; -6 #&gt; [-5] Not included in this version of the questionnaire #&gt; -5 #&gt; [-4] Inadmissable multiple response #&gt; -4 #&gt; [-3] not valid #&gt; -3 #&gt; [-2] does not apply #&gt; -2 #&gt; [-1] no answer #&gt; -1 #&gt; [-1] Employed full-time #&gt; 1 #&gt; [-2] Employed part-time #&gt; 2 #&gt; [3] Training, apprenticeship #&gt; 3 #&gt; [4] Irregular employment or in marginal #&gt; 4 #&gt; [5] Not employed #&gt; 5 #&gt; [6] Garage for disabled people #&gt; 6 Note that the output further tells you # A tibble: 3,550 x 2, i.e. there are 3550 ID-groups (or units or people). Negative values indicate several different forms of missing data in SOEP.10 Actually, there are no negative values in this dataset. As for levels of a factor variables, labels can be empty. table(soep$erwerb) #&gt; #&gt; 1 2 3 4 5 6 #&gt; 8700 3481 695 1446 9169 29 3.3.2 Data Preparation We summarize categories with a combination of mutate() and case_when(). For each value in erwerb conduct a logical comparison via == and assign a new value. In this case it combines Employed part-time with Irregular employment or in marginal. soep &lt;- soep %&gt;% mutate(erwerb = case_when(erwerb == 1 ~ &quot;fulltime&quot;, erwerb == 2 ~ &quot;partime&quot;, erwerb == 3 ~ &quot;partime&quot;, erwerb == 4 ~ &quot;partime&quot;, erwerb == 5 ~ &quot;unemployed&quot;, erwerb == 6 ~ &quot;partime&quot;, TRUE ~ &quot;NA&quot;)) The unemployment rate represents the proportion of the civilian labour force that is unemployed. The labour force consists of all employed and unemployed persons of working age. Filter for working age between 18 and 67 years. There are 472 observations younger than 18 and 4125 older than 67. soep &lt;- soep %&gt;% filter(alter %in% c(18:67)) The moment of glory has come for group_by(). It accepts multiple inputs. The following combination returns for each year and each employment status the number of observations with help of summarise() and n(). soep %&gt;% group_by(syear, erwerb) %&gt;% summarise(n = n()) See result. #&gt; # A tibble: 15 × 3 #&gt; # Groups: syear [5] #&gt; syear erwerb n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 2015 fulltime 2013 #&gt; 2 2015 partime 1261 #&gt; 3 2015 unemployed 1268 #&gt; 4 2016 fulltime 1808 #&gt; 5 2016 partime 1123 #&gt; 6 2016 unemployed 1087 #&gt; 7 2017 fulltime 1750 #&gt; 8 2017 partime 1036 #&gt; 9 2017 unemployed 1007 #&gt; 10 2018 fulltime 1620 #&gt; 11 2018 partime 985 #&gt; 12 2018 unemployed 874 #&gt; 13 2019 fulltime 1467 #&gt; 14 2019 partime 904 #&gt; 15 2019 unemployed 722 From this table, the unemployment rate for 2015 can be calculated manually: \\[\\frac{1268}{2013+1261+1268} = 0.279172 = 27.92 \\%\\] Having done this, step back to focus on years again and use different n-values per year to figure out the unemployment. Relate unemployed to those working either fulltime or partime. soep %&gt;% group_by(syear, erwerb) %&gt;% summarise(n = n()) %&gt;% group_by(syear) %&gt;% mutate(unemployment_rate = n[3]/(n[1]+n[2]+n[3])) See result. #&gt; # A tibble: 15 × 4 #&gt; # Groups: syear [5] #&gt; syear erwerb n unemployment_rate #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2015 fulltime 2013 0.279 #&gt; 2 2015 partime 1261 0.279 #&gt; 3 2015 unemployed 1268 0.279 #&gt; 4 2016 fulltime 1808 0.271 #&gt; 5 2016 partime 1123 0.271 #&gt; 6 2016 unemployed 1087 0.271 #&gt; 7 2017 fulltime 1750 0.265 #&gt; 8 2017 partime 1036 0.265 #&gt; 9 2017 unemployed 1007 0.265 #&gt; 10 2018 fulltime 1620 0.251 #&gt; 11 2018 partime 985 0.251 #&gt; 12 2018 unemployed 874 0.251 #&gt; 13 2019 fulltime 1467 0.233 #&gt; 14 2019 partime 904 0.233 #&gt; 15 2019 unemployed 722 0.233 Definition Tidy data principles are: Every column is a variable. Every row is an observation. Every cell is a single value. The unemployment rate in this data is unreasonably high. 3.3.3 Data Visualization 3.3.3.1 Unemployment status Pick up the calculation of the unemployment rate and pipe it into a ggplot() call. Define the axes elements within aes() (for aesthetics) and ask for points that are connected by a line. soep %&gt;% group_by(syear, erwerb) %&gt;% summarise(n = n()) %&gt;% group_by(syear) %&gt;% mutate(unemployment_rate = n[3]/(n[1]+n[2]+n[3])) %&gt;% ggplot(aes(x = syear, y = unemployment_rate)) + geom_point() + geom_line() + labs(x=&quot;Year&quot;, y=&quot;Unemployment Rate&quot;, title = &quot;Unemployment Rate in Germany between 2015 and 2019&quot;, subtitle = &quot;SOEP practice data&quot;) 3.3.3.2 Obs per individual Pick up the number of observations per individual and pipe it in a barplot with geom_bar(). soep %&gt;% group_by(id) %&gt;% count() %&gt;% ggplot(aes(x = n)) + geom_bar() + labs(title = &quot;Number of observations per individual&quot;) + theme_classic() 3.4 Panel Studies Famous household panel data studies include: United States: Panel Study of Income Dynamics (PSID) since 1968 Germany: Socio-Economic Panel (SOEP) since 1984 United Kingdom: British Household Panel Survey (BHPS) since 1991 Australia: Household, Income and Labour Dynamics in Australia Survey (HILDA) since 2001 These scientific datasets can often be analyzed for research and student theses free of charge. Why are DOIs important? A DOI is a unique identifier for a digital document. DOIs are important in academic citation because they are more permanent than URLs, ensuring that your reader can reliably locate the source. Read More: What is a DOI? | Finding and Using Digital Object Identifiers↩︎ Read more on the SOEPcompanion Missing Conventions↩︎ "],["web-data.html", "Chapter 4 Web Data 4.1 Most expensive paintings 4.2 Student numbers at Viadrina", " Chapter 4 Web Data “Over the last two years alone, 90 percent of the data in the world was generated.” Bernard Marr (2018) How Much Data Do We Create Every Day? The Mind-Blowing Stats Everyone Should Read Forbes. Data scraping is a technique where a computer program extracts data from human-readable output coming from another program. Data scraping often takes place in web scraping (also known as crawling or spidering). In this process, an application is used to extract valuable information from a website. PDF scraping or more general report mining is the extraction of data from human-readable computer reports. It is worth considering alternatives before start scraping data.11 4.1 Most expensive paintings What do The Card Players and Marshall Islands have in common? Figure 4.1: The Card Players.   Figure 4.2: Flag of Marshall Islands. Well, the first was sold for about 250.000.000 million USD in an auction in 2011 and the laters nominal gross domestic product is of similar size 220.000.000 million USD in 2019. About 60.000 people live in Marshall Islands. The most expensive paintings score similar to the gross domestic product of insular states. This is the English Wikipedia article list of highest prices ever paid for paintings. In a web browser use the keyboard shortcut CTRL + U (in Google Chrome, Firefox, Microsoft Edge, Opera) to view the source code of a webpage. Alternatively, right click and choose \"show source code\". &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;style&gt; table, th, td { border:1px solid black; } &lt;/style&gt; &lt;body&gt; &lt;h2&gt;A basic HTML table&lt;/h2&gt; &lt;table style=&quot;width:100%&quot;&gt; &lt;tr&gt; &lt;th&gt;Company&lt;/th&gt; &lt;th&gt;Contact&lt;/th&gt; &lt;th&gt;Country&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alfreds Futterkiste&lt;/td&gt; &lt;td&gt;Maria Anders&lt;/td&gt; &lt;td&gt;Germany&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Centro comercial Moctezuma&lt;/td&gt; &lt;td&gt;Francisco Chang&lt;/td&gt; &lt;td&gt;Mexico&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;To understand the example better, we have added borders to the table.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; One approach is this. Use read_html() from rvest package to download the webpage. Extract the node table.wikitable with the html_nodes() command. Convert this node into a table with html_table() setting header=TRUE. Use this data in a pipe %&gt;% with bind_rows() and as_tibble(). You can use any other scraping approach. Store the final table of expensive paintings in paintings. The following DT table should yield identical results. 4.2 Student numbers at Viadrina Did you notice less and less fellow students sitting next to you? We analyse enrollment numbers of Viadrina European University. Dezernat 1 administers and publishes student statistics. There is an overview page where they provide some time series data (overall student numbers from 1992 to 2020) and the number of study programs (between 1992 to 2014). Information is presented in two ways, as tables in PDF, e.g. the time series on total student numbers:12 And as tables in HTML. For each semester there is summary information on the webpage (and more comprehensive data in PDF), like for winter term 2022/2023: Most web browser allow to inspect the html source: view-source:https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2022-wintersemester/index.html When you search for a characteristic value like 4.797 it shows: &lt;div class=&quot;zeile&quot;&gt;&lt;div class=&quot;dreispaltig&quot;&gt; &lt;div class=&quot;text&quot;&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;width: 400px;&quot;&gt;Studierende gesamt&lt;/td&gt; &lt;td&gt;4.797&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;weiblich&lt;/td&gt; &lt;td&gt;2.785&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;m&amp;auml;nnlich&lt;/td&gt; &lt;td&gt;2.012&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Deutsche&lt;/td&gt; &lt;td&gt;3.201&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ausl&amp;auml;nder/innen&lt;/td&gt; &lt;td&gt;1.596&lt;/td&gt; Let's investigate the student numbers. 4.2.1 PDF scraping Apply a programmatic way to download a PDF Use the built in download.file() function and specify an url and a destination and file name. # Set URL for PDF url &lt;- &quot;https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/Entwicklung-der-Gesamtstudierendenzahl.pdf&quot; # Specify destination where file should be saved (and name) destfile &lt;- &quot;data/Viadrina/Gesamtstudierendenzahl.pdf&quot; # Apply download.file function in R download.file(url, destfile, mode = &quot;wb&quot;) To extract a table from a PDF we use the pdftables package.13 The package is a wrapper for the PDFTables API. It requires an API key from https://pdftables.com/. You can register and get one for free. The result is stored as .csv file. Definition An application programming interface (API) is a way for two or more computer programs to communicate with each other. It is a type of software interface, offering a service to other pieces of software. Be careful with your API keys. If you only use a file locally on your computer, you might be fine. Don't share this file. Don't upload it. If you upload an API key on GIT, you get a notification mail from https://www.gitguardian.com/. Instead, put your API in your environment. This can be done by a .Renviron file. Use usethis::edit_r_environ(scope = \"project\") in order to access and edit your information. 50 pages are for free. Read more * https://daattali.gitbooks.io/stat545-ubc-github-io/content/bit003_api-key-env-var.html * https://resources.numbat.space/using-rprofile-and-renviron.html * https://github.com/expersso/pdftables library(pdftables) convert_pdf(input_file = destfile, output_file = &quot;data/Viadrina/viadrina_students.csv&quot;, api_key = &quot;YOUR_KEY&quot;) Scraped data often requires a lot of cleaning. library(tidyverse) # read csv file viadrina_1992_2020_before &lt;- read.csv(&quot;data/Viadrina/viadrina_students.csv&quot;, header = TRUE) viadrina_1992_2020 &lt;- viadrina_1992_2020_before # replace header names by first row names(viadrina_1992_2020) &lt;- viadrina_1992_2020[1,] # drop first row viadrina_1992_2020 &lt;- viadrina_1992_2020[-1,] # drop last row viadrina_1992_2020 &lt;- viadrina_1992_2020[-30,] # remove all \\n colnames(viadrina_1992_2020) &lt;- gsub(&quot;[\\r\\n]&quot;, &quot;&quot;, colnames(viadrina_1992_2020)) # readable column names colnames(viadrina_1992_2020) &lt;- c(&quot;Year&quot;, &quot;Total&quot;, &quot;Female&quot;, &quot;Female_Pct&quot;, &quot;German&quot;, &quot;German_Pct&quot;, &quot;Foreign&quot;, &quot;Foreign_Pct&quot;, &quot;Pole&quot;, &quot;Pole_Pct&quot;) # remove percentage sign viadrina_1992_2020 &lt;- viadrina_1992_2020 %&gt;% mutate(across(everything(), ~ ifelse(str_detect(.x, &quot;%&quot;), parse_number(.x) / 10, .x))) # convert all chr to numeric viadrina_1992_2020 &lt;- viadrina_1992_2020 %&gt;% mutate_if(is.character,as.numeric) library(DT) datatable(viadrina_1992_2020) 4.2.2 Share of female students The share of female students over time in a line plot. viadrina_1992_2020 %&gt;% ggplot(aes(x=Year)) + geom_line(aes(y = Total, colour = &quot;Total&quot;)) + geom_line(aes(y = Female, colour = &quot;Female&quot;)) + labs(title = &quot;Student numbers at Viadrina&quot;, subtitle = &quot;Share of female students&quot;) 4.2.3 Share of foreign students The share of foreign students over time in a stacked barplot. viadrina_1992_2020 %&gt;% ggplot(aes(x=Year)) + geom_col(aes(y = Total, fill = &quot;Total&quot;)) + geom_col(aes(y = German, fill = &quot;German&quot;)) + geom_col(aes(y = Foreign, fill = &quot;Foreign&quot;)) + labs(title = &quot;Student numbers at Viadrina&quot;, subtitle = &quot;Share of foreign students&quot;) 4.2.4 Web scraping To get more recent numbers, we rely on the webpage information. How can we access all semester data? Investigate the pattern of the URLs. https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2013-Wintersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2014-Sommersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2014-Wintersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2015-Sommersemester/index.html https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2015-wintersemester/index.html Usually the pattern is year-semester. Some winter semesters are capitalized, some are not (Wintersemester vs. wintersemester). We use the rvest package to web scrape static html content. It always starts with reading in the html data. Then pipe and use html_table() to extract tabular information (lucky us, there is only one table). library(rvest) read_html(&quot;https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/2013-Wintersemester/index.html&quot;) %&gt;% html_table() #&gt; [[1]] #&gt; # A tibble: 7 × 2 #&gt; X1 X2 #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Studierende gesamt 6645 #&gt; 2 weiblich 4206 #&gt; 3 männlich 2439 #&gt; 4 Deutsche 5001 #&gt; 5 Ausländer/innen 1644 #&gt; 6 1. Fachsemester 1783 #&gt; 7 1. Hochschulsemester 1110 Now, we would like to do this for every subpage (semester) and combine the data in one table. Focus on the variable component in the URL and create a vector of all semesters. # the manual way winters &lt;- seq(from=2013, to=2022) summers &lt;- seq(from=2014, to=2023) winters &lt;- paste0(winters, &quot;-wintersemester&quot;) summers &lt;- paste0(summers, &quot;-Sommersemester&quot;) all_terms &lt;- c(rbind(winters, summers)) all_terms[1] &lt;- &quot;2013-Wintersemester&quot; all_terms[3] &lt;- &quot;2014-Wintersemester&quot; # we can paste them together all_url &lt;- paste0(&quot;https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/&quot;, all_terms, &quot;/index.html&quot;) Create a for loop to repeat the rvest procedure for each element in the URL list. Definition A loop is a programming structure that repeats a sequence of instructions until a specific condition is met. There are two more components. We initialize a list to store each iteration of the loop. tables &lt;- list() index &lt;- 1 for(i in 1:length(all_url)){ table &lt;- all_url[i] %&gt;% read_html() %&gt;% html_table() tables[index] &lt;- table index &lt;- index + 1 } df &lt;- do.call(&quot;cbind&quot;, tables) More and more cleaning. df[,c(seq(from=3, to=40 , by=2))] &lt;- NULL colnames(df) &lt;- c(&quot;Variable&quot;, all_terms) # transpose dataframe viadrina_2013_2023 &lt;- as.data.frame(t(df)) # replace header names by first row names(viadrina_2013_2023) &lt;- viadrina_2013_2023[1,] # drop first row viadrina_2013_2023 &lt;- viadrina_2013_2023[-1,] Once more, there are slightly different formats for numbers in winter term 2019, 2020 and 2021. In these terms they use a . for digit grouping, e.g. 3.607 instead of 3607. # all chr to numeric # viadrina_2013_2022 &lt;- viadrina_2013_2022 %&gt;% mutate_if(is.character,parse_number) #viadrina_2013_2022 &lt;- viadrina_2013_2022 %&gt;% mutate_if(is.character,as_numeric) viadrina_2013_2023 &lt;- viadrina_2013_2023 %&gt;% mutate_if(is.character,as.numeric) # row 13, 15, 17, columns 1 to 6, multiply by 1000 viadrina_2013_2023[c(13,15,17,19), c(1:6)] &lt;- viadrina_2013_2023[c(13,15,17,19), c(1:6)] * 1000 # round all numbers # viadrina_2013_2022 &lt;- viadrina_2013_2022 %&gt;% mutate_if(is.numeric, round) # multiply some rows by 1000 # viadrina_2013_2022[c(13,15,17),] &lt;- viadrina_2013_2022[c(13,15,17),] %&gt;% # mutate_all(.funs = funs(. * 1000)) # fix last column and divide by 1000 # viadrina_2013_2022[c(13,15,17),7] &lt;- viadrina_2013_2022[c(13,15,17),7]/1000 # create a semester variable viadrina_2013_2023$year &lt;- substr(all_terms, start = 1, stop = 4) viadrina_2013_2023$term &lt;- rep(c(&quot;winter&quot;, &quot;summer&quot;), 10) viadrina_2013_2023$semester &lt;- paste0(viadrina_2013_2023$year, rep(c(&quot;-02&quot;, &quot;-01&quot;), 10)) # row names rownames(viadrina_2013_2023) &lt;- 1:nrow(viadrina_2013_2023) 4.2.5 Most recent student numbers There is a structural difference between summer and winter term. Most new enrollments are in winter. viadrina_2013_2023 %&gt;% ggplot(aes(x=semester, y = `Studierende gesamt`)) + geom_point(aes(col=term), size=2) + #geom_point(aes(y = `Studierende gesamt`, fill = &quot;Total&quot;)) + #geom_point(aes(y = `1. Fachsemester`, fill = &quot;First years&quot;)) + #geom_point(aes(y = `1. Hochschulsemester`, fill = &quot;First sem&quot;)) + labs(title = &quot;Student numbers at Viadrina&quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 4.2.6 The long run trend Combine old and new data. via_1992_2020 &lt;- viadrina_1992_2020 %&gt;% select(Year, Total) %&gt;% mutate(Term = &quot;winter&quot;) via_2013_2022 &lt;- viadrina_2013_2023 %&gt;% rename(Year = year, Term = term, Total = `Studierende gesamt`) %&gt;% select(Year, Total, Term) via_1992_2022 &lt;- rbind(via_1992_2020, via_2013_2022) via_1992_2022$Year &lt;- as.numeric(via_1992_2022$Year) Plot and polish. Assume the enrollment for winter 2020/2023 was not affected by Corona virus. via_1992_2022 %&gt;% filter(Term == &quot;winter&quot;) %&gt;% ggplot(aes(x=Year, y=Total)) + geom_point() + geom_smooth(method = &quot;gam&quot;) + scale_x_continuous(breaks = scales::pretty_breaks(n = 28)) + labs(title = &quot;Student numbers at Viadrina&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + geom_vline(xintercept=2020.5, colour=&quot;grey&quot;, linetype = &quot;solid&quot;, lwd = 1.3) + geom_text(aes(x=2021, y=4000, label=&quot;Corona&quot;), colour=&quot;red&quot;, angle=90, vjust = 1) (1) Look for a download button. (2) Search same or similar data somewhere else. (3) Check if there is an API. (4) Ask the website owner for the data.↩︎ Get the PDF https://www.europa-uni.de/de/struktur/verwaltung/dezernat_1/statistiken/Entwicklung-der-Gesamtstudierendenzahl.pdf↩︎ You may use other free services. Search for Online converter PDF to csv/xlsx↩︎ "],["text-data.html", "Chapter 5 Text Data 5.1 Wikipedia 5.2 Field Trip to Berlin", " Chapter 5 Text Data Text data usually consists of documents which can represent words, sentences or even paragraphs of free flowing text. The inherent unstructured (no neatly formatted data columns!) and noisy nature of textual data makes it harder for data analysts to directly work on raw text data. 5.1 Wikipedia Access the intro information on R from English Wikipedia using the wikifacts package: https://en.wikipedia.org/wiki/R_(programming_language) library(wikifacts) R_EN &lt;- wiki_define(&#39;R (programming language)&#39;) R_EN #&gt; R (programming language) #&gt; &quot;R is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing. Created by statisticians Ross Ihaka and Robert Gentleman, R is used among data miners, bioinformaticians and statisticians for data analysis and developing statistical software. The core R language is augmented by a large number of extension packages containing reusable code and documentation.\\nAccording to user surveys and studies of scholarly literature databases, R is one of the most commonly used programming languages in data mining. As of April 2023, R ranks 16th in the TIOBE index, a measure of programming language popularity, in which the language peaked in 8th place in August 2020.The official R software environment is an open-source free software environment released as part of the GNU Project and available under the GNU General Public License.&quot; 5.1.1 Cleaning Load the tidytext and stringr package. We summarize individual tasks like removing digits, punctuation, whitespaces and seting everything to lower case in the clean_text() function. library(tidyverse) library(tidytext) library(stringr) ## text cleaning clean_text &lt;- function(x) { x %&gt;% ## Remove digits str_remove_all(&quot;[:digit:]&quot;) %&gt;% ## Remove punctuation str_remove_all(&quot;[[:punct:]]&quot;) %&gt;% ## Make everything lowercase str_to_lower() %&gt;% ## Remove any trailing whitespace around the text str_trim(&quot;both&quot;) %&gt;% ## Remove newline character str_replace_all(&quot;[\\r\\n]&quot; , &quot; &quot;) } R_EN_clean &lt;- clean_text(R_EN) R_EN_clean #&gt; [1] &quot;r is a programming language for statistical computing and graphics supported by the r core team and the r foundation for statistical computing created by statisticians ross ihaka and robert gentleman r is used among data miners bioinformaticians and statisticians for data analysis and developing statistical software the core r language is augmented by a large number of extension packages containing reusable code and documentation according to user surveys and studies of scholarly literature databases r is one of the most commonly used programming languages in data mining as of april r ranks th in the tiobe index a measure of programming language popularity in which the language peaked in th place in august the official r software environment is an opensource free software environment released as part of the gnu project and available under the gnu general public license&quot; 5.1.2 Tidytext Format Tidy data has a specific structure: Each variable is a column Each observation is a row We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. library(tidyverse) tidytext &lt;- R_EN_clean %&gt;% as_tibble() %&gt;% unnest_tokens(word, value) %&gt;% count(word, sort=TRUE) head(tidytext) #&gt; # A tibble: 6 × 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 the 9 #&gt; 2 and 8 #&gt; 3 r 8 #&gt; 4 of 6 #&gt; 5 in 5 #&gt; 6 is 5 Notice that the, and, r, of, in, is do not contain a lot of valuable insights. 5.1.3 Stopwords Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so commonly used that they carry very little useful information. data(stop_words) tidytext &lt;- tidytext %&gt;% anti_join(stop_words) head(tidytext) #&gt; # A tibble: 6 × 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 language 4 #&gt; 2 data 3 #&gt; 3 programming 3 #&gt; 4 software 3 #&gt; 5 statistical 3 #&gt; 6 computing 2 5.1.4 Term Frequency Word clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of textual data (such as a speech, blog post, or database), the bigger and bolder it appears in the word cloud. library(wordcloud) wordcloud(words = tidytext$word, freq = tidytext$n, min.freq = 2, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) 5.1.5 Little bit of Scraping Text mining often goes hand in hand with data scraping. Use rvest package and read_html() to get the entire text of the R Wikipedia article. library(tidyverse) library(rvest) read_html(&quot;https://en.wikipedia.org/wiki/R_(programming_language)&quot;) %&gt;% ## extract paragraphs html_nodes(&quot;p&quot;) %&gt;% ## extract text html_text() %&gt;% ## clean clean_text() %&gt;% as_tibble() %&gt;% ## tidy text unnest_tokens(word, value) %&gt;% count(word, sort=TRUE) %&gt;% ## stopwords anti_join(stop_words) %&gt;% ## select first top_n(20) %&gt;% ## reorder mutate(word = reorder(word, n)) %&gt;% ## create frequency plot ggplot(aes(x=word, y = n)) + geom_col() + coord_flip() + ggtitle(&quot;Term Frequency of top 20 clean words in Wikipedia aRticle&quot;) R seems to be top-notch if you like to become fluent in data. 5.2 Field Trip to Berlin On Thursday, 02 June 2022, the class went on a field trip to Humboldt Forum, Berlin to visit the exhibition Berlin Global (get the virtual tour). Berlin Global feels very modern. It combines historical information and stories with modern art and design as well as interactive features. The interaction comes from decisions you can make yourself and interactive elements like a wheel that needs about 5 people to start a video sequence. It triggers interactions between visitors and people who are normally strangers who would not have interacted otherwise. It is educational and fun. Although it covers some of the darkest parts of (German) history. Students wrote down their impressions as in a diary entry or personal report and post them on tripadvisor. All reviews should address the following questions: How is globality represented in \"Berlin Global\"? How is the colonial past depicted in Room 1 \"Weltdenken\"? At the time, the exhibition did not have any review on tripadvisor. Thus almost all reviews on tripadvisor stem from students of the class of 2022. We use this review data on tripadivsor to learn about the student experience. library(DT) library(tidyverse) tripadvisor &lt;- read.csv(&quot;data/tripadvisor_berlin_global.csv&quot;) tripadvisor %&gt;% mutate(Text_Beginning = substr(Text, 1, 50)) %&gt;% select(Author, Title, Text_Beginning) %&gt;% datatable(options = list(pageLength = 5)) 5.2.1 Text Preparation Again, make use of the clean_text() function. # use clean_text() tripadvisor$Text_Clean &lt;- clean_text(tripadvisor$Text) # tidy text format for all text tidy_trip &lt;- tripadvisor %&gt;% unnest_tokens(word, Text_Clean) %&gt;% count(word, sort=TRUE) # tidy text per use tidy_reviews &lt;- tripadvisor %&gt;% group_by(Author) %&gt;% unnest_tokens(word, Text_Clean) tidy_trip &lt;- tidy_trip %&gt;% anti_join(stop_words) 5.2.2 Sentiment Analysis The sentimentr package offers several options to analyse sentiments at the sentence or aggregate level. A description can be found on GitHub https://github.com/trinker/sentimentr. Definition Sentiment analysis is the process of computationally identifying and categorizing text, especially in order to determine whether the writer's attitude is positive, negative, or neutral. 5.2.2.1 Aggregate Level The aggregate level summarizes all text information by author into one text block. The sentiment() function displays the total word count and an overall sentiment score (the higher the more positive). library(sentimentr) library(magrittr) library(dplyr) head(sentiment(tripadvisor$Text_Clean), n=10) #&gt; element_id sentence_id word_count sentiment #&gt; 1: 1 1 453 1.16515866 #&gt; 2: 2 1 73 0.48513556 #&gt; 3: 3 1 143 0.39303374 #&gt; 4: 4 1 317 0.28532123 #&gt; 5: 5 1 101 0.75612876 #&gt; 6: 6 1 391 0.01365449 #&gt; 7: 7 1 229 0.31192290 #&gt; 8: 8 1 152 0.19567959 #&gt; 9: 9 1 495 0.68421173 #&gt; 10: 10 1 136 0.46647615 element_id represents the textauthortitleimages. word_count is the total number of words per review TRUEFALSE. The value of sentiment can be negative TRUEFALSE. 5.2.2.2 Sentence Level Create a table from all reviews such that each sentence per person is a row. Use get_sentences() from the sentimentr package. It returns a list object. Find a way to unlist and collect the information in a dataframe. The sentence level analysis is based on original Text. sentences &lt;- get_sentences(tripadvisor$Text) sentence_level &lt;- data.frame(id = character(), sentence = character(), stringsAsFactors=FALSE) for (i in 1:nrow(tripadvisor)) { # Create data frame for 1 person all sentences tmp &lt;- as.data.frame(sentences[i], col.names = &quot;sentence&quot;) %&gt;% mutate(id = i) # Row bind each iteration sentence_level &lt;- rbind(sentence_level, tmp) } We can get a sentiment per sentence highlighted by red or green. This allows humans to skim through the reviews to discover insights. # Yeah sentence_level %&gt;% mutate(review = get_sentences(sentence)) %$% sentiment_by(sentence, id) %&gt;% highlight() .container { overflow: scroll !important; white-space: nowrap; max-height: 300px; } img { max-width: 1000%; } "],["geo-data.html", "Chapter 6 Geo Data 6.1 Where Are You? 6.2 Geo-coordinates 6.3 From Points And Polygons 6.4 Spatial scale", " Chapter 6 Geo Data Geospatial data, also known as geographic or spatial data, refers to information that contains explicit geographical locations or spatial attributes. Analyzing and visualizing geospatial data is essential in various fields, including geography, environmental sciences, urban planning, epidemiology, and transportation not to mention social science. 6.1 Where Are You? Sites like https://gps-coordinates.org/where-am-i.php can tell where on Earth you are at the moment. They return a map view on your current surroundings and geo-coordinates. Ask Google Maps for Viadrina European University returns two numbers 52.342977500409994, latitude and 14.555877070488613 latitude. 6.2 Geo-coordinates Latitude and longitude are geographic coordinates used to specify locations on the Earth's surface. They are used to precisely determine a point's position in terms of its north-south and east-west positions. (#fig:lon_lat)Longitude lines are perpendicular to and latitude lines are parallel to the Equator. Latitude measures the north-south position of a point on the Earth. The equator is defined as 0 degrees latitude, and it divides the Earth into the Northern Hemisphere (positive latitudes) and the Southern Hemisphere (negative latitudes). The range of latitude extends from -90 degrees (South Pole) to +90 degrees (North Pole). Longitude measures the east-west position of a point on the Earth. It is also measured in degrees, with the Prime Meridian serving as the reference point. The Prime Meridian, located at Greenwich, London, is defined as 0 degrees longitude. Longitude lines extend from the Prime Meridian to the International Date Line, which is roughly 180 degrees longitude. The range of longitude extends from -180 degrees to +180 degrees. 6.2.1 It's convention Who invented latitude and longitude? Why is latitude positive in the north? What is the north anyway? And who decided on the Prime Meridian in England? History The decision to have latitude positive in the north and negative in the south is essentially arbitrary. The convention was established to provide a consistent and universally accepted reference frame for geographic coordinates. It was likely influenced by the fact that most early civilizations and cartographers were based in the northern hemisphere. Regarding the choice of the prime meridian (0 degrees longitude) passing through Greenwich, England, it was largely due to historical reasons and the influence of the British Empire. The concept of establishing a prime meridian dates back to the 19th century when international cooperation in navigation and mapping was increasing. In 1884, at the International Meridian Conference held in Washington, D.C., representatives from various countries agreed to adopt the Greenwich Meridian as the Prime Meridian, mainly because the British Royal Observatory in Greenwich was already internationally recognized for its contributions to astronomy and navigation. 6.2.2 Different precision In general, coordinates with six decimal places (0.000001 degrees) can provide location accuracy to approximately within a few centimeters. Each additional decimal place adds further precision, narrowing down the location to smaller units of measurement. The entrance of Viadrina main building and the best coffee in town are about 40m away. Latitude is different in the 4th and longitude differs in the 3rd decimal. #&gt; name lat lon #&gt; 1 Viadrina Main Building 52.34227 14.55386 #&gt; 2 Best Coffee In Town 52.34212 14.55459 6.2.3 Different units Look again at @ref(fig:lon_lat). The intuition is that latitude and longitude are angles. Angular measurements are commonly expressed in units of degrees, minutes, and seconds (DMS). 1 degree equals 60 minutes, and one minute equals 60 seconds. Decimal degrees (DD): 41.40338, 2.17403 Degrees, minutes, and seconds (DMS): 41°24&#39;12.2&quot;N 2°10&#39;26.5&quot;E Degrees and decimal minutes (DMM): 41 24.2028, 2 10.4418 https://r-spatial.org/book/08-Plotting.html Figure 8.2: Germany in equirectangular projection: with axis units degrees (left) and metres in the equidistant cylindrical projection (right) library(sf) library(rnaturalearth) DE &lt;- st_geometry(ne_countries(country = &quot;germany&quot;, returnclass = &quot;sf&quot;)) DE |&gt; st_transform(&quot;+proj=eqc +lat_ts=51.14 +lon_0=90w&quot;) -&gt; DE.eqc par(mfrow = c(1, 2), mar = c(2.2, 2.2, 0.3, 0.5)) plot(DE, axes = TRUE) plot(DE.eqc, axes = TRUE) 6.2.4 Different perspectives # Now, lets transform Germany into a CRS optimized for Iceland ger_rep.spdf &lt;- st_transform(DE.eqc, crs = 5325) par(mfrow = c(1, 2), mar = c(2.2, 2.2, 0.3, 0.5)) plot(DE.eqc, axes = TRUE) plot(ger_rep.spdf, axes = TRUE) Test this yourself. Go to Google Maps and navigate to the middle of Iceland. Now, look how the shape of Germany changes. 6.2.5 Different distance between two coordinates # Install and load the &#39;geosphere&#39; package #install.packages(&quot;geosphere&quot;) library(geosphere) # Define the coordinates of two points coord1 &lt;- c(40.7128, -74.0060) # New York City coord2 &lt;- c(51.5074, -0.1278) # London # Calculate the distance using the &#39;distGeo&#39; function in meters dist_great_circle &lt;- distGeo(coord1, coord2) # Calculate the beeline distance using the &#39;distMeeus&#39; function dist_bee &lt;- distMeeus(coord1, coord2) dist_great_circle #&gt; [1] 8234358 dist_bee #&gt; [1] 8234367 The distGeo function from the geosphere package calculates the great circle distance between two points on the Earth's surface. The result is stored in the distance variable. Haversine. 6.3 From Points And Polygons Minimal polygon. # Define the coordinates of the polygon vertices x &lt;- c(1, 2, 3, 2) # X-coordinates of vertices y &lt;- c(1, 2, 1, 0) # Y-coordinates of vertices # Create a plotting window plot.new() plot.window(xlim = c(0, 4), ylim = c(-1, 3)) # Plot the polygon polygon(x, y) # Add labels for the vertices (optional) text(x, y, labels = 1:length(x), pos = 3) Polygon for a country. # library(raster) # Ethiopia &lt;- getData(&quot;GADM&quot;, country = &quot;Ethiopia&quot;, level = 2) # plot(Ethiopia) # points(coordinates(Ethiopia[which(Ethiopia$NAME_2 == &quot;Addis Abeba&quot;),]), # pch = 16, col = 2, cex = 2) library(geodata) Ethiopia2 &lt;- gadm(country = &quot;Ethiopia&quot;, path = tempdir(), level=1, version=&quot;latest&quot;, resolution=1) plot(Ethiopia2) 6.3.1 Shapefiles In R, shapefiles are commonly used to represent geographic data. Shapefiles are a popular geospatial vector data format that stores both geometric and attribute information about geographic features. They are widely used in geographic information system (GIS) applications and can be easily imported and manipulated in R using various packages such as sf, rgdal, or maptools. .dbf File: This file is the attribute table file associated with the shapefile. It stores the attribute data for each geographic feature in a tabular format. The attributes can include information such as names, IDs, population, or any other relevant data associated with the features. The .dbf file follows the dBase file format and can be accessed using functions like read.dbf() or read.dbf()$data in R. .prj File: This file contains the coordinate reference system (CRS) information for the shapefile. It specifies the spatial reference system and projection details, such as the coordinate units, projection method, and datum used. The CRS information is crucial for correctly interpreting and aligning the spatial data in the shapefile. In R, the CRS information can be accessed or set using functions provided by the sf package, such as st_crs() or st_set_crs(). .shp File: This file stores the actual geometric data of the shapefile. It contains information about the shape, size, and location of each geographic feature, such as points, lines, or polygons. Each feature is represented by a set of vertices or coordinates. .shx File: This file is the shapefile index file. It provides a quick lookup or index of the geometric features in the shapefile. It helps in efficiently accessing specific features without reading the entire shapefile. 6.4 Spatial scale Zoom in, zoom out, what is the best zoom level for an analysis? Inadequate spatial scales refer to situations where the chosen scale of analysis in spatial research is not suitable for capturing the underlying spatial processes or phenomena. It occurs when the spatial resolution or extent of analysis is either too coarse or too fine to effectively capture the patterns and relationships of interest. Here's more information on inadequate spatial scales, examples, and how to find an appropriate scale: Choosing an appropriate spatial scale is crucial to ensure that the analysis captures the relevant spatial patterns, relationships, and processes. It requires careful consideration of the research question, data characteristics, and prior knowledge of the phenomena under investigation. "],["switzerland.html", "Chapter 7 Switzerland", " Chapter 7 Switzerland It is not clear how the structure of the data looks like or what kind of file ending there is. Often there are several data files that make up geographic information. "],["time-data.html", "Chapter 8 Time data", " Chapter 8 Time data # download data from this https://fred.stlouisfed.org/series/USSTHPI library(quantmod) getSymbols(&quot;USSTHPI&quot;, src = &quot;FRED&quot;) #&gt; [1] &quot;USSTHPI&quot; class(USSTHPI) #&gt; [1] &quot;xts&quot; &quot;zoo&quot; plot(USSTHPI) hp &lt;- as.numeric(USSTHPI) class(hp) #&gt; [1] &quot;numeric&quot; head(USSTHPI) #&gt; USSTHPI #&gt; 1975-01-01 60.07 #&gt; 1975-04-01 61.01 #&gt; 1975-07-01 61.20 #&gt; 1975-10-01 62.24 #&gt; 1976-01-01 62.89 #&gt; 1976-04-01 65.48 head(hp) #&gt; [1] 60.07 61.01 61.20 62.24 62.89 65.48 plot(hp) "],["relationships.html", "Chapter 9 Relationships 9.1 Storks Deliver Babies 9.2 Scatterplot 9.3 Map 9.4 Variance 9.5 Standard Deviation 9.6 Covariance 9.7 Correlation", " Chapter 9 Relationships The most interesting research questions in social science are about relationships. A relationship is the way in which two or more variables or concepts are connected.14 The analysis of relationships is a fundamental approach used to understand the interplay and patterns of interactions within a social system. This chapter introduces the concepts of variance, covariance, and correlation, and how they can help us understand the connections between two variables. The specific statistical form of the analysis depends on the levels of measurement of these variables. In this example we quantify a relationship between two continuous variables that may be illustrated in a scatter plot. A correlation estimate may not represent the relationship best. Beware of spurious relationships, for they can lead us astray in our quest for understanding. Correlation can be the beginning of a more thorough research inquiry. 9.1 Storks Deliver Babies The \"stork-baby-relationship\" is a classic example used to illustrate the concept that correlation does not imply causation. People noticed a correlation between the number of storks (birds) in an area and the number of human babies born. Stork populations and birth rates seem to increase together. Figure 9.1: The first known pair in Finland (2015), representing a northward expansion compared to the species' historical breeding range. We have data for 28 countries in 2005. Storks is the number of pairs of storks in that country. The Area is in square kilometers. Population is the total population in million whereas UrbanPop is the population living in urban areas. Fertility refers to the total fertility rate, that is the average number of children that would be born to a female over their lifetime. 9.2 Scatterplot The scatterplot is a two-dimensional instrument that shows the number of storks on the x-axis and the population on the y-axis. The blue line illustrates the linear trend between the variables. Since its slope is increasing, it suggests a positive connection between storks and population, i.e. countries with more pairs of storks also tend to have a higher population. The data doesn't cluster in a nice dot cloud. Some countries have almost no storks, whereas there are thousands in other places. Countries seem to be very different in the number of storks. Let's explore this in more detail. 9.3 Map The map show the number of stork pairs for the selected countries. There seem to be some extreme values in the number of storks. In other words, the range goes from 3 pairs in Albania to 52500 in Poland. There is high variability in the number of storks. The variance is a measure for variability of data. 9.4 Variance To calculate the variance, we subtract each data point from the mean. Then square those deviations and add them up. Finally there is a scaling factor, we divide by the inverse number of observations minus 1. Definition The variance is defined as the average quadratic deviation from the mean. \\[var(x) = \\frac{1}{n-1} \\sum (x_i - \\overline{x} )^2\\] The variance of Storks and can be calculated via var() in R. var(storks$Storks) #&gt; [1] 154084389 Truly Dedicated When you have collected data from every member of the population that you’re interested in, you can get an exact value for population variance. When you collect data from a sample, the sample variance is used to make estimates or inferences about the population variance. Sample variance is divided by \\(n-1\\). Population variance is divided by \\(n\\). 9.5 Standard Deviation The variance is in squared unit and thus hard to interpret. The standard deviation is derived from variance and tells, on average, how far each value lies from the mean. It’s the square root of variance and calculated via sd(). Variance and standard deviation both measure the variability of a variable. sd(storks$Storks) #&gt; [1] 12413.07 The one-dimensional variable Storks is visualized as points on a line. The mean number of storks is 7718.3571429 (red bold line). The standard variation is 1.2413073^{4} and surrounds the mean. Truly Dedicated In statistics, the empirical rule states that 68% of the observed data will occur within the first standard deviation, 95% will take place in the second deviation and 99.7% within the third standard deviation of the mean within a normal distribution. See 68–95–99.7 rule The following scatterplot shows the number of storks and the population with their respective means as bold red lines. Now, that we know how to describe the variation of each of the two variables, we look for a measure that reflects the co-variation of both variables, i.e. how they change in relation to each other. The new grid of means is a good starting point. When most data points fall into lower left and upper right quadrant, we call this a positive relationship. 9.6 Covariance Covariance is a measure of the joint variability of two variables. The main idea of covariance is to classify three types of relationships: positive, negative or no relationship. Definition The covariance between two variables is the product of the deviations of x and y from their respective means. \\[cov(x,y) = \\frac{1}{n-1} \\sum\\limits (x_i - \\bar{x})(y_i - \\bar{y})\\] For each data point, we multiply the differences with the respective mean. This results in several rectangular areas starting at the intersection of means as a new origin. The covariance sums up all these areas. Finally, the covariance is adjusted by the number of observations. When both values are smaller or greater than the mean, the result will be positive. In R, the covariance is calculated by cov(): cov(x,y) #&gt; [1] 89235.04 The covariance confirms what we saw in the first scatterplot, the relationship between storks and population is positive. Let's try to visualize the calculation procedure of the covariance. Your Turn Can you validate the covariance result from cov(x,y) from the sum of squares from the figure? Covariance qualifies a relationship as positive or negative, i.e. the direction of the relationship. Covariance is expressed in units that vary with the data. Because the data are not standardized, you cannot use the covariance statistic to assess the strength of a linear relationship (a covariance of 117.5 can be very low for one relationship and 0.23 very high for another relationship). Therefore, we need one more measure that is similar and related to the covariance, the correlation coefficient. 9.7 Correlation To assess the strength of a relationship between two variables a correlation coefficient is commonly used. It brings variation to a standardized scale between -1 to +1. Definition The correlation coefficient is a statistical measure of the strength and direction of the relationship between two variables. \\[r(x,y) = \\frac{\\sum\\limits (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits (x_i - \\bar{x})^2 \\sum\\limits (y_i - \\bar{y})^2}}\\] Does the numerator and denominator remind you of something? The formula is made of the components variance and covariance. Thus, the correlation coefficient formula is often expressed in short as: \\[r(x,y,) = \\frac{Cov(x,y)}{\\sqrt{Var(x) Var(y)}}\\] cor() is a basic function to calculate the correlation coefficient. # Basic function cor(x,y) #&gt; [1] 0.2199176 The correlation coefficients confirms ones more, there is a positive relationship. You will find thresholds for different fields of research that classify the magnitude of the correlation coefficient as weak, moderate and strong. Social science usually accept lower correlation values to be meaningful. One possible classification could be: above 0.4 is strong between 0.2 and 0.4 is moderate, and those below 0.2 are considered weak. Thus we may consider the stork-population-relationship as weak to moderate. Keep in mind that these thresholds are not set in stone. Now, let's turn to cor.test(), a more sophisticated version including a hypothesis test. # Advanced function cor.test(x,y) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: x and y #&gt; t = 1.1495, df = 26, p-value = 0.2608 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.1668487 0.5480307 #&gt; sample estimates: #&gt; cor #&gt; 0.2199176 The correlation test is based on a t-value (t = 1.1495056) and returns a p-value (0.2608121) for statistical significance. Definition The p value is a statistical measure to determine whether the results of a statistical analysis are statistically significant or if they could have occurred due to random chance. Small p-values below 0.05 are usually considered to be statistically significant. This is not the case for our stork-population relationship. There is an awesome connection from correlation coefficient to the simple regression coefficient. Amazing Fact The correlation coefficient and the simple regression coefficient coincide when the two variables are on the same scale. The most common way of achieving this is through standardization. \\[\\beta = cor(Y,X) \\cdot \\frac{SD(Y)}{SD(X)} \\] Here is the replication: # The data df = data.frame(x=c(4,13,19,25,29), y=c(10,12,28,32,38)) # The correlation coefficient cor_coef &lt;- cor(df$x, df$y, method=&quot;pearson&quot;) cor_coef #&gt; [1] 0.9564548 The correlation coefficient is 1.1495056. # The regression coefficient linear_model &lt;- lm(y~x, data=df) reg_coef &lt;- linear_model$coefficients[2] reg_coef #&gt; x #&gt; 1.19898 The regression coefficient of x is 1.1989796. Here is the connection: # The connection cor_coef * sd(y) / sd(x) #&gt; [1] 0.002518728 Alternatively standardize the data first, then calculate correlation and regression: # The connection df_scaled &lt;- as.data.frame(scale(df, center = TRUE, scale = TRUE)) # The correlation on standardized variables cor(df_scaled$x, df_scaled$y, method=&quot;pearson&quot;) #&gt; [1] 0.9564548 # The regression on standardized variables lm(y~x, data=df_scaled)$coefficients[2] #&gt; x #&gt; 0.9564548 Everything that can be measured (see chapter Can we measure everything?).↩︎ "],["regression.html", "Chapter 10 Regression 10.1 Old but Gold 10.2 Data is everywhere 10.3 For the truly dedicated 10.4 Survival of the Fittest Line 10.5 On the Shoulders of Giants", " Chapter 10 Regression This chapter introduces the workhorse of empirical research in the social science: Regression. “As an undergraduate I studied economics, which meant I studied a lot of regressions. It was basically 90% of the curriculum (when we’re not discussing supply and demand curves, of course). The effect of corruption on sumo wrestling? Regression. Effect of minimum wage changes on a Wendy’s in NJ? Regression. Or maybe The Zombie Lawyer Apocalypse is more your speed (O.K., not a regression, but the title was cool).” -- Amanda West (2020) -- A Beginner’s Guide to Instrumental Variables 10.1 Old but Gold Are older artists are better than younger artists? While experience and maturity can certainly contribute to an artist's skill and creativity, there are many factors that can influence the quality of an artist's work, such as natural talent, dedication, training, and access to resources. Is there an optimum age for artistic performance as compared to athletic performances which reaches a peak in youth? Do artists improve their skills and performance over an entire lifetime step-by-step such that the longer you live, the more you have time to practice? Does exceptional art happens randomly? Perhaps it takes time to become more well-known. You need time to travel and show or sell your art in different places. Thus when you produce \"more art\" you increase the chance to be discovered by the public or a patron? Have you ever heard of an artist who exactly created one piece of art? There seems to be something to the story. \"Paul Cezanne died in October 1906 at the age of 67. In time he would be generally regarded as the most influential painter who had worked in the nineteenth century (e.g., Clive Bell, 1982; Clement Greenberg, 1993). Art historians and critics would also agree that his greatest achievement was the work he did late in his life.\" -- Galenson, D. W., &amp; Weinberg, B. A. (2001). Creating modern art: The changing careers of painters in France from impressionism to cubism. American Economic Review, 91(4), 1063-1071. What does better mean? In this chapter the research question is: What is the relationship between the age of an artist and his productivity? Definition A research question is focused on a single issue, specific enough to answer thoroughly and feasible to answer within the given time frame or practical constraints not to mention relevant to your field of study. But what exactly is productivity and how can we measure it? To keep things simple we follow the literature and measure productivity via auction prices for paintings. That's a very economic perspective on art. Definition Operationalization is the process of defining the measurement of a phenomenon that is not directly measurable. This is what we gonna explore: What is the relationship between the age of artists and auction price for their paintings? 10.2 Data is everywhere Researchers use auction price data for which they have to pay. We use free information from a Wikipedia List of most expensive paintings of all time. The auction prices are inflation-adjusted by consumer price index in millions of United States dollars in 2019. That's another interesting economic procedure, that we take for given at this analysis. 10.2.1 Data in a table The table is created with the DT package in datatable format. This exploits the full potential of html documents, i.e. the data is searchable and sortable. The first rows are displayed, but in principle it can include the entire dataset. Definition Tabular data is common in data analysis. You can create a table in Word or Excel. 10.2.2 Data in a graph Two continuous variables are plotted in a scatterplot. The x-axis is called abscissa and the y-axis is called ordinate. Note that the axis beginning is not zero. The decision where axes start was made by the ggplot package for this data. Remember, every unit on the y-axis represents a million US dollars. Do we need to show the age between 0 and 20? How many famous artists died before 20 and sold paintings for a hundred million US dollars? When It’s OK to NOT Start Your Axis at Zero. When the data really don’t fluctuate very much but a rise of small values like 1.4 or 1.4% is a big deal. With a graph that starts at zero, these changes can't be detected. The data scientist has to decide. 10.2.3 The trend The graph suggests a positive trend between price and age. There is an increase in price for older artists. The older the artist, the higher the auction prices. 10.2.4 The blackbox The mission is to find a mathematical function that describes the trend. In other words, we are looking for the black box that transforms the input into the output: Definition A mathematical function is an expression, rule, or law that defines a relationship between one variable (the independent variable, on the x-axis) and another variable (the dependent variable, on the y-axis). From looking at the graph, here are two suggestions: \\[\\begin{align} \\text{price} = 80 + 0.5 \\cdot \\text{age} \\tag{Suggestion 1} \\\\ \\text{price} = 90 + 0.2 \\cdot \\text{age} \\tag{Suggestion 2} \\\\ \\end{align}\\] Definition A linear function is defined by two components, intercept (with the y-axis) and slope. How can we compare the two suggested lines? Which linear function represents the relationship best? 10.2.5 Nobody's perfect We all make mistakes. So do the linear functions: \\[ \\begin{align} \\text{price} &amp;= 80 + 0.5 \\cdot \\text{age} \\tag{Suggestion 1} \\\\ 102.5 &amp;= 80 + 0.5 \\cdot 45 \\tag{Age for Holbein} \\\\ \\end{align}\\] The equation tells (or predicts) that for any artist at the age of 45 it expects a auction price for a painting of 102.5 million US Dollar. Darmstadt Madonna was sold for 85 million US dollar. The linear function overestimated the true value. When you look at the graph, you see some predictions are more accurate (close to the true values) than others. All are either above or below the line. Definition A residual (or error) is the vertical distance between the actual and the predicted value. 10.2.6 Vocab Wrap-Up Let's wrap up regression vocab! Find an equation that describes the phenomenon of interest. Equation I shows a generic statistical model; equation II a generic linear model. \\[ \\begin{align} \\text{outcome} &amp;= f(\\text{explanatory}) + \\text{noise} \\tag{I} \\\\ \\text{outcome} &amp;= \\text{intercept} + \\text{slope} \\cdot \\text{explanatory} + \\text{noise} \\tag{II} \\\\ \\end{align} \\] A regression model is suggested by the researcher. A more concrete regression model looks like this: \\[Y = \\beta_1 + \\beta_2 X + \\epsilon\\] A model can be easy or complicated. It definitely contains variables and parameters. Variables: Things that we measure (or have data). Parameters: Constant values we believe to represent some fundamental truth about the relationship between the variables. The calculation is called an estimation. In textbooks the same equation can be found with hats: \\[ \\widehat{Y} = \\widehat{\\beta}_1 + \\widehat{\\beta}_2 \\cdot X \\] \\(\\widehat{Y}\\) are called the fitted or predicted values. \\(\\widehat{\\beta}\\) are regression coefficients (this is the estimate of the unknown population parameter). As we have seen in the graph before, the differences between the actual and the predicted values are the residuals \\(e = Y - \\widehat{Y}\\). The fitting procedure is called ordinary least squares (OLS). 10.3 For the truly dedicated The overall goal is to make as little as possible mistakes! What kind of mistake? The deviation from the observed values! What could come to your mind is to minimize the sum of all errors: \\[\\sum e \\rightarrow \\min\\] But wait, there is more. Is it fair to say that the sum should be small? Compare The Scream and Meules, their deviations are \\(+17.5\\) and \\(-13.6\\) (very similar). So taken these two together, there's almost not mistake! That is to say, positive and negative deviations cancel each other out. Thus we need one more twist in the story: \\[\\sum e^2 \\rightarrow \\min\\] The goal of OLS is to minimize the residual sum of squares or the sum of squared residuals. 10.3.1 Algebra Amazing Fact Algebra comes from Arabic, meaning \"reunion of broken parts\". Let's introduce matrix notation. We began with \\(X\\) and \\(Y\\) being variables in the equation: \\[Y = \\beta_0 + \\beta_1 X + \\epsilon \\] We turn this into: \\[y = X \\beta + \\epsilon \\] Capital letters like \\(X\\) represent a matrix (a table with rows and column), and small letters like \\(y\\) and \\(e\\) represent vectors. Since there are 6 observations and two parameters in our model we get: \\[ \\begin{align} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_5 \\\\ Y_6 \\end{pmatrix} &amp;= \\begin{pmatrix} 1 &amp; X_{11} \\\\ 1 &amp; X_{12} \\\\ 1 &amp; X_{13} \\\\ 1 &amp; X_{14} \\\\ 1 &amp; X_{15} \\\\ 1 &amp; X_{16} \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\\\ \\epsilon_6 \\\\ \\end{pmatrix} \\end{align}\\] Let's turn to the goal, minizing the residual sum of squares (RSS). By convention, the normal version of a vector is a vertical list of numbers in big parentheses (i.e. a column vector). To transpose a vector means change between the row and column format. Squaring a vector thus means the row version of the vector times the column version of the vector: \\[\\sum e^2 = e^T \\cdot e \\rightarrow \\min\\] Notice that the sum operator is gone. Matrix multiplication requires multiplying all elements pairwise with each other and summing them up. Plug in the residuals \\(e = y - X \\beta\\) in the equation: \\[\\begin{align} \\sum e^2 &amp;= e^T \\cdot e \\\\ &amp;= (y - X \\beta )^T (y - X \\beta) \\tag{$(A+B)^T = A^T + B^T$}\\\\ &amp;= (y^T - X^T \\beta^T) (y - X \\beta) \\\\ &amp;= y^T y - y^T X \\beta - X^T \\beta^T y + X^T \\beta^T X \\beta \\\\ &amp;= y^2 \\underbrace{- 2 \\beta^T X^T y}_{??} + \\beta^2 X^2 \\\\ \\end{align}\\] Did you notice what happened in the middle? The transpose of the first term is equal to the second: \\[\\begin{align} (y^T X \\beta)^T = y X^T \\beta^T \\end{align}\\] 10.3.2 Analysis Amazing Fact From Medieval Latin, analysis means \"resolution of anything complex into simple elements\" (opposite of synthesis). Next, we are ready to optimize. Optimization (in math and economics) is done by differentiation: \\[\\begin{align} \\frac{\\partial RSS}{\\partial \\beta} &amp;= -2 X^T y + 2 \\beta X^T X = 0 \\tag{first derivative = zero} \\\\ 2 \\beta X^T X &amp;= 2 X^T y \\tag{rearrange terms}\\\\ \\beta X^T X &amp;= X^T y \\tag{the &quot;normal equation&quot;} \\\\ \\beta &amp;= (X^T X)^{-1} X^T y \\tag{Bam}\\\\ \\end{align}\\] Those \\(\\beta\\) coefficients are the first and most important regression results. Retrieve them step by step to enhance your understanding of the math and coding as the same time. 10.3.3 Take the Long Way Home First, retrieve matrix \\(X\\) from the data set: X #&gt; [,1] [,2] #&gt; [1,] 1 91 #&gt; [2,] 1 86 #&gt; [3,] 1 67 #&gt; [4,] 1 45 #&gt; [5,] 1 80 #&gt; [6,] 1 71 Second, the transpose of \\(X\\) has two rows and six columns (use t()): t(X) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] 1 1 1 1 1 1 #&gt; [2,] 91 86 67 45 80 71 Next, calculate the square of the matrix (transpose times original): t(X)%*%X #&gt; [,1] [,2] #&gt; [1,] 6 440 #&gt; [2,] 440 33632 The inverse of the matrix product can be calculated by solve(): solve(t(X)%*%X) #&gt; [,1] [,2] #&gt; [1,] 4.10546875 -0.0537109375 #&gt; [2,] -0.05371094 0.0007324219 Next, multiply the inverse with the transpose from the right: solve(t(X)%*%X) %*% t(X) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] -0.78222656 -0.513671875 0.506835938 1.68847656 -0.191406250 #&gt; [2,] 0.01293945 0.009277344 -0.004638672 -0.02075195 0.004882813 #&gt; [,6] #&gt; [1,] 0.291992188 #&gt; [2,] -0.001708984 Finally, multiply the vector \\(y\\): solve(t(X)%*%X) %*% t(X) %*% y #&gt; [,1] #&gt; [1,] 22.345215 #&gt; [2,] 1.165747 It's the \\(\\beta\\) vector! The first entry is the intercept and the second is the slope of the linear function. The following graph shows the line created from intercept and slope in a scatter plot. 10.4 Survival of the Fittest Line The linear equation that best describes the data is this: \\[Price = 22.3452 + 1.1657 \\cdot Age\\] 10.5 On the Shoulders of Giants Fortunately, we are standing on the shoulders of giants. Clever people implemented the linear regression (command lm()) and all kinds of regressions and statistical tests in R. lm(Price ~ Age.at.Death, data = artists) #&gt; #&gt; Call: #&gt; lm(formula = Price ~ Age.at.Death, data = artists) #&gt; #&gt; Coefficients: #&gt; (Intercept) Age.at.Death #&gt; 22.345 1.166 The workhorse packs up work. "],["linear-regression.html", "Chapter 11 Linear Regression 11.1 What You Deserve Is What You Get 11.2 Data &amp; Sample 11.3 Data Visualization 11.4 Simplest Regression 11.5 Simple Regression 11.6 Parallel Slopes 11.7 Model Comparison 11.8 Transform to Perform", " Chapter 11 Linear Regression 11.1 What You Deserve Is What You Get Do you believe that people get what they deserve? The statement \"what you deserve is what you get\" is a controversial one. It can be interpreted such that individuals are entirely responsible for their own outcomes and that they receive exactly what they deserve based on their efforts, abilities, and choices. This view assumes a meritocratic (performance-oriented) system where everyone has an equal opportunity to succeed based on their merit, and rewards are distributed accordingly. We investigate annual income and how it is determined. The famous Mincer Equation is single-equation model that explains wage income as a function of schooling and experience. The equation suggests that higher levels of education and experience are positively associated with earnings, and the coefficients can be estimated using statistical methods to quantify the magnitude of these relationships. When everybody has access to schooling and the equal opportunities on the labor market, better school outcomes and work experience may be determined by people efforts, abilities, and choices. 11.2 Data &amp; Sample We use SOEP practice data to analyse yearly income. The analysis is restricted to people who are fulltime employed (Emp == 1) in the working age (Age &lt;= 65) who report an annual income from main job of more than one Euro (Income &gt; 1). We analyse the most recent cross-section of the data (syear == 2019). We drop a few cases with missing information, thus conduct a complete case analysis. library(haven) master &lt;- read_dta(&quot;https://github.com/MarcoKuehne/marcokuehne.github.io/blob/main/data/SOEP/practice_en/practice_dataset_eng.dta?raw=true&quot;) # The data comes with Stata labels that do not work with all tidyverse commands library(sjlabelled) soep &lt;- remove_all_labels(master) # Rename German to English variable names library(tidyverse) soep &lt;- soep %&gt;% rename(&quot;Age&quot; = &quot;alter&quot;, &quot;Income&quot; = &quot;einkommenj1&quot;, &quot;NACE2&quot; = &quot;branche&quot;, &quot;Persons in HH&quot; = &quot;anz_pers&quot;, &quot;Kids&quot; = &quot;anz_kind&quot;, &quot;Education&quot; = &quot;bildung&quot;, &quot;Health&quot; = &quot;gesund_org&quot;, &quot;Satisfaction&quot; = &quot;lebensz_org&quot;, &quot;Emp&quot; = &quot;erwerb&quot;) # Explicitly define the gender variable as a factor soep &lt;- soep %&gt;% mutate(Female = factor(sex)) # Round annual income to two digits soep &lt;- soep %&gt;% mutate(Income = round(Income, 2)) # Build the estimation sample based on the topic soep &lt;- soep %&gt;% filter(Emp == 1) %&gt;% filter(Age &lt;= 65) %&gt;% filter(Income &gt; 1) %&gt;% filter(syear == 2019) # Conduct a complete case analysis soep &lt;- soep %&gt;% filter(complete.cases(.)) The estimation sample looks like this: DT::datatable(soep[,-c(1:3,8,13:15)], rownames = FALSE) The descriptive statistics of the sample look like this: library(modelsummary) datasummary_skim(soep[,-c(1,2,8,13:15)]) Unique (#) Missing (%) Mean SD Min Median Max sex 2 0 0.4 0.5 0.0 0.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Age 47 0 45.4 11.2 18.0 47.0 65.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Persons in HH 10 0 2.9 1.4 1.0 3.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Kids 8 0 0.7 1.1 0.0 0.0 8.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Education 16 0 12.9 2.8 7.0 12.0 18.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } NACE2 77 0 57.0 25.9 1.0 56.0 97.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Health 5 0 2.4 0.9 1.0 2.0 5.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Satisfaction 11 0 7.6 1.4 0.0 8.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Income 1211 0 42145.2 23488.8 915.5 37337.3 257886.2 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } The data is clean and ready for analysis. There are no missings. The working age ranges from 18 to 65 years. People have between 0 and 8 children. There are NACE codes for the job industry ranging from 1 to 97 in the data (theoretically from 1 to 99). But not all jobs are represented in the data (77 unique values). Health status is measured on a scale from 1 to 5 whereas life satisfaction is measured on a scale between 0 and 10. The minimum annual income is about 915 Euro. We could have restricted the income variable to the minimum wage in Germany. Assume it is 12€/h and people in full-time work 8h/day. Starting from 365 days a year, there are 104 weekends, assume there are 11 public holidays and 30 days of vacation. 220 working days remain earning a minimum yearly income of about 220*8*12 = 21120 Euro. In the data, 165 people report earning less than that. 11.3 Data Visualization We create a scatterplot of income (on the y-axis) versus age (on the x-axis). Data points are colored by gender (blue for men, red for women). A linear regression line is added per gende.r 11.4 Simplest Regression The simplest regression or empty model does not contain any explanatory variable. simplest &lt;- lm(Income ~ 1, data=soep) library(modelsummary) modelsummary(title = &#39;Empty Model.&#39;, list(&quot;Income&quot; = simplest), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, coef_map = cm) Table 11.1: Empty Model.  Income Constant 42145.215 (674.976) Num.Obs. 1211 The graphs below illustrate the empty model that estimates an average annual income. The left panel shows the mean income for the entire sample (sometimes referred to as grand mean), the right panel shows the data colored by gender (it can be considered two overlie plots). 11.5 Simple Regression Simple regression suggest a one-to-one relationship between two variables. In this section we focus on the continuous outcome variable income. We relate income to three different variables, age, gender and industry. Where age is another continuous variable, gender is a binary dummy and industry is a categorical variable. 11.5.1 X is continuous Income and age are continuous in principle, i.e. they can be any real number in an interval. Actually, age is reported in natural numbers, full years due to measurement restrictions. In the real SOEP data, a more precise estimate of age can be derived as a date difference between date of birth and date of interview. Thus creating a measure with day units. In practice, a scale from 0 to 10 is often treated as continuous. Although technically speaking a categorical or ordinal approach would match the nature of the measure better. simple1 &lt;- lm(Income ~ Age, data=soep) modelsummary(title = &#39;Continuous Predictor.&#39;, list(&quot;Income&quot; = simple1), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, coef_map = cm) Table 11.2: Continuous Predictor.  Income Age 524.554 (58.487) Constant 18341.031 (2733.469) Num.Obs. 1211 F 80.439 11.5.2 X is a dummy A dummy or binary variable describes two groups. In the SOEP case the gender variable is named Female and coded 0 for male and 1 for female. In practice, it is likely to ease interpretation by renaming the variable to female, where 1 stands for having the feature and 0 for not having it. Dummies also come with different values like 1 and 2. Such a variable could be recoded to 0 and 1. Last but not least, note that SOEP and other studies still have a binary perspective on gender. We can imagine more gender groups, making it a categorical variable. There might be something like a continuous gender scale as well.15 simple2 &lt;- lm(Income ~ Female, data=soep) modelsummary(title = &#39;Dummy Predictor.&#39;, list(&quot;Income&quot; = simple2), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, coef_map = cm) Table 11.3: Dummy Predictor.  Income Female -9401.118 (1383.445) Constant 45498.875 (826.288) Num.Obs. 1211 F 46.178 11.5.3 X is categorical We investigate annual income across different industries. NACE is the acronym used to designate the various statistical classifications of economic activities developed since 1970 in the European Union (EU). NACE provides the framework for collecting and presenting a large range of statistical data according to economic activity in the fields of economic statistics (e.g. business statistics, labour market, national accounts) and in other statistical domains. In level 2 of NACE there are 88 divisions identified by two-digit numerical codes (01 to 99). In the following we built a categorical variable industry from the level 2 NACE information in SOEP: soep &lt;- soep %&gt;% mutate(industry = case_when(NACE2 %in% c(1,2,3) ~ &quot;Agriculture&quot;, NACE2 %in% c(5:9) ~ &quot;Mining&quot;, NACE2 %in% c(10:32) ~ &quot;Manufacturing&quot;, NACE2 %in% c(35:38) ~ &quot;Energy&quot;, NACE2 %in% c(41:43) ~ &quot;Construction&quot;, NACE2 %in% c(50,51,52,55) ~ &quot;Trade&quot;, NACE2 %in% c(60,61,62,63,64) ~ &quot;Transport&quot;, NACE2 %in% c(65,66,67) ~ &quot;Banking&quot;, NACE2 %in% c(70,71,72,73,74,75,80,85,90,91,92,93,95,98,99) ~ &quot;Services&quot;, TRUE ~ &quot;Other&quot;)) table(soep$industry) #&gt; #&gt; Agriculture Banking Construction Energy Manufacturing #&gt; 14 9 80 26 299 #&gt; Mining Other Services Trade Transport #&gt; 3 539 134 34 73 simple3 &lt;- lm(Income ~ industry, data=soep) modelsummary(title = &#39;Categorical Predictor.&#39;, list(&quot;Income&quot; = simple3), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, coef_map = cm) Table 11.4: Categorical Predictor.  Income Banking 46123.264 (9569.929) Construction 8057.920 (6489.108) Energy 22774.586 (7425.221) Manufacturing 22032.617 (6124.951) Mining 31592.741 (14250.493) Transport 36264.336 (6535.281) Trade 11947.600 (7112.908) Services 25698.418 (6291.360) Other 14521.498 (6063.652) Constant 23434.559 (5986.405) Num.Obs. 1211 F 14.399 Note that with a categorical variable, one category is missing in the output by default (the so called base level). In this case it is Agriculture. All coefficients are in comparison with the average annual income in Agriculture. All coefficients are positive, indicating that all industries earn more than the Agriculture sector. 11.5.4 X is categorical, is it? Actually, categorical variables are split into multiple dummy variable during the estimation process. Truly Dedicated We code the equivalence between linear regression with a categorical variable coded as a factor in R and a categorical variable split into multiple dummy variables. # All variables are internally coded as &quot;double&quot; glimpse(mtcars) #&gt; Rows: 32 #&gt; Columns: 11 #&gt; $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,… #&gt; $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,… #&gt; $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16… #&gt; $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180… #&gt; $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,… #&gt; $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.… #&gt; $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18… #&gt; $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,… #&gt; $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… #&gt; $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,… #&gt; $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,… # Cars have either 4, 6 or 8 cylinders table(mtcars$cyl) #&gt; #&gt; 4 6 8 #&gt; 11 7 14 There are # Factor coding: Use built in R style mtcars &lt;- mtcars %&gt;% mutate(cyl_fct = as_factor(cyl)) # Creating a dummy for each group mtcars &lt;- mtcars %&gt;% mutate(cyl4 = ifelse(cyl == 4, 1, 0), cyl6 = ifelse(cyl == 6, 1, 0), cyl8 = ifelse(cyl == 8, 1, 0)) # One hot encoding: every level of factor/categorical get its own column/dummy # Dummy coding: there are k-1 dummies, because one is redundant The following types of regression are possible. First, treat cyl like a continuous variable. This might or might not make sense. R assumes, there are cars with all values of cylinders. # Treated like continuous model1 &lt;- lm(mpg ~ cyl, data=mtcars) # Treated like categorical (not showing base category) model2 &lt;- lm(mpg ~ cyl_fct, data=mtcars) # One hot coding (all included, but one is NA) model3 &lt;- lm(mpg ~ cyl4 + cyl6 + cyl8, data=mtcars) # Dummy coding (same coefficients, no NA) model4 &lt;- lm(mpg ~ cyl4 + cyl6, data=mtcars) modelsummary(title = &#39;Categorical Predictor.&#39;, list(&quot;MPG&quot; = model1, &quot;MPG&quot; = model2, &quot;MPG&quot; = model3, &quot;MPG&quot; = model4), statistic = NULL, gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;) Table 11.5: Categorical Predictor. MPG MPG MPG MPG (Intercept) 37.885 26.664 15.100 15.100 cyl -2.876 cyl_fct6 -6.921 cyl_fct8 -11.564 cyl4 11.564 11.564 cyl6 4.643 4.643 Num.Obs. 32 32 32 32 # Connection # cylfct6 is -6.9 less than invisible base category lm(mpg ~ cyl_fct, data=mtcars) #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ cyl_fct, data = mtcars) #&gt; #&gt; Coefficients: #&gt; (Intercept) cyl_fct6 cyl_fct8 #&gt; 26.664 -6.921 -11.564 # recalculate this as coefficient of cyl6 - cyl4 lm(mpg ~ cyl4 + cyl6, data=mtcars)$coefficients[3]-lm(mpg ~ cyl4 + cyl6, data=mtcars)$coefficients[2] #&gt; cyl6 #&gt; -6.920779 11.6 Parallel Slopes Parallel Slopes is a special case of a multiple regression where there are multiple input variables that describe or explain an outcome. In particular, one of the variables is continuous (in this case age) and the other is a dummy variable (in this case gender). The resulting graph shows two parallel linear regression lines shifted by the dummy variable. 11.6.1 X is continuous + dummy In contrast to the first Figure, we know consider all data at once and recognize gender as a factor influencing the annual income. The dummy variable gender offsets the age-income relationship. parallel1 &lt;- lm(Income ~ Age + Female, data=soep) modelsummary(title = &#39;Categorical Predictor.&#39;, list(&quot;Income&quot; = parallel1), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, coef_map = cm) Table 11.6: Categorical Predictor.  Income Age 501.348 (57.643) Female -8670.282 (1345.244) Constant 22487.063 (2764.643) Num.Obs. 1211 F 62.338 11.7 Model Comparison multiple &lt;- lm(Income ~ Age + Female + industry, data=soep) modelsummary(title = &#39;Linear regression models compared.&#39;, list(&quot;Income&quot; = simplest, &quot;Income&quot; = simple1, &quot;Income&quot; = simple2, &quot;Income&quot; = simple3, &quot;Income&quot; = parallel1, &quot;Income&quot; = multiple), gof_omit = &#39;RMSE|Log.Lik.|F|AIC|BIC&#39;, metrics = &quot;all&quot;, coef_map = cm) Table 11.7: Linear regression models compared.  Income  Income  Income  Income  Income  Income Age 524.554 501.348 478.369 (58.487) (57.643) (54.886) Female -9401.118 -8670.282 -9164.068 (1383.445) (1345.244) (1323.594) Banking 46123.264 46653.149 (9569.929) (9090.316) Construction 8057.920 8349.413 (6489.108) (6164.261) Energy 22774.586 23587.586 (7425.221) (7053.065) Manufacturing 22032.617 23113.979 (6124.951) (5819.116) Mining 31592.741 31652.597 (14250.493) (13539.296) Transport 36264.336 37942.568 (6535.281) (6210.851) Trade 11947.600 14909.762 (7112.908) (6761.498) Services 25698.418 28857.012 (6291.360) (5993.555) Other 14521.498 17854.146 (6063.652) (5773.785) Constant 42145.215 18341.031 45498.875 23434.559 22487.063 2670.410 (674.976) (2733.469) (826.288) (5986.405) (2764.643) (6232.343) Num.Obs. 1211 1211 1211 1211 1211 1211 R2 0.000 0.062 0.037 0.097 0.094 0.187 R2 Adj. 0.000 0.062 0.036 0.091 0.092 0.180 Definition R square (R2) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a regression model. It is also known as the coefficient of determination. R2 measures how well the regression model fits the data. R2 can range from 0 to 1, where 0 indicates that none of the variance in the dependent variable is explained by the independent variable(s), and 1 indicates that all of the variance in the dependent variable is explained by the independent variable(s). A higher R2 indicates a better fit between the model and the data, meaning that more of the variability in the dependent variable can be explained by the independent variable(s) in the model. The formula for R2 is: \\[R2 = 1 - \\frac{SSR}{SST} = 1 - \\frac{\\text{sum of squares residuals}}{\\text{total sum of squares}} = 1 - \\frac{\\displaystyle\\sum \\left(\\hat{y}- \\overline{y}\\right)^2}{\\displaystyle\\sum \\left(y - \\overline{y}\\right)^2} \\] A good R2 – In some fields like social sciences or economics, an R2 value of 0.3 or higher may be considered a good fit for a model. In other fields like physics or engineering, a higher R2 value of 0.7 or above may be necessary to demonstrate a good fit. R2 inflation – R2 usually increases with sample size. In least squares regression using typical data, R2 is at least weakly increasing with increases in the number of regressors in the model. Because increases in the number of regressors increase the value of R2, R2 alone cannot be used as a meaningful comparison of models with very different numbers of independent variables. Definition Adjusted R2 is a modified version of R2 that takes into account the number of independent variables in a regression model. \\[\\text{Adjusted R2} = 1 - \\left[ (1 - R2) \\cdot \\frac{n - 1}{n - k} \\right] \\tag{with intercept}\\] \\[\\text{Adjusted R2} = 1 - \\left[ (1 - R2) \\cdot \\frac{n - 1}{n - k - 1} \\right] \\tag{no intercept}\\] 11.8 Transform to Perform While a lot of statistics and econometrics deals with linear relationships, we live in a very non-linear world. By taking the logarithm of your data, you can reduce the range of values and make it easier to see patterns and relationships. Log transformation is a common technique to deal with skewed or highly variable data, such as income. Income data often have a long right tail, meaning there are few individuals with very high incomes, but many with lower incomes (left panel). When modeling income in a regression, this can cause issues because the distribution violates the normality assumption that underpins many regression techniques. Remember some key points of log: The log can only be calculated for numbers &gt; 0. The log of values between 0 and 1 is negative. The log of values above 1 is positive. The log of 1 is 0. How to do logs in R? # Logs in R x &lt;- c(0.1,0.5,1,2,5) x_log &lt;- log(x) data.frame(x, x_log) #&gt; x x_log #&gt; 1 0.1 -2.3025851 #&gt; 2 0.5 -0.6931472 #&gt; 3 1.0 0.0000000 #&gt; 4 2.0 0.6931472 #&gt; 5 5.0 1.6094379 It's natural. The default of log() is the natural logarithm. There are other logs as well, e.g. with base 10 log(x, base = exp(10)). The above specification of income versus age as two non-transformed variables is also known as a linear-linear model or level-level model. You can have a log transformation on the left, the right or both sides. All log-transformations change the interpretation of the regression coefficient. We explore these options in the following. The log-linear or log-level model. log_linear &lt;- lm(log(Income) ~ Age + Female + industry, data=soep) log_log &lt;- lm(log(Income) ~ log(Age) + Female + industry, data=soep) linear_log &lt;- lm(Income ~ log(Age) + Female + industry, data=soep) Table 11.8: Linear regression with log transformations.  Income  Log (Income)  Log (Income)  Income Age 478.369 0.014 (54.886) (0.001) Log (Age) 0.624 20595.612 (0.058) (2226.835) Num.Obs. 1211 1211 1211 1211 R2 0.187 0.177 0.189 0.193 R2 Adj. 0.180 0.169 0.182 0.186 Regression coefficients interpretation. Here are the interpretations: Linear-Linear: For each additional year of age, we expect 478 € more annual income on average. Easy peasy. Log-Linear (semi-elasticity): For each additional year of age, we expect annual income to increase by \\((e^{0.014} - 1) * 100 = 1.4\\) % on average. Starting from the average annual income of 42145 € one more year in age increases income by 590 €. Log-Log (elasticity): For each additional 1% increase in age, we expect the annual income to increase by 0.62 % €. For a 30-year-old earning average annual income, a 1% increase in age means 0.3 years or 3.6 months, increasing annual income by 261 €. Linear-Log: For each additional 1% increase in age, we expect the annual income to increase by 206 € For a 30-year-old, additional 1% or 3.6 month are expected to increase annual income by 206 €. Of course other mathematical transformations are possible and some are common, for example the square root or polynomial transformation (the power of two or three). Stutzer and Frey (2008) study the effect of commuting time (and commuting time squared) on life satisfaction (Tables 1 to 4). The same consideration is so handedness. Normally, people are classified a left-hander or right-hander. But you can make a case for ambidextrous people using both hands equally good as well as a mixture of hand usage, making handedness a continuous concept.↩︎ "],["interaction-models.html", "Chapter 12 Interaction Models 12.1 Motivation 12.2 Data &amp; Sample 12.3 Throwback Parallel Slopes 12.4 Regression with Moderators 12.5 Model Comparison", " Chapter 12 Interaction Models 12.1 Motivation A sports doctor routinely measures the muscle percentages of his clients. He also asks them how many hours per week they typically spend on training. Our doctor suspects that clients who train more are also more muscled. Furthermore, he thinks that the effect of training on muscularity declines with age. In multiple regression analysis, this is known as a moderation or interaction effect illustrated in the figure below. In mathematical terms an interaction is an additional multiplication term: \\[ muscle = training + age + training*age \\] 12.2 Data &amp; Sample In the following we further explore the aforementioned income-age relationship and how the relationship itself may be influenced by gender and other characteristics. Show me all the data wrangling. library(haven) master &lt;- read_dta(&quot;https://github.com/MarcoKuehne/marcokuehne.github.io/blob/main/data/SOEP/practice_en/practice_dataset_eng.dta?raw=true&quot;) # The data comes with Stata labels that do not work with all tidyverse commands library(sjlabelled) soep &lt;- remove_all_labels(master) # Rename German to English variable names library(tidyverse) soep &lt;- soep %&gt;% rename(&quot;Age&quot; = &quot;alter&quot;, &quot;Income&quot; = &quot;einkommenj1&quot;, &quot;NACE2&quot; = &quot;branche&quot;, &quot;Persons in HH&quot; = &quot;anz_pers&quot;, &quot;Num_Kids&quot; = &quot;anz_kind&quot;, &quot;Education&quot; = &quot;bildung&quot;, &quot;Health&quot; = &quot;gesund_org&quot;, &quot;Satisfaction&quot; = &quot;lebensz_org&quot;, &quot;Emp&quot; = &quot;erwerb&quot;) # Modify some variables soep &lt;- soep %&gt;% mutate(Single = ifelse(`Persons in HH` == 1, 1, 0), Kids = ifelse(Num_Kids == 0, 0, 1), Female = sex) %&gt;% mutate(Industry = case_when(NACE2 %in% c(1,2,3) ~ &quot;Agriculture&quot;, NACE2 %in% c(5:9) ~ &quot;Mining&quot;, NACE2 %in% c(10:32) ~ &quot;Manufacturing&quot;, NACE2 %in% c(35:38) ~ &quot;Energy&quot;, NACE2 %in% c(41:43) ~ &quot;Construction&quot;, NACE2 %in% c(50,51,52,55) ~ &quot;Trade&quot;, NACE2 %in% c(60,61,62,63,64) ~ &quot;Transport&quot;, NACE2 %in% c(65,66,67) ~ &quot;Banking&quot;, NACE2 %in% c(70,71,72,73,74,75,80,85,90,91,92,93,95,98,99) ~ &quot;Services&quot;, TRUE ~ &quot;Other&quot;)) # Round annual income to two digits soep &lt;- soep %&gt;% mutate(Income = round(Income, 2)) # Build the estimation sample based on the topic soep &lt;- soep %&gt;% filter(Emp == 1) %&gt;% filter(Age &lt;= 65) %&gt;% filter(Income &gt; 1) %&gt;% filter(syear == 2019) # Conduct a complete case analysis soep &lt;- soep %&gt;% filter(complete.cases(.)) The datasummary() function from modelsummary package provides a good overview for clean, selected variables. library(modelsummary) soep %&gt;% dplyr::select(Income, Age, Kids, Female, Satisfaction) %&gt;% datasummary_skim() Unique (#) Missing (%) Mean SD Min Median Max Income 1211 0 42145.2 23488.8 915.5 37337.3 257886.2 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Age 47 0 45.4 11.2 18.0 47.0 65.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Kids 2 0 0.4 0.5 0.0 0.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Female 2 0 0.4 0.5 0.0 0.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Satisfaction 11 0 7.6 1.4 0.0 8.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } The average annual income in this sample is 42145. Notice that the income is left skewed whereas life satisfaction is skewed to the right. As expected, most people earn a low or average income and there are several extreme high incomes at the right. On the other hand, most people are on average pretty satisfied with their lifes in general. The mean of a dummy variables is the share of people with the characteristic coded with 1, i.e. about 40 % of the people have kids. In the following, binary dummy variables are coded as factors. # Explicitly code factor variables soep &lt;- soep %&gt;% mutate(Female = factor(Female), Kids = factor(Kids)) 12.3 Throwback Parallel Slopes Simple regression is very similar to the correlation between two variables. A multiple regression with one continuous and one dummy variable is called a parallel slopes model. The data looks like this: Conclusion from Parallel Slopes: The fundamental truth is that the effect of age on income is identical across gender (i.e. for males and females). Males and female are equipped (by nature, for some reason) with a different starting annual income on average. -- Convinced? 12.4 Regression with Moderators In the following we consider all possible kinds of two-way interactions (between two variables). Higher-order interaction might be useful under special conditions, e.g. in a study where the effect of a new drug depends on the patient's age and gender. An interaction can be coded implicitly or explicitly in R. Show me the coding. # implicit coding, that automatically extends lm1 &lt;- lm(mpg ~ vs*am, data=mtcars) # explicit coding, each single term and their interaction lm2 &lt;- lm(mpg ~ vs + am + vs:am, data=mtcars) # only use the interaction term? lm3 &lt;- lm(mpg ~ vs:am, data=mtcars) modelsummary(title = &#39;Coding Interaction in R.&#39;, list(&quot;Version 1&quot; = lm1, &quot;Version 2&quot; = lm2, &quot;Version 3&quot; = lm3), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;) Table 12.1: Coding Interaction in R.  Version 1  Version 2  Version 3 (Intercept) 15.050 15.050 17.772 (1.002) (1.002) (0.826) vs 5.693 5.693 (1.651) (1.651) am 4.700 4.700 (1.736) (1.736) vs × am 2.929 2.929 10.599 (2.541) (2.541) (1.766) Num.Obs. 32 32 32 The single terms vs and am are sometimes called the main effects. 12.4.1 Dummy * Dummy We estimate how income depends on having kids and being female. interact1 &lt;- lm(Income ~ Kids*Female, data=soep) modelsummary(title = &#39;Interaction Model 1.&#39;, list(&quot;Income&quot; = interact1), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;) Table 12.2: Interaction Model 1.  Income (Intercept) 43476.056 (1128.318) Kids1 4329.056 (1650.629) Female1 -6131.395 (1727.116) Kids1 × Female1 -8708.424 (2954.627) Num.Obs. 1211 We plot the interaction with via cat_plot() from the interactions package. This interaction dedicated package makes visualization easy. library(interactions) cat_plot(interact1, pred = Kids, modx = Female) This graph suggests that men earn more than women. Men with kids earn more than men without kids. The kids-effect is reversed for women, which report higher incomes when they do not have kids. 12.4.2 Categorical * Dummy We estimate how income depends on working in different industries and being female. interact2 &lt;- lm(Income ~ Industry*Female, data=soep) library(broom) library(knitr) # tidy(interact2) %&gt;% # select(term, estimate, std.error) %&gt;% # #filter(! is.na(estimate)) %&gt;% # kable(digits=3) # modelsummary(title = &#39;Interaction Model 2.&#39;, # list(&quot;Income&quot; = interact2), # gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;) parts = 2 ## multi-column parts to display alongside tidy(interact2) |&gt; dplyr::select(term, coefficient = estimate) |&gt; mutate(part = rep(1:parts, each = ceiling(n()/parts), length.out = n()), row_index = rep(1:ceiling(n()/parts), length.out = n()) ) |&gt; split(~ part) |&gt; Reduce(f = \\(x, y) x |&gt; select(-part) |&gt; left_join(y, by = &#39;row_index&#39;)) |&gt; dplyr::select(-c(row_index, part)) |&gt; kable() term.x coefficient.x term.y coefficient.y (Intercept) 23513.113 Female1 -549.8783 IndustryBanking 46946.525 IndustryBanking:Female1 -3508.2902 IndustryConstruction 8443.728 IndustryConstruction:Female1 -4093.7365 IndustryEnergy 21340.939 IndustryEnergy:Female1 9357.9811 IndustryManufacturing 24884.761 IndustryManufacturing:Female1 -11453.9382 IndustryMining 31740.912 IndustryMining:Female1 -130.2967 IndustryOther 18604.883 IndustryOther:Female1 -8681.7468 IndustryServices 31579.784 IndustryServices:Female1 -11550.5649 IndustryTrade 14766.445 IndustryTrade:Female1 -8405.7213 IndustryTransport 39986.219 IndustryTransport:Female1 -12060.6618 # The function can be modified by ggplot commands. cat_plot(interact2, pred = Industry, modx = Female) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 12.4.3 Continuous * Dummy Note that R automatically adds main effects if you use multiplication operator: Thus \\[ height * female \\] translates to \\[ height + female + height*female \\] interact3 &lt;- lm(Income ~ Age*Female, data=soep) modelsummary(title = &#39;Interaction Model 2.&#39;, list(&quot;Income&quot; = interact3), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;) Table 12.3: Interaction Model 2.  Income (Intercept) 17367.198 (3438.645) Age 612.892 (72.860) Female1 4649.196 (5506.496) Age × Female1 -296.045 (118.698) Num.Obs. 1211 The visualization for non-parallel slopes looks like this: interact_plot(interact3, pred = Age, modx = Female, main.title = &quot;Each group has a different slope.&quot;) Conclusion The fundamental truth is that the effect of age on income is positive but not identical between males and females. Men and women at the exact same age earn different annual incomes and the gap widens over time. There is effect heterogeneity. There are two more rather uncommon combination in which one of the main effects is dropped. Graphically, the result is two lines with same intercept and different slopes. interact3b &lt;- lm(Income ~ Age + Age:Female, data=soep) interact_plot(interact3b, pred = Age, modx = Female) + scale_x_continuous(limits = c(0, 65)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) # modelsummary(title = &#39;Interaction Model 2.&#39;, # list(&quot;Income&quot; = interact3b, # &quot;Income&quot; = interact3c), # gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;) 12.4.4 Continuous * Continuous interact4 &lt;- lm(Income ~ Age*Satisfaction, data=soep) interact4 #&gt; #&gt; Call: #&gt; lm(formula = Income ~ Age * Satisfaction, data = soep) #&gt; #&gt; Coefficients: #&gt; (Intercept) Age Satisfaction Age:Satisfaction #&gt; 20923.43 59.48 -378.77 62.08 The interaction of two continuous variables is harder to interpret. There are conventions to help you choose the best values of the continuous moderator for plotting predicted values. But these conventions don't always work in every situation. For example, one convention suggested by Cohen and Cohen and popularized by Aiken and West is to use three values of the moderator: the mean, the value one standard deviation above, and the value one standard deviation below the mean. This is what interact_plot() does by default. interact_plot(interact4, pred = Age, modx = Satisfaction) This shows you that interaction between two continuous variables works basically the same way as for a categorical and continuous variable. An interaction says that there's not a fixed offset: you need to consider both values of x1 and x2 simultaneously in order to predict y. You can see that even with just two continuous variables, coming up with good visualizations are hard. But that's reasonable: you shouldn't expect it will be easy to understand how three or more variables simultaneously interact! But again, we're saved a little because we're using models for exploration, and you can gradually build up your model over time. The model doesn't have to be perfect, it just has to help you reveal a little more about your data. 12.5 Model Comparison How to decide if the model with interaction is better than the model without interaction? library(gt) multiple3 &lt;- lm(Income ~ Age + Female, data=soep) multiple4 &lt;- lm(Income ~ Age + Satisfaction, data=soep) modelsummary(title = &#39;Comparison&#39;, list(&quot;Income&quot; = multiple3, &quot;Income&quot; = interact3, &quot;Income&quot; = multiple4, &quot;Income&quot; = interact4), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, output = &quot;gt&quot;) %&gt;% # column labels tab_spanner(label = &#39;Interaction&#39;, columns = c(3,5)) #mbjdrtjbqd table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #mbjdrtjbqd thead, #mbjdrtjbqd tbody, #mbjdrtjbqd tfoot, #mbjdrtjbqd tr, #mbjdrtjbqd td, #mbjdrtjbqd th { border-style: none; } #mbjdrtjbqd p { margin: 0; padding: 0; } #mbjdrtjbqd .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #mbjdrtjbqd .gt_caption { padding-top: 4px; padding-bottom: 4px; } #mbjdrtjbqd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #mbjdrtjbqd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #mbjdrtjbqd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mbjdrtjbqd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mbjdrtjbqd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mbjdrtjbqd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #mbjdrtjbqd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #mbjdrtjbqd .gt_column_spanner_outer:first-child { padding-left: 0; } #mbjdrtjbqd .gt_column_spanner_outer:last-child { padding-right: 0; } #mbjdrtjbqd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #mbjdrtjbqd .gt_spanner_row { border-bottom-style: hidden; } #mbjdrtjbqd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #mbjdrtjbqd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #mbjdrtjbqd .gt_from_md > :first-child { margin-top: 0; } #mbjdrtjbqd .gt_from_md > :last-child { margin-bottom: 0; } #mbjdrtjbqd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #mbjdrtjbqd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #mbjdrtjbqd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #mbjdrtjbqd .gt_row_group_first td { border-top-width: 2px; } #mbjdrtjbqd .gt_row_group_first th { border-top-width: 2px; } #mbjdrtjbqd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mbjdrtjbqd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #mbjdrtjbqd .gt_first_summary_row.thick { border-top-width: 2px; } #mbjdrtjbqd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mbjdrtjbqd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mbjdrtjbqd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #mbjdrtjbqd .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #mbjdrtjbqd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #mbjdrtjbqd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mbjdrtjbqd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mbjdrtjbqd .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #mbjdrtjbqd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mbjdrtjbqd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #mbjdrtjbqd .gt_left { text-align: left; } #mbjdrtjbqd .gt_center { text-align: center; } #mbjdrtjbqd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #mbjdrtjbqd .gt_font_normal { font-weight: normal; } #mbjdrtjbqd .gt_font_bold { font-weight: bold; } #mbjdrtjbqd .gt_font_italic { font-style: italic; } #mbjdrtjbqd .gt_super { font-size: 65%; } #mbjdrtjbqd .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #mbjdrtjbqd .gt_asterisk { font-size: 100%; vertical-align: 0; } #mbjdrtjbqd .gt_indent_1 { text-indent: 5px; } #mbjdrtjbqd .gt_indent_2 { text-indent: 10px; } #mbjdrtjbqd .gt_indent_3 { text-indent: 15px; } #mbjdrtjbqd .gt_indent_4 { text-indent: 20px; } #mbjdrtjbqd .gt_indent_5 { text-indent: 25px; } Table 12.4: Comparison Income Interaction Income Income Income (Intercept) 22487.063 17367.198 20923.425 -230.558 (2764.643) (3438.645) (13829.780) (4385.352) Age 501.348 612.892 59.480 527.982 (57.643) (72.860) (296.198) (57.826) Female1 -8670.282 4649.196 (1345.244) (5506.496) Age × Female1 -296.045 (118.698) Satisfaction -378.773 2423.054 (1794.752) (450.609) Age × Satisfaction 62.082 (38.496) Num.Obs. 1211 1211 1211 1211 F 62.338 43.812 37.987 55.606 "],["logistic-regression.html", "Chapter 13 Logistic Regression 13.1 Think inside the box 13.2 From probability to odds 13.3 From odds to log of odds 13.4 Titanic Survival", " Chapter 13 Logistic Regression Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). 13.1 Think inside the box The table contains data on the relationship between hours of study and the outcome of an exam (pass/fail) sorted ascending for study hours. The pass outcome is coded 1 for \"Yeah, I passed the exam\" and 0 for \"I'll be back\". Linear regression is agnostic to the structure of the data. lm() fits a linear model to the data. Logistic regression acknowledges the floor and ceiling of values between 0 and 1. It squeezes the line to a squiggle inside the box. Well, the x-axis don't necessarily have this restriction. It is usually difficult to model a variable which has restricted range, such as probability. Get around the restricted range problem with a transformation. 13.2 From probability to odds Everybody has a good sense for probability. Not so for odds. Let’s say that the probability to pass the exam is \\(0.8 = 80\\%\\). Then the probability of failure is \\(1 – 0.8 = 0.2 = 20\\%\\). The odds of passing the exam are defined as the ratio of the probabilities: \\(\\text{odds} = \\frac{0.8}{0.2} = 4\\). That is to say that the odds of passing are 4 to 1. Odds provide a measure of the likelihood of a particular outcome. Probability is naturally restricted between 0 and 1, what about odds? Odds range from 0 to positive infinity. The transformation from probability to odds is a monotonic transformation, meaning the odds increase as the probability increases or vice versa. 13.3 From odds to log of odds The log() of values between 0 and 1 is negative, above 1 positive. Logodds range between negative and positive infinity. Range problem solved. Imagine you flip the axes, doesn't this look like the initial S curve we were looking for? 13.4 Titanic Survival Figure 13.1: RMS Titanic. The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. --- Kaggle Competition 13.4.1 The Data Load a comprehensive dataset on Titanic. titanic &lt;- read.csv(&quot;https://raw.githubusercontent.com/MarcoKuehne/marcokuehne.github.io/main/data/titanic.csv&quot;) titanic &lt;- titanic[,-1] Let's check the number of missing values per variable: ## missings per variable sapply(X = titanic, FUN = function(x) sum(is.na(x))) #&gt; PassengerId Pclass Name Sex Age SibSp #&gt; 0 0 0 0 263 0 #&gt; Parch Ticket Fare Cabin Embarked Survived #&gt; 0 0 1 1014 2 0 There are 2 missing values in Embarked and 263 in Age. The most missings fall upon Cabin though. Cabin information is not a very useful predictor for survival, thus we can remove this information for our analysis. We conduct a complete case analysis. The dataset should look like this (n = 1043): titanic &lt;- titanic %&gt;% select(!c(Cabin, PassengerId, Name, Ticket)) %&gt;% filter(complete.cases(.)) DT::datatable(titanic, rownames = FALSE) 13.4.2 No Predictor Variables In an empty model, there is only the intercept and no predictor: \\[logit (p) = \\beta_0\\] library(tidyverse) logistic0 = glm(Survived ~ 1, family = binomial(link = &#39;logit&#39;), data = titanic) library(modelsummary) modelsummary(title = &#39;Empty Model.&#39;, list(&quot;Survival&quot; = logistic0), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.&#39;, coef_map = cm) Table 13.1: Empty Model. Survival Constant -0.414 (0.063) Num.Obs. 1043 What does \\(-0.414\\) represent? \\[logit (p) = \\log \\frac{p}{1-p} = \\beta_0 = -0.414 \\] What is p here? The overall probability of survival (Survived = 1). Let’s take a look at the frequency table for survival. table(titanic$Survived) #&gt; #&gt; 0 1 #&gt; 628 415 \\(p = 415/1043 = 0.3978907\\) is the share of survivors. \\(0.3978907 / (1-0.3978907) = 0.660828\\) is the odds of surviving. \\(log(0.660828) = -0.4142617\\) is the logodds of surviving. In logistic regression models regression coefficients are reported in log odds. 13.4.3 Single Dichotomous Predictor We include gender as an explanatory variable and expect being male reduces the probability of surviving (Women and children first).16 “Passengers’ chances of surviving the sinking of the S.S. Titanic were related to their sex and their social class: females were more likely to survive than males, and the chances of survival declined with social class as measured by the class in which the passenger travelled.” Hall (1986) \\[logit (p) = \\beta_0 + \\beta_1 \\cdot sex\\] logistic1 = glm(Survived ~ Sex, family = binomial(link = &#39;logit&#39;), data = titanic) modelsummary(title = &quot;Single Predictor.&quot;, list(&quot;Survival&quot; = logistic1), gof_omit = &#39;R2|AIC|BIC|RMSE|Log.Lik.|F&#39;, coef_map = cm) Table 13.2: Single Predictor. Survival Constant 1.616 (0.137) Male -3.418 (0.177) Num.Obs. 1043 The dummy variable behaves as in the linear model in the sense that it either adds to the intercept when it is 1 or does not add anything when it is 0. The logodds of survival for women are: 1.6156685 The logodds of survival for men are: -1.8024548 We calculate the logodds for men based on the frequencies of male and female survival: table(titanic$Survived, titanic$Sex) #&gt; #&gt; female male #&gt; 0 64 564 #&gt; 1 322 93 In our dataset, what are the odds of a male being in the survival class and what are the odds of a female being in the survival class? The odds for males are 93 to 564 is 0.1648936. The odds for females are 322 to 64 5.03125. The odds female are about 30 times higher than the odds for males. The ratio of the odds for female to the odds for male is \\[odds ratio^{female} = \\frac{odds^{female}}{odds^{male}} = \\frac{\\frac{322}{64}}{\\frac{93}{564}} = \\frac{322}{64} \\frac{564}{93} = \\frac{7567}{248} = 30.5121.\\] Where the odds ratio for males to survive is: \\[odds ratio^{male} = \\frac{odds^{male}}{odds^{female}} = \\frac{\\frac{93}{564}}{\\frac{322}{64}} = \\frac{1}{30.5121} = 0.03277388.\\] We can get this figure more easily by exponentiating the regression coefficient, since: \\[e^{-3.418} = 0.03277793\\] Table 13.3: Logistic regression in default coefficients and odds ratio. Survival Survival (Odds Ratio) Constant 1.616 5.031 (0.137) (0.689) Male -3.418 0.033 (0.177) (0.006) Num.Obs. 1043 1043 Definition Odds ratio (OR) is a statistical measure used to describe the strength of the association between two events, such as the presence of a risk factor (being male) and the occurrence of the outcome (i.e. survival). We interpret OR as follows. An odds ratio greater than 1 indicates that the event is more likely to occur in the exposed group than in the non-exposed group, while an odds ratio less than 1 indicates that the event is less likely to occur in the exposed group than in the non-exposed group. Odds-ratios are sometimes misinterpreted as if they were relative risks/probabilities. Nonetheless presenting odds-ratios is standard practice in the medical literature. 13.4.4 Effect plot We use a nested version of plot() on allEffects() on the logistic1 model (effects package) to visualize the effect of gender: library(effects) plot(allEffects(logistic1)) What kind of effect does this plot show? And how did the effects package make those estimates? Instead of odds ratio or odds the effect plot shows the plain male and female survival rates. ## male survival: (~ 14%) table(titanic$Survived, titanic$Sex)[2,2]/(table(titanic$Survived, titanic$Sex)[2,2] + table(titanic$Survived, titanic$Sex)[1,2]) #&gt; [1] 0.1415525 ## ## female survival: (~ 83%) table(titanic$Survived, titanic$Sex)[2,1]/(table(titanic$Survived, titanic$Sex)[2,1] + table(titanic$Survived, titanic$Sex)[1,1]) #&gt; [1] 0.8341969 Recap: The odds for males are 93 to 564 is 0.1648936. The male survival is 93 to (564+93) = 0.1415525. The odds for females are 322 to 64 5.03125. The female survival is 322 to (322+64) = 0.8341969. 13.4.5 Chi Square Logistic regression checked the relationship between survival (binary) and gender (binary). So can \\(\\chi^2\\). Definition The Chi Square Test for Independence is a statistical test used to determine whether there is a significant association between two categorical variables. \\[X^2 = \\sum_i \\frac{(O_i-E_i)^2}{E_i}\\] Where \\(O_i\\) is the observed frequency and \\(E_i\\) is the expected frequency. It turns out that the 2 X 2 contingency analysis with chi-square is really just a special case of logistic regression, and this is analogous to the relationship between ANOVA and regression. chisq.test(titanic$Survived, titanic$Sex) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: titanic$Survived and titanic$Sex #&gt; X-squared = 484.02, df = 1, p-value &lt; 0.00000000000000022 In the standard output of the test we usually conclude from the p-value (being less than 0.05) that there's is a relationship between the two variables (survival on Titanic is related to gender.) We derive the test statistic. Everything starts from the contingency table of survival by gender. The design is improved by the epitab package. Survival All Deceased Survived Total 1043 628 (100) 415 (100) Sex Female 386 (37) 64 (10.2) 322 (77.6) Male 657 (63) 564 (89.8) 93 (22.4) Chi Square Test Statistic We derive the test statistic of Chi. tab_margins &lt;- addmargins(table(titanic$Survived, titanic$Sex)) tab_obs &lt;- table(titanic$Survived, titanic$Sex) ## Step 1: Expected absolute frequency library(&quot;DescTools&quot;) tab_exp &lt;- ExpFreq(table(titanic$Survived, titanic$Sex), freq =&quot;abs&quot;) ## Step 2: Subtract the expected value from the observed value (and square differences) (tab_obs-tab_exp)^2 #&gt; #&gt; female male #&gt; 0 28363.34 28363.34 #&gt; 1 28363.34 28363.34 ## Step 3: Divide by expected values (tab_obs-tab_exp)^2/tab_exp #&gt; #&gt; female male #&gt; 0 122.03790 71.69959 #&gt; 1 184.67422 108.49962 ## Step 4: Sum up all tab_temp &lt;- (tab_obs-tab_exp)^2/tab_exp tab_temp[1,1] + tab_temp[1,2] + tab_temp[2,1] + tab_temp[2,2] #&gt; [1] 486.9113 13.4.6 Some Are More Equal Than Others Let's extend the group analysis to multiple categories using the Pclass variable. The passenger class represents the socio-economic status of people. We hypothesize that people with more money used their influence to get into one of the lifeboats. First, the contingency table of the situation: Survival All Deceased Survived Total 1043 628 (100) 415 (100) Passenger Class 1 282 (27) 114 (18.2) 168 (40.5) 2 261 (25) 149 (23.7) 112 (27) 3 500 (47.9) 365 (58.1) 135 (32.5) In first class, more people survived (n=168) than died (n=114) whereas this is not true for the third class (365 died and 135 survived). All men may be created equal, but some are more equal than others. The logistic regression with one categorical variable is straightforward. Note that we tell R to handle Pclass like a factor (group variable): logistic2 = glm(Survived ~ factor(Pclass), family = binomial(link = &#39;logit&#39;), data = titanic) Table 13.4: Logistic regression in default coefficients and odds ratio. Survival Survival (Odds Ratio) Constant 0.388 1.474 (0.121) (0.179) Class 2 -0.673 0.510 (0.174) (0.089) Class 3 -1.382 0.251 (0.158) (0.040) Num.Obs. 1043 1043 The lower your socio-economic status, the less survival can you expect. 13.4.7 The Cheaper The Deader? We have more information on the effect of socio-economic class, the Fare variable (ticket price). In the data the Fare variable is like this: datasummary_skim(titanic$Fare) Unique (#) Missing (%) Mean SD Min Median Max data 255 0 36.6 55.8 0.0 15.8 512.3 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } The prices of tickets on the Titanic in 1912 ranged from £870 or $4,350 for a first-class parlor suite to a maximum of £8 or $40 for a third-class passage. logistic3 = glm(Survived ~ Fare, family = binomial(link = &#39;logit&#39;), data = titanic) Table 13.5: Logistic regression in default coefficients and odds ratio. Survival Survival (Odds Ratio) Constant -0.807 0.446 (0.084) (0.037) Fare 0.011 1.011 (0.002) (0.002) Num.Obs. 1043 1043 The coefficient and intercept estimates give us the following equation: \\[ \\text{logit}(p) = -0.80666 + 0.01109 \\cdot \\text{fare} \\] In this case, the estimated coefficient for the intercept is the log odds of a passenger with a fare of zero surviving the accident. Of course, there is nothing like a free lunch. So the intercept in this model corresponds to the log odds of survival when fare is at the hypothetical value of zero. How do we interpret the slope coefficient for Fare? Let's fix Fare at some value, e.g. the mean which roughly equals 37 (rounded to 0 decimals). Then the conditional logit of surviving when the fare is equal to the mean is: \\[\\begin{aligned} \\text{logit}(p | \\text{fare}=37) &amp;= -0.3964565 \\\\ \\text{logit}(p | \\text{fare}=38) &amp;= -0.38537 \\\\ \\end{aligned}\\] We can examine the effect of a one-unit increase in fare by taking the difference of the two equations: \\[\\text{logit}(p | \\text{fare}=37) - \\text{logit}(p | \\text{fare}=38) = 0.0110866 \\] In other words, for a one-unit increase in the fare price, the expected change in log odds is 0.0110866. Can we translate this change in log odds to the change in odds? Indeed, we can. Recall that logarithm converts multiplication and division to addition and subtraction. Its inverse, the exponentiation converts addition and subtraction back to multiplication and division. If we exponentiate both sides of our last equation, we have the following: \\[ e^{\\text{logit}(p | \\text{fare}=38) - \\text{logit}(p | \\text{fare}=37)} = e^{0.0110866} = 1.011148 \\] So we can say for a one-unit increase in fare price, we expect to see about 1.1% increase in the odds of surviving. This 1.1% of increase does not depend on the value that fare is held at. Can we get this information more directly? Yes, we can. require(MASS) exp(cbind(coef(logistic3), confint(logistic3))) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 0.4463467 0.3780949 0.5250284 #&gt; Fare 1.0111482 1.0080791 1.0145089 13.4.8 The Bigger Picture First, we include all variables that we covered so far, i.e. Sex, Class and Fare in a model. In a second version we add Age, SibSp, Parch and Embarked. How do two models compare? logistic4_small = glm(Survived ~ Sex + factor(Pclass) + Fare, family = binomial(link = &#39;logit&#39;), data = titanic) logistic4_big = glm(Survived ~ Sex + factor(Pclass) + Fare + Age + SibSp + Parch + Embarked, family = binomial(link = &#39;logit&#39;), data = titanic) modelsummary(title = &#39;Logistic regression comparison with model statistics.&#39;, list(&quot;Survival&quot; = logistic4_small, &quot;Survival (Odds Ratio)&quot; = logistic4_small, &quot;Survival&quot; = logistic4_big, &quot;Survival (Odds Ratio)&quot; = logistic4_big), gof_omit = &#39;R2|RMSE|Log.Lik.|F&#39;, #metrics = &quot;all&quot;, coef_map = cm, exponentiate = c(FALSE,TRUE,FALSE,TRUE)) Table 13.6: Logistic regression comparison with model statistics. Survival Survival (Odds Ratio) Survival Survival (Odds Ratio) Constant 2.605 13.526 4.387 80.396 (0.284) (3.843) (0.472) (37.966) Class 2 -0.822 0.440 -1.149 0.317 (0.270) (0.119) (0.300) (0.095) Class 3 -1.644 0.193 -2.086 0.124 (0.264) (0.051) (0.310) (0.039) Male -3.554 0.029 -3.690 0.025 (0.194) (0.006) (0.211) (0.005) Fare 0.001 1.001 0.002 1.002 (0.002) (0.002) (0.002) (0.002) Age -0.036 0.965 (0.007) (0.007) SibSp -0.319 0.727 (0.118) (0.086) Parch -0.116 0.890 (0.113) (0.101) Num.Obs. 1043 1043 1043 1043 AIC 825.9 825.9 804.0 804.0 BIC 850.6 850.6 853.5 853.5 Definition Akaike Information Criterion (AIC) is a measure of the quality of a model that takes into account the complexity of the model. Lower AIC values indicate a better model fit. To compare two models, calculate the AIC for each model and select the model with the lower AIC value. Bayesian Information Criterion (BIC) is similar to AIC but places a stronger penalty on the number of parameters in the model. Like AIC, lower BIC values indicate a better model fit. AIC is lower in the comprehensive model whereas BIC is lower in the base model. Another possibility for comparison of model performance is the ROC curve. Definition Receiver Operating Characteristic (ROC) curve is a plot of sensitivity against 1-specificity. A better model will have an ROC curve that is closer to the upper left corner, which represents perfect classification. From the ROC curve there seems to be a slight outperformance of the bigger model. https://en.wikipedia.org/wiki/Women_and_children_first↩︎ "],["marginal-effects.html", "Chapter 14 Marginal Effects", " Chapter 14 Marginal Effects Now we better understand that the effect of our main explanatory variable might be increased or decreased by another variable. Still, a question is open to answer: What exactly is the effect of our main explanatory variable in the interaction model? In OLS framework regression coefficients have direct interpretation as unconditional marginal effects: predicted change in y due to a unit change in x. Interactions or higher-order terms (e.g. age square) make interpretation difficult or impossible. For interpretation of the effect and the statistical significance, we have to investigate all interacted variables at the same time. Marginal effects are partial derivatives of the regression equation with respect to a variable from the model. \\[ \\begin{align} weight &amp;= \\beta_0 + \\beta_1 \\cdot height \\tag{simple regression} \\\\ \\frac{\\partial weight}{\\partial height} &amp;= \\beta_1 \\tag{unconditional effect} \\\\ weight &amp;= \\beta_0 + \\beta_1 \\cdot height + \\beta_2 \\cdot female + \\beta_3 \\cdot height \\cdot female \\tag{moderated regression} \\\\ \\frac{\\partial weight}{\\partial height} &amp;= \\beta_1 + \\beta_3 \\cdot female \\tag{conditional effect} \\\\ \\end{align} \\] library(stargazer) stargazer(simple, interact1, type=&quot;text&quot;) #&gt; #&gt; ===================================================================== #&gt; Dependent variable: #&gt; ------------------------------------------------- #&gt; Income #&gt; (1) (2) #&gt; --------------------------------------------------------------------- #&gt; Age 524.554*** #&gt; (58.487) #&gt; #&gt; Kids1 4,329.056*** #&gt; (1,650.629) #&gt; #&gt; Female1 -6,131.395*** #&gt; (1,727.116) #&gt; #&gt; Kids1:Female1 -8,708.424*** #&gt; (2,954.627) #&gt; #&gt; Constant 18,341.030*** 43,476.060*** #&gt; (2,733.469) (1,128.318) #&gt; #&gt; --------------------------------------------------------------------- #&gt; Observations 1,211 1,211 #&gt; R2 0.062 0.045 #&gt; Adjusted R2 0.062 0.042 #&gt; Residual Std. Error 22,753.730 (df = 1209) 22,985.580 (df = 1207) #&gt; F Statistic 80.439*** (df = 1; 1209) 18.853*** (df = 3; 1207) #&gt; ===================================================================== #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 For the simple regression, the coefficient is identical (unconditional). library(margins) margins(simple) #&gt; Age #&gt; 524.6 14.0.1 Marginal effects at representatives We can use representative values which we literally plug in the equation. These are the effects for each group (what we have seen before in the interaction plot): \\[ \\begin{align} weight &amp;= \\beta_0 + \\beta_1 \\cdot height + \\beta_2 \\cdot female + \\beta_3 \\cdot height \\cdot female \\tag{interact1} \\\\ \\frac{\\partial weight}{\\partial height} &amp;= \\beta_1 + \\beta_3 \\cdot female \\tag{conditional effect} \\\\ \\end{align} \\] Here is the R code: margins(interact1, at = list(Female = c(&quot;0&quot;, &quot;1&quot;))) #&gt; at(Female) Kids1 Female1 #&gt; 0 4329 -9633 #&gt; 1 -4379 -9633 Please calculate the MER of height in interact1 for the two gender programmtic in R. 14.0.2 Marginal effects at the mean Plugging in values is not always as easy as it seems to be. What should we use for age? Values as 0, 1 or 2 does not makes sense for age. Okay, we could use some representative age, e.g. 30 and 40. But what is our theoretical justification to report the effect for age 30 and not age 31? Another strategy is to use mean values of the variables. #&gt; [1] 45.37985 \\[ \\begin{align} weight &amp;= \\beta_0 + \\beta_1 \\cdot height + \\beta_2 \\cdot age + \\beta_3 \\cdot height \\cdot age \\tag{interact2} \\\\ \\frac{\\partial weight}{\\partial height} &amp;= \\beta_1 + \\beta_3 \\cdot age \\tag{conditional effect} \\\\ \\end{align} \\] library(&quot;stargazer&quot;) stargazer(simple, interact2, type=&quot;text&quot;) #&gt; #&gt; ================================================================================ #&gt; Dependent variable: #&gt; -------------------------------------------------- #&gt; Income #&gt; (1) (2) #&gt; -------------------------------------------------------------------------------- #&gt; Age 524.554*** #&gt; (58.487) #&gt; #&gt; IndustryBanking 46,946.530*** #&gt; (10,448.620) #&gt; #&gt; IndustryConstruction 8,443.728 #&gt; (6,850.222) #&gt; #&gt; IndustryEnergy 21,340.940*** #&gt; (7,884.232) #&gt; #&gt; IndustryManufacturing 24,884.760*** #&gt; (6,508.270) #&gt; #&gt; IndustryMining 31,740.910* #&gt; (16,779.550) #&gt; #&gt; IndustryOther 18,604.880*** #&gt; (6,469.352) #&gt; #&gt; IndustryServices 31,579.780*** #&gt; (6,878.945) #&gt; #&gt; IndustryTrade 14,766.440* #&gt; (7,823.505) #&gt; #&gt; IndustryTransport 39,986.220*** #&gt; (7,048.821) #&gt; #&gt; Female1 -549.878 #&gt; (16,779.550) #&gt; #&gt; IndustryBanking:Female1 -3,508.290 #&gt; (24,327.690) #&gt; #&gt; IndustryConstruction:Female1 -4,093.737 #&gt; (18,670.560) #&gt; #&gt; IndustryEnergy:Female1 9,357.981 #&gt; (20,595.100) #&gt; #&gt; IndustryManufacturing:Female1 -11,453.940 #&gt; (17,038.220) #&gt; #&gt; IndustryMining:Female1 -130.297 #&gt; (31,710.370) #&gt; #&gt; IndustryOther:Female1 -8,681.747 #&gt; (16,886.980) #&gt; #&gt; IndustryServices:Female1 -11,550.570 #&gt; (17,203.610) #&gt; #&gt; IndustryTrade:Female1 -8,405.721 #&gt; (18,612.280) #&gt; #&gt; IndustryTransport:Female1 -12,060.660 #&gt; (17,690.580) #&gt; #&gt; Constant 18,341.030*** 23,513.110*** #&gt; (2,733.469) (6,342.074) #&gt; #&gt; -------------------------------------------------------------------------------- #&gt; Observations 1,211 1,211 #&gt; R2 0.062 0.139 #&gt; Adjusted R2 0.062 0.125 #&gt; Residual Std. Error 22,753.730 (df = 1209) 21,969.590 (df = 1191) #&gt; F Statistic 80.439*** (df = 1; 1209) 10.112*** (df = 19; 1191) #&gt; ================================================================================ #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Here is the R code with margins: interact2 &lt;- lm(Income ~ Age*Satisfaction, data=soep) interact2 #&gt; #&gt; Call: #&gt; lm(formula = Income ~ Age * Satisfaction, data = soep) #&gt; #&gt; Coefficients: #&gt; (Intercept) Age Satisfaction Age:Satisfaction #&gt; 20923.43 59.48 -378.77 62.08 #summary(margins(interact2)) margins(interact2, at = list(Age = mean_age)) #&gt; at(Age) Age Satisfaction #&gt; 45.38 531.3 2439 Please calculate the MEM of height in interact2 programmtic in R. 14.0.3 Average marginal effects The default margins commands shows you something else than MER and MEM, namely average marginal effects. AMEs calculate marginal effects at every observed value of X and average across the resulting effect estimates. AMEs are particularly useful because -- unlike MEMs -- produce a single quantity summary that reflects the full distribution of X rather than an arbitrary prediction. margins(interact1) #&gt; Kids1 Female1 #&gt; 1222 -9633 A summary provides you with information that cannot easily be calculated by hand, e.g. standard error, p-value, confidence interval: summary(margins(interact1)) #&gt; factor AME SE z p lower upper #&gt; Female1 -9633.4620 1412.6589 -6.8194 0.0000 -12402.2226 -6864.7013 #&gt; Kids1 1222.4998 1375.3741 0.8888 0.3741 -1473.1839 3918.1836 "],["software-is-everywhere.html", "Chapter 15 Software is everywhere 15.1 Software 15.2 File formats", " Chapter 15 Software is everywhere \"The Zen of Organization is not to be found in Fancy Software.\" --- Kieran Healy. 2019. Data is inseparable from software.17 Data comes in various file formats that are connected to a specific software. Although most software can handle most of the common file formats. So, if almost all software can handle almost all data, how should we choose one? When choosing a software for data analysis there are many factors to consider:18 Does it run on your computer? Can you afford it? Do your colleagues use it? Does the software provide all the methods you need? Reading Kieran Healy (2019): The Plain Person’s Guide to Plain Text Social Science Kieran is speaking about two revolutions in computing. One puts single-purpose application in the foreground and by this hides the workings of the researcherdata scientistinternet backboneoperating system from the user. On the other side, open-source tools for licensingplain-text codingfundingkangaroos, data analysis, and writing are also better and more accessible than they have ever been. 15.1 Software Irrespective of the software, how to talk with it? 15.1.1 Command line interface (CLI) A command-line interpreter or command-line processor uses a command-line interface (CLI) to receive commands from a user in the form of lines of text. To open up the command prompt (on Windows), just press the windows key and search for cmd. When R is installed, it comes with a utility called Rscript. This allows you to run R commands from the command line. If Rscript is in your PATH, then typing Rscript into the command line, and pressing enter, will not result in an error. Otherwise, you might get a message saying “‘Rscript’ is not recognized as an internal or external command.” If that’s the case, then to use Rscript, you will either need to add it to your PATH (as an environment variable), or append the full directory of the location of Rscript on your machine. To find the full directory, search for where R is installed your computer. Figure 15.1: Command Line Interface on Windows with R The above example runs the commands 1*2*3*4 factorial(4) class(4) 1:4 15.1.2 Graphical user interface (GUI) The GUI, graphical user interface, is a form of user interface that allows users to interact with electronic devices through graphical icons and audio indicator such as primary notation, instead of text-based UIs, typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of CLIs (command-line interfaces), which require commands to be typed on a computer keyboard. Figure 15.2: RGui on Windows. Feels like no GUI. When you can click File -&gt; Open, you are on a GUI. 15.1.3 Integrated development environment (IDE) RStudio is an integrated development environment (IDE) for R. \"The RStudio IDE is a set of integrated tools designed to help you be more productive with R and Python. It includes a console, syntax-highlighting editor that supports direct code execution, and a variety of robust tools for plotting, viewing history, debugging and managing your workspace.\" RStudio RStudio comes with four panes Source Editor (open, edit and execute R code) Console (type and execute R code, it is like the command line) Environment (shows objects) Misc tabs (file manager, plots, package manager, help, ...) Figure 15.3: Graphical User Interface in RStudio 15.1.4 Online Compiler An online compiler allows to test a statistical (or programming) language, without any download, install or payment. There are various programming and web languages available at https://onecompiler.com/. The type setting processor LaTeX can be tested on https://www.overleaf.com/. Your Turn Copy paste the code in the console below. Click Run. Scroll for results. 1*2*3*4 factorial(4) class(4) 1:4 15.1.5 Cloud solutions You can pay for software as a service and use cloud computing for data analysis. Here are two cloud IDE: Posit Cloud: https://posit.cloud/ Codeanywhere: https://codeanywhere.com/ These cloud services are free with limited resources. You can buy more computational power or storage. Hardware, software and energy cost money. Amazing Fact Google’s own estimate which is a decade old shows that the energy required to power a Google search could power a low energy (10 watt) light bulb for 108 seconds. Read more on: fullfact.org 15.1.6 Dashboards A dashboard is a type of graphical user interface which often provides at-a-glance views of key performance indicators. It can be thought of as another form of a report and considered a form of data visualization. Dashboards have been common in business and finance. The COVID-19 pandemic of 2020 brought dashboards to the fore, with the Johns Hopkins and the RKI coronavirus tracker being good examples. Software to built dashboards is Microsoft Power BI, Tableau and of course R (flexdashboard and shinydashboard). 15.1.7 Shiny Apps R Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. Those apps allow the user some form of interaction with the data. 15.2 File formats 15.2.1 Text files A text file is a simple way of storing information line by line. Imagine the following dialogue. 11.04.20, 09:05 - Mum: Marcus discipulus est. 11.04.20, 10:46 - Marco: Tabellam tenet. 11.04.20, 10:53 - Mum: Scholam intrat, grammaticum salutat. 11.04.20, 10:46 - Marco: Graece et Latine scribere et legere libenter discit. 11.04.20, 10:46 - Mum: Grammaticum autem timet. It represents the typical structure of a WhatsApp chat export, i.e. DATE, TIME - AUTHOR: MESSAGE The information is separated by comma, hyphen and colon. A .txt file can be opened with any text editor. Other notable file types can be opened and read by a human with a text editor as well, e.g. an R script (ending .R) or R Markdown file (ending .Rmd). Here is the minimal file chat.txt. #&gt; [1] &quot;11.04.20, 09:05 - Mum: Marcus discipulus est. &quot; #&gt; [2] &quot;11.04.20, 10:46 - Marco: Tabellam tenet. &quot; #&gt; [3] &quot;11.04.20, 10:53 - Mum: Scholam intrat, grammaticum salutat. &quot; #&gt; [4] &quot;11.04.20, 10:46 - Marco: Graece et Latine scribere et legere libenter discit.&quot; #&gt; [5] &quot;11.04.20, 10:46 - Mum: Grammaticum autem timet.&quot; 15.2.2 Text with structure The trusty .csv (comma separated values) file is, still, one of the most common file types for data storage and exchange. It is knocking around ten years before the first personal computer. A .csv file stores tabular data in plain text format. Each line is a data record and each record has one or more fields, separated by commas. Tabular data can also be separated by a TAB or semicolon. Definition Tabular data refers to data that is organized in a table with rows and columns. Within the table, the rows represent observations and the columns represent attributes. Here is the minimal file chat.csv. #&gt; Date Time Name Message #&gt; 1 11.04.20 09:05 Mum Marcus discipulus est. #&gt; 2 11.04.20 10:46 Marco Tabellam tenet. #&gt; 3 11.04.20 10:53 Mum Scholam intrat, grammaticum salutat. #&gt; 4 11.04.20 10:46 Marco Graece et Latine scribere et legere libenter discit. #&gt; 5 11.04.20 10:46 Mum Grammaticum autem timet. Truly Dedicated Do you think text information is outdated? If you have a Netflix account, request your data from Netflix. It's easy and fun. Check what Netflix knows about your devices, ratings, profiles, avatars, subscriptions, billing and more. It comes in about two dozen .txt and .csv files. 15.2.3 File formats Interesting file types The .xlsx file is Microsoft Excel Open XML Format Spreadsheet file. The .dta file is a proprietary binary format designed for use as the native format for datasets with Stata, a system for statistics and data analysis. Stata 1.0 was released in 1985 for the IBM PC. The .sav file is from SPSS. The .Rdata file comes from R. The .JSON stand for JavaScript Object Notation. JSON is a standard text-based format for representing structured data based on JavaScript object syntax. Reading Amazing information on file formats (file-format classes, bitstream structures etc.) can be found at Library of Congress Collections. How to choose between file formats? Think about it. CSV’s are costing you time, disk space, and money. It’s time to end it. Picture this — you collect large volumes of data and store them in the cloud. You didn’t do much research on file formats, so you opt for CSVs. Your expenses are through the roof! A simple tweak can reduce them by half, if not more. That tweak is — you’ve guessed it — choosing a different file format. Dario Radečić (2021) Stop Using CSVs for Storage — This File Format Is 150 Times Faster .feather is a young file format created to improve exchange of data between R and Python. Feather is a fast, lightweight, and easy-to-use binary file format for storing data frames. Check the category of data analysis software and the list of statistical software.↩︎ Check out the versatile discussion on the measurement of data analysis software popularity by Robert München: The Popularity of Data Analysis Software↩︎ "],["references.html", "References 15.3 Resources", " References 15.3 Resources https://informationisbeautiful.net/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
